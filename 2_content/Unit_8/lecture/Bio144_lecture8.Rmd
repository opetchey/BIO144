---
title: "Lecture 8: Model/variable selection"
subtitle: "BIO144 Data Analysis in Biology"
author: "Owen Petchey, Stephanie Muff, Erik Willems"
institute: "University of Zurich"
date: "22 April 2024"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
library(knitr)
```

## Overview

-   Predictive vs. explanatory models
-   Selection criteria: AIC, AIC$_c$, and BIC
-   Automatic model selection and its caveats
-   Model selection bias
-   Collinearity among explanatory variables
-   Occam's razor

## Course material covered today

Today's lecture material is partially based on the following literature:

-   "Lineare regression" chapters 5.1-5.4
-   Chapter 27.1 and 27.2 by Clayton and Hills "Choice and
    Interpretation of Models" (pdf provided)

\textcolor{blue}{\bf Optional reading:}

-   Paper by Freedman 1983: "A Note on Screening Regression Equations"
    (Sections 1 and 2 are sufficient to get the point)

## Developing a model

So far, our regression models "fell from heaven": The model family and
the terms in the model were almost always given.

However, it is often not immediately obvious which terms are relevant to
include in a model.

Importantly, the approach to formulate a model **heavily depends on the
aim** for which the model is built.

The following distinction is important:

-   The aim is to \alert{predict} future values of **y** from known
    regressors. Variables in the model are covariates.
-   The aim is to \alert{explain} **y** using known regressors.
    Ultimately, the aim is to find causal relationships. Variables in
    the model are explanatory variables.

## 

$\rightarrow$ Even among statisticians there is no real consensus about
how, if, or when to select a model:

```{=tex}
\begin{center}
\includegraphics[width=10cm]{pictures/brewer_title.png}
\end{center}
```
Note: The first sentence of a paper in *Methods in Ecology and
Evolution* from 2016 is: "Model selection is difficult."

## Why is finding a model so hard?

Remember from week 1:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
Ein Modell ist eine Ann\"aherung an die Realit\"at. Das Ziel der Statistik und Datenanalyse ist es immer, dank Vereinfachungen der wahren Welt gewisse Zusammenh\"ange zu erkennen.
\end{minipage}}
```
\

Box (1979): \alert{"All models are wrong, but some are useful."}

$\rightarrow$ There is often not a "right" or a "wrong" model -- but
there are more and less useful ones.

$\rightarrow$ Finding a model with good properties is sometimes an
art...

## Predictive and explanatory models

Before we continue to discuss model/variable selection, we need to be
clear about the scope of the model:

-   \alert{\bf Predictive models}: These are models that aim to predict
    the outcome of future subjects.\
    \underline{Example:} In the bodyfat example the aim is to predict
    people's bodyfat from factors that are easy to measure (age, BMI,
    weight,..).

-   \alert{\bf Explanatory models}: These are models that aim at
    understanding the (causal) relationship between explanatory
    variables and the response.\
    \underline{Example:} The mercury study aims to understand if
    Hg-concentrations in the soil (explanatory) influence the
    Hg-concentrations in humans (response).

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
$\rightarrow$ The model selection strategy depends on this distinction.
\end{minipage}}
```
## Prediction vs explanation

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
When the aim is \emph{\bf prediction}, the best model is the one that best predicts the value of the outcome for a future subject. This is a well defined task and "objective" variable selection strategies to find the model which is best in this sense are potentially useful.\\
\\However, when used for \emph{\bf explanation} the best model will depend on the scientific question being asked, {\bf and automatic variable selection strategies have no place}. 
\end{minipage}}
```
\scriptsize(Clayton and Hills, 1993, chapters 27.1 and 27.2)

## A predictive model: The bodyfat example

The bodyfat study is a typical example of a **predictive model**.

There are 12 potential predictors for the response variable. Let's fit
the full model (without interactions):

\scriptsize

```{r, echo=FALSE, eval=TRUE, results='asis', message=FALSE, warning=FALSE}
d.bodyfat <- read.table(here("3_datasets/bodyfat.clean.txt"),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","chest","abdomen","hip","thigh","knee","ankle","biceps")]
r.bodyfat <- lm(bodyfat ~ ., d.bodyfat)
# library(biostatUZH)
# tableRegression(r.bodyfat)
kable(summary(r.bodyfat)$coefficients, digits= 3)

```

## Model selection for predictive models

-   \underline{Remember:} $R^2$ is not suitable for model selection,
    because it \emph{always} increases (improves) when a new variable is
    included.

-   Ideally, the predictive ability of a model is tested by a
    cross-validation (CV) approach.
    \href{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}
    {\beamergotobutton{Find a description of the CV idea here.}}

-   CV can be a bit cumbersome, and sometimes would require additional
    coding.

-   Approximations to CV: So-called \alert{information-criteria} like
    AIC, AIC$_c$, BIC...

-   The idea is that the "best" model is the one with the smallest value
    of the information criterion (where the criterion is selected in
    advance).

## Information-criteria

Information-criteria for model selection were made popular by Burnham &
Anderson (2002) <!-- \citet{burnham.anderson2002}. -->

\

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The idea is to find a \alert{balance between}  

\begin{center}  

{\bf Good model fit} $\quad\leftrightarrow\quad$ {\bf Low model complexity}
\end{center}
\end{minipage}}
```
$\rightarrow$ Reward models with a better fit to the data.\
\
$\rightarrow$ Penalize models with more parameters.

## AIC

The most prominent criterion is the
\alert{AIC (Akaike Information Criterion)}, which measures the
\alert{relative quality of a model}.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The AIC of a model with likelihood $L$ and $p$ parameters is given as:
\begin{equation*}
AIC = -2\log(L) + 2p \
\end{equation*}
\end{minipage}}
```
\
**Important:** The \underline{lower} the AIC, the \underline{better} the
model!

The AIC is a \alert{trade-off} between:

-   a high likelihood $L$ (good model fit)
-   few parameters $p$ (low model complexity)

## AIC$_c$: The AIC for low sample sizes

When the number of data points $n$ is small with respect to the number
of parameters $p$ in a model, the use of a
\alert{corrected AIC, the AIC$_c$} is recommended.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The {\bf corrected AIC} of a model with $n$ data points, likelihood $L$ and $p$ parameters is given as:
\begin{equation*}
AIC_c = -2\log(L) + 2p\cdot\frac{n}{n-p-1} \
\end{equation*}
\end{minipage}}
```
Burnham and Anderson **recommend to use AIC**$_c$ in general, especially
**when** $n/p<40$.

\

In the \alert{bodyfat example}, we have 243 data points and 13
parameters (including the intercept $\beta_0$), thus
$n/p = 143/13 \approx 19 <40$ $\Rightarrow$ use AIC$_c$ for model
selection!

## BIC, the brother/sister of AIC

Other information criteria were suggested as well. Another prominent
example is the \alert{BIC (Bayesian Information Criterion)}, which is
similar in spirit to the AIC.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The BIC of a model for $n$ data points with likelihood $L$ and $p$ parameters is given as:
\begin{equation*}
BIC = -2\log(L) + p \cdot \ln(n) \
\end{equation*}
\end{minipage}}
```
**Again:** The \underline{lower} the BIC, the \underline{better} the
model!

The only difference to AIC is the penalty for model complexity.

## Model selection with AIC/AICc/BIC

Given $m$ potential variables to be included in a model. Â 

-   In principle it is possible to minimize the AIC/AICc/BIC over all
    $2^m$ possible models. Simply fit all models and take the "best" one
    (lowest AIC).\
-   This is cumbersome to do "by hand". Useful to rely on implemented
    procedures in R, which search for the model with the lowest
    AIC/AICc/BIC.

\

-   \alert{Backward selection:} **Start with a large/full model.** In
    each step, **remove** the variable that leads to the largest
    improvement (smallest AIC/AICc/BIC). Do this until no further
    improvement is possible.
-   \alert{Forward selection:} **Start with a null model.** In each
    step, **add** the predictor that leads to the largest improvement
    (smallest AIC/AICc/BIC). Do this until no further improvement is
    possible.

## "Best" predictive model for bodyfat

Given the predictive nature of the bodyfat model, we search for the
model with minimal AICc, for instance using the `stepAIC()` function
from the `MASS` package:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(MASS)
library(AICcmodavg)
# Remember: r.bodyfat <- lm(bodyfat ~ ., d.bodyfat)
r.AIC <- stepAIC(r.bodyfat, direction= c("both"), trace= F, AICc= T)
AICc(r.bodyfat)
AICc(r.AIC)
```

$\rightarrow$ $\Delta$AICc= 5.52 in favour of `r.AIC`.

\scriptsize Note: Owen will also use `direction=c("forward")` and
`direction=c("backward")` in the BC videos.

## 

The model was reduced, with only 8 of the 12 initial predictor variables
retained:

```{r results='asis', echo = FALSE}
# tableRegression(r.AIC)
kable(summary(r.AIC)$coefficients, digits= 3)

```

**Note 1:** AICc minimization may lead to a model that retains variables
with relatively large $p$-values (*e.g.*, ankle).

**Note 2:** We could continue and *e.g.* include interactions,
transformations, *etc.*

## Cautionary note about the "best" predictive model

It is tempting to look at the coefficients and try to interpret what you
see, in the sense of "Increasing the weight by 1kg will cause a bodyfat
reduction by -0.75 percentage points."

However, the coefficients of such an optimized "best" model should **not
be interpreted**!

\

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
$\rightarrow$ {\bf Model selection may lead to biased parameter estimates: do not draw (biological, medical,..) conclusions from models that were optimized for prediction}
\end{minipage}}
```
\scriptsize See *e.g.*, Freedman1983, Copas 1983.

## Your aim is explanation?

"Explanation" means you want to interpret the regression coefficients,
95% CIs, and $p$-values. It is then often assumed that some sort of
causality ($x\rightarrow y$) exists.

In such a situation, you should formulate a \alert{confirmatory model}:

-   **Start with a clear hypothesis**
-   **Select your explanatory variables according to** \alert{a priori}
    knowledge.
-   Ideally formulate **only one** or a few model(s) **before you start
    analysing your data.**

## 

Confirmatory models have a long tradition in medicine. In fact, the main
conclusions in a study are only allowed to be drawn from the main model
(which needs to be specified even before data are collected):

```{=tex}
\begin{center}
\includegraphics[width=10.5cm]{pictures/claytonHills.png}
\end{center}
```
\scriptsize (chapters 27.1 and 27.2, Clayton Hills 1993)

## Confirmatory vs. exploratory

Any **additional analyses** that you potentially do with your data are
**exploratory**.

$\rightarrow$ Two types of **explanatory models/analyses**:

-   \alert{Confirmatory}:

    -   Clear hypothesis and **a priori** selection of regressors for
        ${y}$.

    -   **No variable selection!**

    -   Allowed to interpret the results and draw quantitative
        conclusions.

-   \alert{Exploratory}:

    -   Build whatever model you want, but the results should only be
        used to generate new hypotheses, *a.k.a.* "speculations".

    -   Clearly report the results as "exploratory".

## Interpretation of exploratory models?

Results from exploratory models can be used to generate new hypotheses,
\alert{not to draw causal conclusions}, or to over-interpret
effect-sizes.

\

In biological publications it is (unfortunately) still common practice
to draw conclusions from exploratory models, optimized using model
selection criteria (like AIC), as if the models were predictive!

$\rightarrow$ We illustrate why this is a problem with a (simulation)
example on the next slides.

## Model selection bias

**Aim of the example:**

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
To illustrate how model selection based on AIC can lead to biased parameters, and inflated effect sizes.
\end{minipage}}
```
**Procedure:**

1.  Randomly generate 100 data points for 50 explanatory variables
    ${x}^{(1)},\ldots, {x}^{(50)}$, and a response variable ${y}$:

    ```{r echo = T, eval = T}
    set.seed(123456)
    data<- data.frame(matrix(rnorm(100 * 51), ncol= 51))
    names(data)[51]<- "Y"
    ```

    `data` is a 100$\times$ 51 matrix, where the last column is the
    response. The **data are generated randomly and independently**, the
    covariates do not have any explanatory power for the response!

## 

2.  Fit a linear model of ${y}$ against all 50 variables
    \begin{equation*}
    y_i = \beta_0 + \beta_1 x_i^{(1)} + \ldots + \beta_{50}x_i^{(50)} + \epsilon_i \ .
    \end{equation*}

    ```{r echo=TRUE}
    r.lm<- lm(Y~., data)
    ```

    As expected, the distribution of the $p$-values is (more or less)
    uniform between 0 and 1, with none below 0.05:

    ```{r echo = FALSE, fig.width=4, fig.height=4, out.width="5cm", fig.align = 'center'}
    par(mfrow=c(1,1))
    hist(summary(r.lm)$coef[-1,4],freq=T,main="50 variables",xlab="p-values")
    ```

## 

3.  Use AICc minimization to obtain the "best predictive" model:

    ```{r}
    r.AICmin<- stepAIC(r.lm, direction= "both", 
                       trace= FALSE, AICc=TRUE)
    ```

    ```{r, fig.width=4, fig.height=4,out.width="5cm", fig.align='center', echo = FALSE}
    hist(summary(r.AICmin)$coef[-1,4],freq=T,main="18 variables of minimal AICc model",xlab="p-values")
    ```

    The distribution of the $p$-values is now skewed: many reach rather
    small values, with `r sum(summary(r.AICmin)$coef[-1,4]<0.05)` :
    $p<0.05$. This happened
    \alert{although none of the variables has any explanatory power!}

## 

**Main problem with model selection:**

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
When model selection is carried out based on objective criteria, effect sizes will  be too large, and the uncertainty too small
$\rightarrow$ you end up being overly confident about effects that are too large. 
\end{minipage}}
```
Thus:

Model selection procedures *should not be used* when the aim of the
analysis is explanation!

## Variable selection using $p$-values?

In many publications you might see that people use $p$-values to select
models instead. Also Stahel (Section 5.3) recommends this procedure.

However:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
Variable selection using $p$-values is an especially bad idea.
\end{minipage}}
```
\

\alert{$\rightarrow$ Please NEVER perform variable selection based on $p$-values$^{(\star)}$}

\
What is the problem?

\

\scriptsize $^{(\star)}$Even not when the aim is prediction.

## Importance is not reflected by $p$-values

A widely used practice to determine the "importance" of a term is to
look at the $p$ value ofthe $t$- or $F$-test and check whether it falls
below a certain threshold (usually $p<0.05$).

**However, there are a few problems with this approach:**

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}

* A small $p$-value does not necessarily imply that a term is (biologically, medically) interesting. Especially with large sample sizes even variables of minimal "real world" significance can reach statitistical significance. 

* When carrying out the tests with $H_0: \beta_j=0$ for all variables sequentially, one runs into a \alert{multiple testing problem} {\scriptsize (Remember the ANOVA lecture of week 6, slide 25-26).}

* You run a real risk of overfitting, in which the model fits noise specific to your dataset, rather than the general signal. This leads to poor generalization.

* Variables are sometimes \alert{collinear}, which leads to more uncertainty in the estimation of the respective regression parameters, and thus to larger $p$-values.

\end{minipage}}
```
## 

For all these reasons, we **disagree** with Stahel Section 5.2, second
part in paragraph d.

```{=tex}
\begin{center}
\includegraphics[width=10cm]{pictures/52d_2.png}
\end{center}
```
We also disagree with model selection based on $p$-values, as suggested
in Section 5.3, because:

-   This too will lead to model selection bias **Freedman 1983**.

-   $p$-values are even less suitable for model selection than
    AIC/AICc/BIC for the reasons mentioned on the previous slide.

## An explanatory model: Mercury example

The **research question** was:

"Gibt es einen Zusammenhang zwischen Quecksilber(Hg)-Bodenwerten von
WohnhÃ¤usern und der Hg-Belastung im KÃ¶rper (Urin, Haar) der Bewohner?"

-   *Hg concentration in urine* ($Hg_{urine}$) is the **response
    variable**.

-   *Hg concentration in the soil* ($Hg_{soil}$) is the **explanatory
    variable** of interest.

\alert{In addition}, the following variables were monitored for each
person, as they might also influence mercury levels in a person's body:

*smoking status; number of amalgam fillings; age; number of monthly fish
meals; indicator if fish was eaten in the last 3 days; mother vs child;
indicator if vegetables from garden are eaten; migration background;
height; weight; BMI; sex; education level.*

**Thus: 13 additional** (potentially confounding) **variables!**

## How many variables can I include in my model?

**Crude rule of thumb:**

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
Include no more than $n/10$ (dummy) variables in your linear model, where $n$ is the number of data points.
\end{minipage}}
```
In the mercury example there are 156 individuals, so a **maximum of 15
(dummy) variables** can be included in the model.

\

**Remarks:** - Categorical variables with $k$ levels require $k-1$ dummy
variables. For example, if \`education level' has $k=3$ categories,
$k-1=2$ parameters are used up.\
- Whenever possible, the model should **not be blown up** unnecessarily.
Even if there are many data points, the use of too many variables may
lead to **overfitting**.\
$\rightarrow$ \scriptsize See
\href{https://en.wikipedia.org/wiki/Overfitting}{https://en.wikipedia.org/wiki/Overfitting}.

## 

In the mercury study, the following variables were included using *a
priori* knowledge/expectations:\

\

```{=tex}
\begin{tabular}{llll}
Variable & Meaning & type & transformation\\
\hline
Hg$\_$urin & Hg conc.\ in urine (response) & continuous & $\log$\\ 
Hg$\_$soil & Hg conc.\ in the soil  & continuous & $\log$\\
vegetables & Eats vegetables from garden? & binary\\
migration & Migration background & binary \\
smoking & Smoking status & binary \\
amalgam & No.\ of amalgam fillings & count & $\sqrt{.}$ \\
age & Age of participant &  continuous\\ 
fish & Number of fish meals/month & count  & $\sqrt{.}$\\
last$\_$fish & Fish eaten in last 3 days? &   binary\\
mother & Mother or child?  & binary\\
mother:age & Interaction term & binary:continuous\\
\end{tabular}
```
## 

Let us now fit the full model in R:

```{r fig.height=5, out.width="5cm", echo = FALSE}
d.hg <- read.table(here("3_datasets/hg_urine_fuzzed.csv"), header=T, sep=",")
d.hg["106","amalgam_quant"] <- 5 # correct for the outlier
d.hg <- d.hg[-11]
names(d.hg) <- c("Hg_urin", "Hg_soil", "vegetables","migration", "smoking","amalgam", "age", "fish","last_fish","mother")
```

```{r, echo=FALSE}
r.lm1 <- lm(log10(Hg_urin) ~ log10(Hg_soil) + vegetables + migration + smoking + 
             sqrt(amalgam) + age * mother + sqrt(fish) + last_fish,d.hg)
```

\scriptsize

```{r results="asis", echo = FALSE}
# tableRegression(r.lm1)
kable(summary(r.lm1)$coefficients, digits= 3)
```

-   The $p$-value for mercury in soil, $\log_{10}(Hg_{soil})$, is rather
    high: p=`r format(summary(r.lm1)$coef[2,4],2,2,2)`.

## 

Always check the model, *e.g.* (see Lecture 5):

```{r fig.width=7, fig.height=4,out.width="8cm", echo=FALSE, message=FALSE,warning=FALSE, fig.align='center'}
library(ggfortify)
autoplot(r.lm1,which=c(1,2),smooth.col=NA)
```

This looks ok, no need to improve the model from this point of view.

## 

Once we've convinced ourselves the model can be trusted, we can ask
questions like:

-   Which of the terms in our model are **important/relevant**?
-   Are there **additional terms** that might be relevant?
-   Can we find **other patterns** in the data?

\

$\rightarrow$ We could continue to analyse the data in an
\alert{exploratory} manner. Such additional models can be useful to
generate new hypotheses.

## 

For example, it might be tempting to check whether there are models with
a lower AICc.

```{r echo = FALSE, warning=FALSE, message=FALSE}
r.lm0 <- lm(log10(Hg_urin) ~ log10(Hg_soil) + vegetables + migration + smoking + 
             sqrt(amalgam) + age + mother + sqrt(fish) + last_fish,d.hg)
```

We could fit models from which certain terms are omitted. Let's try a
model without the interaction $mother\cdot age$ (denoted as `r.lm0`).

```{r echo = TRUE}
AICc(r.lm0)
AICc(r.lm1)
```

The AICc increases quite a bit, confirming that the term is relevant.

## 

In contrast, a model from which the binary \emph{migration} variable is
omitted, results in a reduced AICc:

```{r echo = TRUE}
r.lm0<- lm(log10(Hg_urin)~ log10(Hg_soil) + vegetables + smoking + 
             sqrt(amalgam) + age * mother + sqrt(fish) + last_fish,
             d.hg)
AICc(r.lm0)
```

**But:** Given that the mercury model is an **explanatory, confirmatory
model**, we should not remove a variable (*e.g.*, migration) simply
because it reduces AICc.

$\rightarrow$ Therefore, given the *a priori* selection of variables and
the model validation results, the model from slide 34 was used in the
final publication (Imo *et al.* 2017).

$\rightarrow$ Any further analyses with other models needs to be
labelled as \alert{exploratory}.

## Another complication: Collinearity

\small (See Stahel chapter 5.4)

Given a set of variables
${x^{(1)}}, {x^{(2)}}, {x^{(3)}}, ...,{x^{(p)}}$. If it is possible to
write one of the variables as a \alert{linear combination of the others}
\begin{equation*}
x_i^{(j)} = \sum_{k\neq j} c_k x_i^{(k)} \quad \text{for all} \quad  i=1,...,n
\end{equation*}

the set of variables is said to be \alert{collinear}.

\

**Examples:**

-   Three vectors in a 2D-plane are always collinear.

-   Any set of variables for which one can be written as a linear
    combination of the two others:
    $x^{(j)} = c_1\cdot x^{(1)} + c_2 \cdot x^{(2)}$.

## 

In statistics, the expression "collinearity" is also used when such a
collinearity relationship is \emph{approximately} true. For example,
when two variables ${x^{(1)}}$ and ${x^{(2)}}$ are highly correlated.

**What is the problem with collinearity?**

A simple (and extreme) example to understand the point: Assume two
variables are identical ${x^{(1)}}={x^{(2)}}$. In the regression model
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i \ ,
\end{equation*}

the slope coefficients $\beta_1$ and $\beta_2$
\alert{cannot be uniquely determined} (there are many equally "optimal"
solutions to the equation)!

When the variables are collinear, this problem is less severe, but the
$\beta$ coefficients can be estimated \alert{less precisely}

$\rightarrow$ standard errors too high.

$\rightarrow$ $p$-values too large.

## Detecting collinearity

The \alert{Variance Inflation Factor} (VIF) is a measure of
collinearity. It is defined for each variable ${x^{(j)}}$ as:
\begin{equation*}
VIF_j = \frac{1}{1-R_j^2} \qquad
\end{equation*} where $R_j^2$ is the $R^2$ of the regression of
${{x^{(j)}}}$ against all other variables (Note: if $R_j^2$ is large,
this means large collinearity and thus a large VIF).

**Examples**

-   $R^2_j=0$ $\rightarrow$ no collinearity $\rightarrow$ VIF=1/1 = 1.
-   $R^2_j=0.5$ $\rightarrow$ some collinearity $\rightarrow$
    VIF=1/(1-0.5) = 2.\
-   $R^2_j=0.9$ $\rightarrow$ high collinearity $\rightarrow$
    VIF=1/(1-0.9) = 10.

## What to do against collinearity

-   \alert{Avoid} it, *e.g.* in experiments.
-   \alert{Do not include a variable} with an unacceptably high $R^2_j$
    or $VIF_j$. There are many critical VIF-values in the literature,
    ranging from 3 to 10.
-   Be \alert{aware} and interpret your results with appropriate care.
-   See also Stahel 5.4(i) for a "recipe".

\
**Note:** We would probably not care much about collinearity in a
predictive model. If collinearity was a problem, AIC/AICc/BIC would
anyway select a subset where some collinearity is eliminated (because
model complexity is balanced against model fit).

## Recommended procedure for explanatory models I

Before you start:

-   **Think about a suitable model**. This includes the model family
    (*e.g.*, a normal linear model), but also variables that are of
    interest, using **a priori** knowledge.\

-   Devise a strategy on how to handle when modelling assumptions are
    not met.

    -   What kind of variable transformations would you try, in which
        order, and why?
    -   What model simplifications will be considered if it is not
        possible to fit the intended model?
    -   How to deal with outliers?
    -   How to treat missing values in the data?
    -   How to treat collinear variables?
    -   ...

It is advisable to write your strategy down as a "protocol" before doing
any analyses.

## Recommended procedure for explanatory models II

Analyze the data following your "protocol":

-   Fit the model and check if all assumptions are met.
-   If assumptions are not met, **adapt the model** according to your
    protocol.
-   Interpret model coefficients (effect sizes) and the $p$-values
    correctly (next week!).

\
Following the analysis that was specified in the "protocol":

-   Any additional analyses that you did not specify in advance, are
    exploratory!

## One more thing: Occam's Razor

This principle essentially states that an **explanatory model** should
not be made more complicated than necessary.

This is also known as the \alert{principle of parsimony} (Prinzip der
Sparsamkeit):

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
Systematic effects should be included in a model \textbf{only} if there is knowledge or convincing evidence for the need of them.
\end{minipage}}
```
```{=tex}
\href{https://de.wikipedia.org/wiki/Ockhams_Rasiermesser}
{\beamergotobutton{See Wikipedia for ``Ockham's Rasiermesser''}}
```
## Summary

-   Model/variable selection is difficult and controversial.
-   There are different approaches for predictive or explanatory models.
-   Explanatory models are either confirmatory or exploratory.
-   AIC, AIC$_c$, BIC: balance between model fit and model complexity.
-   Automatic model selection leads to biased parameter estimates and
    $p$-values.
-   Therefore, automatic model selection procedures are inappropriate
    for explanatory models.
-   $P$-values should not be used for model selection, not even for
    predictive models.

## 

\footnotesize

References:

Brewer, M. J., A. Butler, and S.L. Cooksley (2016). The relative
performance of $AIC$, $AIC_c$ and $BIC$ in the presence of unobserved
heterogeneity. *Methods in Ecology and Evolution 7*, 679-692.

Burnham, K.P. and D.R. Anderson (2002). *Model selection and multimodel
inference: a practical information-theoretic approach.* New York:
Springer.

Clayton, D. and M. Hills (1993). *Statistical Models in Epidemiology.*
Oxford: Oxford University Press.

Copas. J.B. (1983). Regression, prediction and shrinkage. *Journal of
the Royal Statistical Society. Series B (Statistical Methodology) 45*,
311-354.

Freedman, D.A. (1983). A note on screening regression equations. *The
American Statistician 37,* 152-155.

Imo, D., S. Muff, R. Schierl, K. Byber, C. Hitzke, M. Bopp, M. Maggi, S.
Bose-O'Reilly, L. Held, and H. Dressel (2017). Risk assessment for
children and mothers in a mercury-contaminated area using
human-biomonitoring and individual soil measurements: a cross-sectional
study. *International Journal of Environmental Health Research 28,*
1-16.
