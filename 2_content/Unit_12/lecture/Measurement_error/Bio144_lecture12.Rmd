---
title: "Lecture 12: Measurement Error"
subtitle: "BIO144 Data Analysis in Biology"
author: "Owen Petchey, Stephanie Muff & Erik Willems"
institute: "University of Zurich"
date: "27 May, 2024"
output:
  beamer_presentation:
    includes:
      in_header: ../../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## A request from your TA's

"Help us help you... (even?) better next year!"
\

```{r out.width='40%', fig.align='center', echo=FALSE}
knitr::include_graphics("images/qr-code.png")
```

## Overview

* Measurement error (ME) in one or more explanatory variable(s) ($x$)
* Effects of ME on model parameters
* When do you have to worry?
* An example of a method to correct for ME 

## Course material covered today

The lecture material is partially based on:

* Chapter 6.1 in "Lineare regression" (BC reading)


## Sources of measurement error (ME)

* **Measurement imprecision** in the field or in the lab (length, weight, blood pressure, etc.).
* Errors due to **incomplete** or **inaccurate observations** (e.g., self-reported dietary aspects, health history).
* Rounding error, digit preference.
* **Classification error** (e.g., exposure or disease classification).
* \ldots

\begin{center}
\textcolor{teal}{"Error" is often used synonymous to "uncertainty".}
\end{center}

## Yet another assumption...

It is an \alert{implicit assumption} of most statistical tests that explanatory variables are measured or estimated \alert{without error}. This is true for: 

* correlation
* regression and ANOVA
* Generalized Linear Models (e.g. Poison and binomial GLMs)\
\

Violation of this assumption may lead to:

* biased parameter estimates, standard errors, and thus wrong $p$-values
* incorrect (relative) variable importance, and thus *even more* misguided conclusions

\begin{center}
\textcolor{teal}{Standard statistics textbooks often do not mention this assumption at all!}
\end{center}

## Classical measurement error

A very common type of error:

Let \alert{$x_i$} be the \alert{correct but unobserved} variable and $w_i$ the observed variable with error $u_i$. Then the **classical ME model** is: 
$$w_i = x_i + u_i, \qquad u_i \sim N(0,\sigma^2_{u})$$

```{r fig.width=1.5, fig.height=1.1, fig.align='center', echo = FALSE}
par(mar=c(0.1,0.1,0.1,0.1))
tx<-seq(-4,4,0.01)
par(mfrow=c(1,1))
plot(x = tx, dnorm(tx,0,1),type="l",xaxt="n",yaxt="n",xlab="",ylab="")
abline(v=0,lty=2,lwd=0.5)
text(0,0.02,labels=expression(x[i]),cex=0.6)
text(-1.5,0.3,labels=expression(w[i]),cex=0.6)
```
**Examples:**   Inaccurate measurements of a concentration, a mass, a length etc. $\rightarrow$ the observed value $w_i$ varies around the true value $x_i$.



## Illustration of the problem


Find regression parameters $\beta_0$ and $\beta_x$ for the model with explanatory variable $\mathbf{x}$:


$$y_i = 0 + 1 \cdot x_i + \epsilon_i, \qquad \epsilon_i \sim N(0,\sigma^2)$$
\vspace{1mm}
\small
```{r, message=FALSE, echo = FALSE}
library(ggplot2)
set.seed(84522)
n <- 100
beta_0 <- 0
beta_1 <- 1
epsilon <- rnorm(n, 0, 0.2)
x <- rnorm(n, 0, 1)
u <- rnorm(n, 0, 1)
w <- x + u
# Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)
```
\normalsize

\small
```{r, message=FALSE, fig.width=3, fig.height=1.8, fig.align='center', echo=FALSE}
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,col="X")) +  
  geom_smooth(method="lm") + geom_point(size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) +
  xlab("X") +
  xlim(c(-3.5,3.5)) +
  ylim(c(-2.7,2.7)) +
  theme_bw()
```
\normalsize



## Illustration of the problem


However, assume that only an erroneous proxy $\mathbf{w}$ is observed with classical ME


$$w_i = x_i + u_i, \qquad u_i \sim N(0,\sigma_u^2), \qquad \sigma^2_u = \sigma^2_x$$
\vspace{1mm}
\small
```{r, message=FALSE, fig.width=3, fig.height=2, fig.align='center', echo = FALSE}
w <- x + u ## error added to x in this step

## Classical
y <- beta_0 + beta_1*x + epsilon
m1 <- lm(y~x)

## Now with added error
m2 <- lm(y~w)
```
\normalsize


\small
```{r, message=FALSE, fig.width=3, fig.height=1.8, fig.align='center', echo = FALSE}
cols <- c("X"="black","W"="red")
ggplot(mapping=aes(x=x,y=y,color="X")) +  geom_smooth(method="lm") +
  geom_point(size=0.9) + 
  geom_smooth(mapping=aes(x=w,y=y,color="W"),method="lm") + 
  geom_point(mapping=aes(x=w,y=y,color="W"),size=0.9) + 
  scale_colour_manual(name="Variable",values=cols) + xlab("X or W") +
  theme_bw()
```
\normalsize

## A tool you can play around with...

[\beamergotobutton{Illustration in a browser application} ](https://stefaniemuff.shinyapps.io/MEC_ChooseL/)

```{r out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}
knitr::include_graphics("images/me_browser_spp.png")
```


##  The "Triple Whammy of Measurement Error"

(Carroll et al. 2006)


1. \alert{Bias}ed parameter estimates

2. \alert{Loss of power} to detect signals

3. \alert{Masks important features} of the data, making graphical model inspection difficult


## How to correct for ME?

* Generally, to correct for the error you need an **error model** and knowledge of the **error model parameters**.

**Example:** If classical error \alert{$w_i = x_i + u_i$ with $u_i \sim N(0,\sigma_u^2)$} is present, knowledge of the **error variance** \alert{$\sigma_u^2$} is required.

**Strategy**: Take repeated measurements to estimate the error variance!

* In \alert{simple cases}, formulas for the bias exist.

* In most cases, such simple relations don't exist, and dedicated error modelling methods are needed.


## Attenuation in normal linear regression

Given the simple linear regression equation $y_i = \beta_0 + \beta_x x_i + \epsilon_i$ with $w_i = x_i + u_i$. Assume that $w_i$ instead of $x_i$ is used in the regression:
$$y_i = \beta^\star_0 + \beta^\star_x w_i + \epsilon_i$$


The **naive slope parameter** $\beta_x^\star$ underestimates the true slope $\beta_x$ by **attenuation factor** $\lambda$: 
$$\beta_x^\star =\underbrace{ \left(\frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2}\right)}_{=\lambda} \beta_x$$
$\rightarrow$ knowing $\sigma_u^2$ and $\sigma_x^2$, the correct slope can be retrieved!

**Example:** $\sigma_x^2 = 5$, $\sigma_u^2 = 1$, $\rightarrow$  $\lambda = \frac{5}{5 + 1} = 0.83$




## Error modeling

**Two common approaches**:

* **SIMEX**: SIMulation EXtrapolation, a heuristic and intuitive idea.
* **Bayesian methods**: Information about the error enters the model as a $prior$.
\

\colorbox{lightgray}{\begin{minipage}{13.5cm}
{Both, however, require that the \alert{error model}, and its respective parameters (e.g., $\sigma_u^2$) \alert{are known!}} 
\end{minipage}}
\

Thus, information about the error mechanism is essential, and potential sources of error must be identified at the planning stages of a study!


## SIMEX: An intuitive idea

Suggested by Cook & Stefanski (1994), SIMEX takes a two-step approach:

1. **Simulation phase:** The error in the data is \alert{progressively aggravated} in order to determine how the model parameter of interest is affected.
2. **Extrapolation phase:** The simulated trend is then \alert{extrapolated back} to a hypothetical error-free value of the model parameter.




## Illustration of the SIMEX idea

Parameter of interest: $\beta_x$ (e.g. a regression slope).

Problem: The respective explanatory variable $x$ was estimated with error: $$w=x+u\ , \quad u\sim N(0,\sigma_u^2)$$
\small
```{r, message=FALSE, echo = FALSE}
set.seed(212356)
sigmax <- 1    # variance in x
sigmau <- 0.25 # measurement error

B <- 1 # True Beta
n <- 4 # number of measurements

# Additional observation error (simulated)
s <- seq(0, 3, 0.3)/n # note s=0 is the observed

# Observed Betas ~ sigmau
Bo <- B*sigmax/(sigmax + sigmau*(1+s))
```
\normalsize


\small
```{r, message=FALSE, fig.width=3, fig.height=1.8, fig.align='center', echo = FALSE}
# plot the simulated data
par(mar=c(3,4,0.5,1), mgp=c(2.2,0.8,0))
plot((1+s), Bo, ylim=c(0.6,1), xlim=c(0,2), xlab=expression(sigma[u]^2),
     ylab=expression(beta[x]))
abline(v=1, lty=3)
points((1+s[1]),Bo[1],pch=18,cex=2.5,col=2) # highlight Bo
p <- recordPlot()
```
\normalsize

## Extrapolate to obtain an estimate of the corrected beta
\small
```{r, message=FALSE, fig.width=3, fig.height=1.8, fig.align='center', echo = FALSE}
print(p) # add plot again from above
model <- lm(Bo ~ poly(s,2)) # fit a quadratic line
newx <- seq(-1,1,0.1) # fit line to demonstrate fit
lines((1+newx), predict(model,newdata=data.frame(s=newx)), lty=2)
Be <- predict(model,newdata=data.frame(s=-1)) # B at s=-1 -> sigmau=0
points(0,Be,pch=18,cex=2.5,col=4) # add to plot
legend("topright",legend=c("Naive","Corrected"),pch=18,col=c(2,4),cex=0.7)
```
\normalsize

## Example of SIMEX use (part 1)

Let's consider a linear regression model

$y_i = \beta_0 + \beta_x x_i + \beta_z z_i + \epsilon_i$ , $\epsilon_i = N(0,\sigma^2)$

with

* $\mathbf{y}=(y_1,\ldots, y_{100})^\top$: variable with \% Bodyfat of 100 individuals.
* $\mathbf{x}=(x_1,\ldots, x_{100})^\top$ the BMI of the individuals.

**Problem:** The BMI was self-reported and thus suffers from measurement error. Not $x_i$ was observed, but rather 
$$w_i = x_i + u_i \ , \quad u_i \sim N(0,4)$$ 

* $\mathbf{z}=(z_1,\ldots, z_{100})^\top$ a binary explanatory variable that indicates if the $i$-th person was a male ($z_i=1$) or female ($z_i=0$).


$\rightarrow$ Apply the SIMEX procedure!



## Simulated example

\small
```{r}
set.seed(3243445)
x<- rnorm(100, 24, 4)
w<- x + rnorm(100, 0, 2)
z<- ifelse(x > 25, rbinom(100, 1, 0.7), rbinom(100, 1, 0.3))

y<- -15 + 1.6*x - 2*z + rnorm(100, 0, 3)

data<- data.frame(cbind(w, z, y))
  names(data)<- c("BMI", "sex", "bodyfat")
```
\normalsize

## Check out the results

Use the error-prone BMI variable to fit a "naive" regression:
\small
```{r}
r.lm <- lm(bodyfat ~ BMI + sex, data, x= TRUE)
summary(r.lm)$coef
```
\normalsize


## Now run simex procedure
Then run the SIMEX procedure using the `simex()` function:
\small
```{r warning=FALSE}
library(simex)
r.simex <- simex(r.lm,
                 SIMEXvariable = "BMI",
                 measurement.error= sqrt(4),
                 lambda= seq(0.1, 2.5, 0.1),
                 B = 100,
                 fitting.method= "quadratic")
summary(r.simex)$coef$asymptotic
```
\normalsize


## Graphical results with quadratic extrapolation function:
\small
```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=2.4, fig.align='center'}
par(mfrow = c(1,3), mar = c(4,4,2,1))
plot(r.simex)

```
\normalsize

**Note:** The `sex` variable has \emph{not} been mismeasured, nevertheless it is affected by the error in BMI!
**Reason:** `sex` and BMI are correlated.




## Practical advice

* Think about measurement error **before** you start collecting your data.
* Ideally, take **repeated measurements**, maybe of a subset of data points
* Figure out if error is a problem and what the bias in your parameters might be. You might need simulations to find out.
* If needed, model the error. **Seek help from a statistician!**





## References



Carroll, R. J., D. Ruppert, L. A. Stefanski, and C. M. Crainiceanu (2006). Measurement Error in Nonlinear Models: A Modern Perspective (2 ed.). Boca Raton: Chapman & Hall.

Cook, J. R. and L. A. Stefanski (1994). Simulation-extrapolation estimation in parametric measurement error models. Journal of the American Statistical Association 89, 1314–1328.



