\documentclass[english,9pt,aspectraio=169]{beamer}
\usepackage{etex}
\usetheme{uzhneu-en-informal}
%\usepackage{uarial}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\RequirePackage{graphicx,ae}
\usepackage{bm}
\usepackage{fancybox,amssymb,color}
\usepackage{pgfpages}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{animate}
\usepackage{numprint}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{amsmath,natbib}
\usepackage{mathbbol}
\usepackage{babel}
%\usepackage{SweaveSlides}
\usepackage{multicol}
\usepackage{xcolor}


\usetheme{uzhneu-en-informal}
\DeclareMathOperator{\po}{Poisson}
\DeclareMathOperator{\G}{Gamma}
\DeclareMathOperator{\Be}{Beta}
\DeclareMathOperator{\logit}{logit}
\def\n{\mathop{\mathcal N}}

\definecolor{Gray}{RGB}{139,137,137}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{Green}{rgb}{0,0.8,0.3}
\definecolor{lightgreen}{rgb}{0,0.7,0.3}
\definecolor{Blue}{rgb}{0,0,1}
\def\myalert{\textcolor{darkred}}
\def\myref{\textcolor{Gray}}
\setbeamercovered{invisible}

\renewcommand{\baselinestretch}{1.2}
\beamertemplateballitem
\DeclareMathOperator{\cn}{cn} % Copy number
\DeclareMathOperator{\ccn}{ccn} % common copy number
\DeclareMathOperator{\p}{p} % common copy number
\DeclareMathOperator{\E}{E} % common copy number
\DeclareMathOperator{\given}{|} % common copy number
\def\given{\,|\,}
\def\na{\tt{NA}}
\def\nin{\noindent}
\pdfpageattr{/Group <</S /Transparency /I true /CS /DeviceRGB>>}
\def\eps{\varepsilon}

\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Wahrscheinlichkeitsma√ü
\def\eps{\varepsilon}
\def\logit{\text{logit}}
%\newcommand{\E}{\mathsf{E}} % Erwartungswert
\newcommand{\Var}{\text{Var}} % Varianz
\newcommand{\NBin}{\text{NBin}}
\newcommand{\Po}{\text{Po}}
\newcommand{\N}{\mathsf{N}}
\newcommand{\hl}{\textcolor{blue}}

\newcommand{\ball}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{1ex}{1ex}
\usebeamercolor[fg]{item projected}

{\pgftransformscale{1.75}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%
\usepackage{multicol}
\newcommand{\ballsmall}[1]{\begin{pgfpicture}{-1ex}{-0.65ex}{.2ex}{.2ex}

{\pgftransformscale{1}\pgftext{\normalsize\pgfuseshading{bigsphere}}}
{\pgftransformshift{\pgfpoint{0pt}{0.5pt}}
\pgftext{\usebeamerfont*{item projected}{#1}}}
\end{pgfpicture}}%




\begin{document}
%\SweaveOpts{width=6,height=4}
\fboxsep5pt


<<setup, include=FALSE, cache=FALSE, results='hide'>>=
library(knitr)
## set global chunk options
opts_chunk$set(fig.path='figure/', 
cache.path='cache/', fig.align='center', 
fig.show='hold', par=TRUE, fig.align='center', cache=FALSE, 
message=FALSE, 
warning=FALSE,
echo=FALSE, out.width="0.4\\linewidth", fig.width=6, fig.height=4.5, size="scriptsize", width=40)
## opts_chunk$set(fig.path='figure/', 
## cache.path='cache/', echo=FALSE, out.width="0.55\\linewidth",fig.width=6,fig.height=6, fig.align="center", message = FALSE)
## par(mar=c(1,4.1,4.1,1.1))
opts_chunk$set(purl = TRUE)
knit_hooks$set(purl = hook_purl)
options(size="scriptsize")
opts_chunk$set(message = FALSE)
@

\frame{
\title[]{ \centering \Huge Kurs Bio144: \\
Datenanalyse in der Biologie}%\\[.3cm]
\author[Stefanie Muff, Owen L.\ Petchey]{\centering Stefanie Muff (Lecture) \& Owen L.\ Petchey (Practical)}
\date[]{Lecture 3: Simple linear regression\\ 4.~March 2019}


\maketitle
}


\frame{\frametitle{Overview}
\begin{itemize}
\item Introduction of the linear regression model
\item Parameter estimation
\item Simple model checking
\item Goodness of the model: Correlation and $R^2$
\item Tests and confidence intervals
\item Confidence and prediction ranges
\end{itemize}
}



\frame{\frametitle{Course material covered today}

The lecture material of today is based on the following literature:\\[4mm]

\begin{itemize}
\item Chapter 2 of \emph{Lineare Regression}, p.7-20 (Stahel script),
\item Alternatively, chapters 13.1 - 13.4 in the Stahel book ``Statistische Datenanalyse''.\\[6mm]
\end{itemize}

 
}


\frame[containsverbatim]{\frametitle{The body fat example}
Remember: Aim is to find prognostic factors for body fat, without actually measuring it. \\
Even simpler question: How good is BMI as a predictor for body fat?\\[2mm]
 

<<read.bodyfat,echo=F,eval=T>>=
path <- "../../data_examples/bodyFat/"
d.bodyfat <- read.table(paste(path,"bodyfat.clean.txt",sep=""),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
@


 
\begin{center}
<<bmi2, eval=T,fig.width=3, fig.height=2.5,warning=FALSE,echo=F,out.width="7cm">>=
library(ggplot2)
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")
@
\end{center}
 

}


\frame[containsverbatim]{\frametitle{Linear relationship}
\begin{itemize}
\item The most simple relationship between an \emph{explanatory variable} ($X$) and a \emph{target/outcome variable} ($Y$) is a linear relationship. All points $(x_i,y_i)$, $i= 1,\ldots, n$, on a  straight line follow the equation
$$y_i = \alpha + \beta x_i\ .$$

\item Here, $\alpha$ is the \myalert{axis intercept} and $\beta$ the \myalert{slope} of the line. 
$\beta$ is also denoted as the regression coefficient of $X$.\\[4mm]


\item If $\alpha=0$ the line goes through the origin $(x,y)=(0,0)$.

\item \alert{Interpretation} of linear dependency: proportional increase in $y$ with increase (decrease) in $x$.
\end{itemize}
}



\frame[containsverbatim]{\frametitle{}

But which is the ``true'' or ``best'' line?


\begin{center}
<<eval=T,fig.width=3.5, fig.height=3,out.width="7cm">>=
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")  + 
  geom_abline(intercept = -25, slope = 1.7, color="red",   size=0.6) +
  geom_abline(intercept = -35, slope = 2.1, color="green",    size=0.6) +
geom_abline(intercept = -36, slope = 2.25, color="blue",    size=0.6) 
@
\end{center}

{\bf Task:} Estimate the regression parameters $\alpha$ and $\beta$ (by ``eye'') and write them down.
}

 

\frame[containsverbatim]{\frametitle{}
It is obvious that 
\begin{itemize}
\item the linear relationship does not describe the data perfectly.
\item another realization of the data (other 243 males) would lead to a slightly different picture.\\[10mm]
\end{itemize}

$\Rightarrow$ We need a {\bf model} that describes the relationship between BMI and bodyfat. \\[8mm]

}


\frame{\frametitle{The simple linear regression model}

\colorbox{lightgray}{\begin{minipage}{10cm}
In the linear regression model the dependent variable $Y$ is related to the independent variable $x$ as
%
$$Y = \alpha + \beta x + \epsilon \ , \qquad \epsilon \sim \N(0,\sigma^2) \ .$$
\end{minipage}}
~\\[2mm]
In this formulation $Y$ is a random variable $Y \sim \N(\alpha + \beta x, \sigma^2$) where
$$Y \quad= \quad \underbrace{\text{\ expected value\ }}_{\E(Y) = \alpha + \beta x} \quad + \quad \underbrace{\text{\ random error\ }}_{\epsilon}  \ .$$

\vspace{2mm}
Note:

\begin{itemize}
\item The model for $Y$ given $x$ has \myalert{three parameters}: $\alpha$, $\beta$ and $\sigma^2$ .
\item $x$ is the \myalert{independent}/ \myalert{explanatory} / \myalert{regressor} variable.
\item $Y$ is the \myalert{dependent} / \myalert{outcome} / \myalert{response} variable. \\
\end{itemize}

}



\frame{\frametitle{}

{\bf Note:} 
\begin{itemize}
\item The linear model propagates the most simple relationship between two variables. When using it, please always think if such a relationship is meaningful/reasonable/plausible.\\[2mm]
\item Always look at the data \alert{before} you start with model fitting.
\end{itemize}
}

\frame{\frametitle{Visualization of the regression assumptions}
 
The assumptions about the linear regression model lie in the error term $$\epsilon \sim \N(0,\sigma^2) \ . $$ 

\vspace{-2mm}
\includegraphics[width=11cm]{pictures/regrAssumptions.jpg}

Note: The true regression line goes through $\E(Y)$.\\
}

\frame[containsverbatim]{\frametitle{Insight from data simulation}
{\scriptsize (Simulation are \emph{always} a great way to understand statistics!!)}\\[2mm]

Generate an independent (explanatory) variable $\bm{x}$ and {\bf two} samples of a dependent variable $\bm{y}$ assuming that
$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim \N(0,0.5^2) \ .$$



\begin{center}
<<eval=T,fig.width=3.5,fig.height=3.3>>=
set.seed(134539)
par(mar=c(4,4,1,1))
x <- runif(25,-2,2)
y1 <- 4 - 2*x + rnorm(25,0,sd=0.5)
y2 <- 4 - 2*x + rnorm(25,sd=0.5)
plot(x,y1,ylim=c(min(c(y1,y2)),max(c(y1,y2))),ylab="y")
points(x,y2,col=2)
abline(c(4,-2))
legend("topright",legend=c("sample 1","sample 2"),col=1:2, pch=1)
@
\end{center}
%Note the different y-coordinates for the two samples.\\[2mm]

$\rightarrow$ Random variation is always present. This leads us to the next question.

}

 



\frame{\frametitle{Parameter estimation}
In a regression analysis, the task is to estimate the \myalert{regression coefficients} $\alpha$, $\beta$ and the \myalert{residual variance} $\sigma^2$ for a given set of $(x,y)$ data.\\[4mm]

\begin{itemize}
\item {\bf Problem:} For more than two points $(x_i,y_i)$, $i=1,\ldots, n$, there is generally no perfectly fitting line.\\[4mm]

\item {\bf Aim:} We want to find the parameters $(a,b)$ of the best fitting line $Y = a + b x$. \\[4mm]

\item {\bf Idea:} Minimize the deviations between the data points $(x_i,y_i)$ and the regression line. \\[8mm]
\end{itemize}

But how? 

}



\frame{\frametitle{Should we minimize these distances...}

\vspace{-1cm}
\begin{center}
<<eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm">>=
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
x1 <- -0.66
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x1,4-2*(x1),col=2,lwd=2)
#text(-0.4, y=5.6,  labels=expression(d[i]),cex=1.3)
@
\end{center}

}


\frame[containsverbatim]{\frametitle{... or these?}
\vspace{-1cm}
\begin{center}
<<eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm">>=
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x[11],4-2*x[11],col=2,lwd=2)
#text(-0.35, y=5.5,  labels=expression(e[i]),cex=1.3)
@
\end{center}
}

\frame[containsverbatim]{\frametitle{Or maybe even these?}
\vspace{-1cm}
\begin{center}
<<eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm">>=
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],(y[11]-4)/(-2),y[11],col=2,lwd=2)
#text(-0.35, y=5.5,  labels=expression(e[i]),cex=1.3)
@
\end{center}
}

\frame{\frametitle{Least squares}
For multiple reasons (theoretical aspects and mathematical convenience), the parameters are estimated using the \myalert{least squares} approach. In this, yet something else is minimized: \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
The parameters $\alpha$ and $\beta$ are estimated such that the sum of \myalert{squared vertical distances} (sum of squared residuals)

$$SSE = \sum_{i=1}^n e_i^2 \ , \qquad \text{where} \quad e_i = y_i - \underbrace{(a + b x_i)}_{=\hat{y}_i} $$

is being minimized.
\end{minipage}}

~\\[4mm]
{\bf Note:} $\hat y_i = a + b x_i$ are the \myalert{predicted values}.
}



\frame[containsverbatim]{\frametitle{So we minimize the sum of these areas!}
\vspace{-1cm}
\begin{center}
<<eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="10cm">>=
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
dd <- 0.38
from_x <- c(x[11],x[11],x[11]+dd,x[11] + dd) 
from_y <- c(y[11],(4-2*x[11]),(4-2*x[11]),y[11])

to_x <- c(x[11],x[11] + dd,x[11]+ dd,x[11])
to_y <- c(4-2*x[11],4-2*x[11],y[11], y[11])

plot(x,y)
abline(c(4,-2),lwd=2)
polygon(from_x,from_y,to_x,to_y,col=2,lwd=2)
#segments(from_x,from_y,to_x,to_y,col=2,lwd=2)
@
\end{center}
}


\frame{\frametitle{Least squares estimates}
For a given sample $(x_i,y_i), i=1,..,n$, with mean values $\overline{x}$ and $\overline{y}$, the least squares estimates $\hat\alpha$ and $\hat\beta$ are computed as \\[2mm]
\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{eqnarray*}
\hat\beta &=& \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)} \ , \\[4mm]
\hat\alpha &=& \overline{y} - \hat\beta \overline{x}  \ .
\end{eqnarray*}
\end{minipage}}
~\\

Moreover,\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{eqnarray*}
\hat\sigma^2 &=& \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\alpha + \hat\beta x_i)
\end{eqnarray*}
\end{minipage}}
~\\
is an unbiased estimate of the residual variance $\sigma^2$.


{\small (The derivation of the parameters can be looked up in the Stahel script 2.A b. Idea: Minimization through derivating equations and setting them =0.)}

}



\frame{\frametitle{Do-it-yourself ``by hand''}

Go to the Shiny gallery and try to ``estimate'' the correct parameters.\\[6mm]

You can do this here:\\[2mm]

\url{https://gallery.shinyapps.io/simple_regression/} \\[2mm]


}


\frame[containsverbatim]{\frametitle{Estimation using R}
Let's estimate the regression parameters from the bodyfat example\\[3mm]


<<lmbodyfat,echo=T,eval=T>>=
r.bodyfat <- lm(bodyfat ~ bmi,d.bodyfat)

summary(r.bodyfat)
@

~\\[2mm]
$\Rightarrow$ $\hat\alpha=\Sexpr{round(r.bodyfat$coef[1],2)}$ ,  $\hat\beta=\Sexpr{round(r.bodyfat$coef[2],2)}$, $\hat\sigma_e = \Sexpr{round(summary(r.bodyfat)$sigma,2)}$.
}


\frame[containsverbatim]{
The resulting line can be added to the scatterplot:\\[4mm]

\begin{center}
<<echo=F>>=
library(reporttools)
library(biostatUZH)
@
<<plotbodyfat,eval=T,fig.width=3.5, fig.height=3,warning=FALSE,echo=F,out.width="7cm">>=
ggplot(d.bodyfat,aes(bmi,bodyfat)) + geom_point() + geom_smooth(method='lm',se=F) + theme_bw()
@
\end{center}

\underline{Interpretation:} for an increase in the BMI by one index point, we roughly expect a 1.82\% percentage increase in bodyfat.
}


\frame[containsverbatim]{\frametitle{Uncertainty in the estimates $\hat\alpha$ and $\hat\beta$}

Important: $\hat\alpha$ and $\hat\beta$ are themselves \myalert{random variables} and as such contain \myalert{uncertainty}!\\[4mm]

Let us look again at the regression output, this time only for the coefficients. The second column shows the standard error of the estimate:\\[2mm]

<<lmbodyfat.uncertainty,echo=T,eval=T>>=
summary(r.bodyfat)$coef
@

~\\
$\rightarrow$ The logical next question is: what is the distribution of the estimates?

}




\frame[containsverbatim]{\frametitle{Distribution of the estimators for $\hat\alpha$ and $\hat\beta$}
 
To obtain an idea, we generate data points according to model
%
$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim \N(0,0.5^2). $$
In each round, we estimate the parameters and store them:

<<>>=
set.seed(1)
@
<<echo=T, eval=T>>=
niter <- 1000
pars <- matrix(NA,nrow=niter,ncol=2)
for (ii in 1:niter){
  x <- rnorm(100)
  y <- 4 - 2*x + rnorm(100,0,sd=0.5)
  pars[ii,] <- lm(y~x)$coef
}
@
 
Doing it \numprint{1000} times, we obtain the following distributions for $\hat\alpha$ and $\hat\beta$:

\begin{center}
<<eval=T,fig.width=6, fig.height=3,warning=FALSE,out.width="5cm">>=
library(cowplot)
pars <- data.frame(pars)
names(pars) <- c("alpha","beta")
 
p1 <-  ggplot(pars,aes(x=alpha)) + geom_histogram() +  theme_bw()
p2 <-  ggplot(pars,aes(x=beta)) + geom_histogram() +  theme_bw()
p <- plot_grid(p1,p2,  ncol = 2, rel_heights = c(1, .2))
p
# 
# hist(pars[,1],xlab =expression(hat(alpha)),main="",freq=F,nclass=20)
# hist(pars[,2],xlab=expression(hat(beta)),main="",freq=F,nclass=20)
@
\end{center}

}


\frame{
This looks suspiciously normal!\\[2mm]

In fact, from theory it is known that  

\begin{eqnarray*}
\hat\beta \sim \N(\beta,{\sigma^{(\beta)2}}) & \quad \text{and} \quad & \hat\alpha \sim \N(\alpha,{\sigma^{(\alpha)2}})
\end{eqnarray*}

For formulas of the standard deviations ${\sigma^{(\beta)2}}$ and ${\sigma^{(\alpha)2}}$, please consult Stahel 2.2.h.\\[6mm]


\colorbox{lightgray}{\begin{minipage}{10cm}
{\bf To remember:}
\begin{itemize}
\item $\hat\alpha$ and $\hat\beta$ are \myalert{unbiased estimators} of $\alpha$ and $\beta$.
\item the parameters estimates $\hat\alpha$ and $\hat\beta$ are \myalert{normally distributed.}
\item the formulas for the variances depend on the residual variance $\sigma^2$, the sample size $n$ and the variability of $X$ (SSQ$^{(X)(\star)})$.
\end{itemize}
\end{minipage}}

\vspace{1cm}
$^{(\star)}$  $\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 $

}


\frame{\frametitle{Are the modelling assumptions met?}
In practice, it is advisable to check if all our \myalert{modelling assumptions are met}.\\[2mm]

$\rightarrow$ Otherwise we might draw invalid conclusions from the results.\\[4mm]

Remember: Our assumption is that $\epsilon_i \sim \N(0,\sigma^2)$. This implies\\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[a)]
\item The expected value of $\epsilon_i$ is 0: $\E(\epsilon_i)=0$.\\[2mm]
% which implies that for a given x, the y are distributed around  alpha + beta * x
\item All $\epsilon_i$ have the same variance: $\Var(\epsilon_i)=\sigma^2$. \\[2mm]
\item All $\epsilon_i$ are normally distributed.\\[2mm]
%\item $\epsilon$ is independent of any variable, observation number etc.
\end{enumerate}
\end{minipage}}
~\\

In addition, it is assumed that\\[3mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{enumerate}[d)]
\item  $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are independent.
\end{enumerate}
\end{minipage}}

~\\[2mm]
{\bf Note:} We do not actually observe $\epsilon_i$, but only the residuals $e_i$.
Let us introduce two simple graphical model checking tools for our residuals $e_i$.
}


\frame[containsverbatim]{\frametitle{Model checking tool I: Tukey-Anscombe diagram}

The \myalert{Tukey-Anscombe} diagram plots the residuals against the fitted values. For the bodyfat data it looks like this:\\
\begin{center}
<<TA,eval=T,fig.width=3, fig.height=3,warning=FALSE>>=
library(ggfortify)
autoplot(r.bodyfat,smooth.colour=NA)[1]#,r.bodyfat$residuals,xlab="Fitted", ylab="Residuals")
@
\end{center}
This plot is ideal to check if assumptions a) and b) (and partially d)) are met. Here, this seems fine.
}


\frame[containsverbatim]{\frametitle{Model checking tool II: Quantile-quantile diagram}
Remember the QQ-plot from Section 8.8. We can generate the respective plot for the residuals:
\begin{center}
<<QQ,eval=T,fig.width=3, fig.height=3,warning=FALSE>>=
library(ggfortify)
autoplot(r.bodyfat,smooth.colour=NA)[2]#,r.bodyfat$residuals,xlab="Fitted", ylab="Residuals")
@
\end{center}
If the residuals are normally distributed, they should approximately lie on a straight line.
$\rightarrow$ Assumption (c) seems ok here.
 
}




% \frame[containsverbatim]{\frametitle{Uncertainty in the estimates $\hat\alpha$ and $\hat\beta$}
% Let us look again at the regression output, this time only for the coefficients:\\[6mm]
% 
% %<<lmbodyfat.uncertainty,echo=T,eval=T>>=
% <<lmbodyfat.uncertainty2,eval=T>>=
% summary(r.bodyfat)$coef
% @
% ~\\[6mm]
% The second column shows a standard error of the estimate. \\
% $\rightarrow$ This implies: the estimates contain \myalert{uncertainty}!\\[4mm]
% 
% The logical next question is: what is the distribution of the estimates?
% 
% }



% \frame[containsverbatim]{\frametitle{Distribution of $\hat\alpha$ and $\hat\beta$}
% \vspace{-6mm}
% Again, a simulation can help to get an idea. We generate data points according to the model
% %
% $$y_i = 4 - 2x_i + e_i \ , \quad e_i\sim \N(0,0.5^2). $$
% In each round, we estimate the parameters and store them:\\[3mm]
% 
% <<echo=T, eval=T>>=
% niter <- 1000
% pars <- matrix(NA,nrow=niter,ncol=2)
% for (ii in 1:niter){
% x <- rnorm(100)
% y <- 4 - 2*x + rnorm(100,0,sd=0.5)
% pars[ii,] <- lm(y~x)$coef
% }
% @
% ~\\[2mm]
% Doing it \texttt{niter <-} \numprint{1000} times, we obtain the following distributions for $\hat\alpha$ and $\hat\beta$.
% }

% \frame[containsverbatim]{\label{sl:dists}
% \begin{center}
% \setkeys{Gin}{width=0.85\textwidth}
% <<eval=T,fig.width=7,fig.height=3.5>>=
% par(mfrow=c(1,2))
%  # library(reshape2)
%  # pars <- melt(pars)
%  # ggplot(pars,aes(x=value)) + geom_histogram() + facet_wrap(~Var2,scales="free_x") + theme_bw()
% hist(pars[,1],xlab =expression(hat(alpha)),main="",freq=F,nclass=20)
% hist(pars[,2],xlab=expression(hat(beta)),main="",freq=F,nclass=20)
% @
% \end{center}
% 
% This looks suspiciously normal... \\[2mm]
% In fact, from theory:
% \begin{eqnarray*}
% \hat\beta \sim \N(\beta,{\sigma^{(\beta)2}}) & \quad \text{and} \quad & \hat\alpha \sim \N(\alpha,{\sigma^{(\alpha)2}})
% \end{eqnarray*}
% }

% \frame{
% For formulas of the standard deviations ${\sigma^{(\beta)2}}$ and ${\sigma^{(\alpha)2}}$, please consult Stahel 2.2.h.\\[6mm]



% $$\sigma^{(\beta)2} = \sigma_e^2 / \text{SSQ}^{(X)} \qquad \sigma^{(\alpha)2} = \sigma_e^2 \left(\frac{1}{n} + \overline{x}^2/\text{SSQ}^{(X)}\right)$$
% 
% with the sum-of-squares for $X$ given as
% $$\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 \ .$$
% 
% {\small(See also Stahel 2.2.h)}\\[2mm]
% \colorbox{lightgreen}{\begin{minipage}{10cm}
% Don't worry, you do not need to know these formulas by heart! 
% \end{minipage}}
% ~\\

% \colorbox{lightgray}{\begin{minipage}{10cm}
% {\bf No need to remember these formulas, but you should know that}
% \begin{itemize}
% \item the parameters estimates $\hat\alpha$ and $\hat\beta$ are \myalert{normally distributed.}
% \item the formulas to calculate the variances depend on the residual variance $\sigma_e^2$, the sample size $n$ and the variance of $X$ (SSQ$^{(X)(\star)})$.
% \end{itemize}
% \end{minipage}}
% 
% \vspace{2cm}
% $^{(\star)}$  $\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 \ $
% }



\frame[containsverbatim]{\frametitle{How good is the regression model?}
This is, per se, a difficult question.... \\[2mm]

One often considered index is the {\bf coefficient of determination (Bestimmtheitsmass)} $R^2$.
Let us again look at the regression output form the bodyfat example:\\

<<lmbodyfat4,echo=T,eval=T>>=
summary(r.bodyfat)$r.squared
@

% ~\\[0mm]
% This is the $R^2$ from the regression of bodyfat against bmi. \\
Compare this to the squared correlation between the two variables:\\[2mm]
% 
% <<lmbodyfat3,echo=T,eval=T>>=
% cor(d.bodyfat$bodyfat,d.bodyfat$bmi)
% @
% ~\\
% ... and square it:\\[2mm]
<<lmbodyfat2,echo=T,eval=T>>=
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)^2
@


\colorbox{lightgray}{\begin{minipage}{10cm}
$\rightarrow$ In simple linear regression, $R^2$ is the squared correlation between the independent and the dependent variable.
\end{minipage}}\\[6mm]
}

\frame{



\colorbox{lightgray}{\begin{minipage}{10cm}
\begin{itemize} 
\item $R^2$ indicates the proportion of variability of the response variable $\bm{y}$ that is {\bf explained by the ensemble of all covariates}. 
\item Its value lies between 0 and 1.
\end{itemize}
 \end{minipage}}\\[10mm] 

The {\bf larger} $R^2$ \\ 
$\Rightarrow$  the {\bf more} variability of $\bm{y}$ is captured (``explained'') by the covariate \\
$\Rightarrow$ the {\bf ``better''} is the model. \\[2mm]

{\scriptsize(However, we will qualify this statement later in the course...)}\\[6mm]

%$R^2$ becomes more interesting in \emph{multiple} linear regression.\\

}




\frame{\frametitle{Testing and Confidence Intervals}\label{sl:testsCI}
After the regression parameters and their uncertainties have been estimated, there are typically two fundamental questions:\\[4mm]

\begin{enumerate}
\item {\bf ''Are the parameters compatible with some specific value?''} \\
Typically, the question is whether the slope $\beta$ might be 0 or not, that is: ``Is there an effect of the covariate $\bm{x}$ or not?''\\[2mm]
$\Rightarrow$ This leads to a \alert{\bf statistical test}.\\[4mm]

\item {\bf ``Which values of the parameters are compatible with the data?''}\\[2mm]
$\Rightarrow$ This leads us to determine \alert{\bf confidence intervals}.\\[4mm]
\end{enumerate}
}

\frame[containsverbatim]{
Let's first go back to the output from the bodyfat example:\\[5mm]
<<lmbodyfatTest,echo=T,eval=T>>=
summary(r.bodyfat)$coef
@
~\\[4mm]
Besides the estimate and the standard error (which we discussed before), there is a \myalert{\texttt{t value}} and a probability \myalert{\texttt{Pr(>|t|)}} that we need to understand. \\[2mm]

How do these things help us to answer the two questions above?
}


\frame{\frametitle{Testing the effect of a covariate}
Remember: in a statistical test you first need to specify the \emph{null hypothesis}. Here, typically, the null hypothesis is

\colorbox{lightgray}{\begin{minipage}{10cm}
$$H_0: \quad \beta = \beta_0 =  0  \ .$$

\begin{center}
In words: $H_0$ =   ``no effect'' \\
\end{center}
\end{minipage}}
~\\[2mm]
{\small (Included in $H_0$ is the assumption that the data follow the simple linear regression model!)} \\[8mm]
%{\small (However, you might want to test against another null hypothesis, see Stahel 2.3 a,b).}\\[2mm]

Here, the \emph{alternative hypothesis} is given by
\colorbox{lightgray}{\begin{minipage}{10cm}
$$H_A: \quad \beta \neq  0  $$
\end{minipage}}

 }
 
\frame{
Remember: To carry out a statistical test, we need a \emph{test statistic}.\\[4mm]

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{center}
What is a test statistic? %\\[6mm]%(Clicker exercise!)
\end{center}
\end{minipage}}
\end{center}

%\pause
\vspace{2mm}
$\rightarrow$ It is some type of {\bf summary statistic} that follows a known distribution under $H_0$. For our purpose, we use the so-called {\bf $T$-statistic} \\[4mm]

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{equation}\label{eq:beta}
T=\frac{\hat\beta - \beta_0}{se^{(\beta)}}\ . %\quad \text{with} \quad se^{(\beta)}=\sqrt{\hat\sigma_e^2/SSQ^{(X)}}  \ .
\end{equation}
\end{minipage}}
\end{center}

~\\[2mm]
Again: typically, $\beta_0=0$, so the formula simplifies to.... (please think:-))\\[2mm]

Under $H_0$, $T$ has a $t$-distribution with $n-2$ degrees of freedom ($n=$ number of data points).\\[2mm]

{\small (You should try to recall the t-distribution. Check Mat183, keyword: t-test.)}

}


\frame[containsverbatim]{\label{sl:bmi}
So let's again go back to the bodyfat regression output:\\[4mm]

<<echo=T,eval=T>>=
summary(r.bodyfat)$coef
@
~\\[6mm]

Task:\\[2mm]
$\rightarrow$ Please use equation \eqref{eq:beta} to find out how the first three columns (Estimate, Std. Error and t value) are related! Check your ideas by doing some calculations...\\[7mm]

Note: The last column contains the {\bf $p$-value} of the test $\beta=0$.\\[2mm]
 
}


\frame{\frametitle{Recap: Formal definition of the $p$-value}

 

\colorbox{lightgray}{\begin{minipage}{10cm}
The {\bf formal definition of $p$-value} is the probability to observe a data summary (e.g., an average) that is at least as extreme as the one observed, given that the Null Hypothesis is correct.
\end{minipage}}\\
%{\scriptsize \citep{goodman2016}}

~\\[2mm]
Example (normal distribution): Assume the observed test-statistic leads to a $z$-value = -1.96 \\
$\Rightarrow$ $\P(|z|\geq 1.96)=0.05$ and $\P(z\leq-1.96)=0.025$ .
 
\setkeys{Gin}{width=1\textwidth}
%<<pValFig,fig=T,width=8,height=4,echo=F>>=
<<pValFig,eval=T,fig.width=8,fig.height=4,out.width="10cm">>=
par(mfrow=c(1,2))

zz1 <- qnorm(0.025)
zz2 <- qnorm(0.975)
zz3 <- qnorm(0.025)

cord.x1 <- c(-4,seq(-4,zz1,0.01),zz1)
cord.y1 <- c(0,dnorm(seq(-4,zz1,0.01)),0)

cord.x2 <- c(zz2,seq(zz2,4,0.01),4)
cord.y2 <- c(0,dnorm(seq(zz2,4,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="Two-sided p-value (0.05)",xlab="")
polygon(cord.x1,cord.y1,col='gray')
polygon(cord.x2,cord.y2,col='gray')
text(-3,0.05,labels="2.5%")
text(3,0.05,labels="2.5%")

cord.x3 <- c(-4,seq(-4,zz3,0.01),zz3)
cord.y3 <- c(0,dnorm(seq(-4,zz3,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="One-sided p-value (0.025)",xlab="")
polygon(cord.x3,cord.y3,col='gray')
text(-3,0.05,labels="2.5%")

@

} 



\frame[containsverbatim]{
The regression output on slide \ref{sl:bmi} indicates that the $p$-value for BMI is very small ($p<0.0001$).


~\\[6mm]
Conclusion: there is {\bf very strong evidence} that the BMI is associated with bodyfat, because $p$ is extremely small (thus it is very unlikely that such a slope $\hat\beta$ would be seen if there was no effect of BMI on body fat).\\[4mm]

This basically answers question 1 from slide \ref{sl:testsCI}.\\[6mm]

%\textcolor{gray}{\small (Remark: if you forgot the details of the $p$-value, I have a special task for you: Go back to the book ``Statistische Datenanalyse'' from W. Stahel and read chapter 8.7.)}
}




\frame[containsverbatim]{\frametitle{A cautionary note on the use of $p$-values}
Maybe you have seen that in statistical testing, often the criterion $p\leq 0.05$ is used to test whether $H_0$ should be rejected. This is often done in a black-or-white manner.\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 
\end{minipage}}
}

% \frame{
% Question: On slide \ref{sl:dists} we have seen that $\hat\beta \sim \N(\beta,{\sigma^{(\beta)2}})$. So why does $T$ in equation \eqref{eq:beta} follow a $t$-distribution, and not a normal distribution? \\[4mm]
% 
% Remember: In Mat183 you have learned that when the variance $\sigma^2_E$ in the formula is replaced by its estimate $\hat\sigma^2_E$, then the normal distribution must be replaced by the $t$-distribution (Keyword: t-test).
% 
% }


\frame{\frametitle{Confidence intervals of regression parameters}
Question 2 from slide \ref{sl:testsCI}: \\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
``Which values of the parameters are compatible with the data?''
\end{minipage}}
~\\[4mm]

To answer this question, we can determine the confidence intervals of the regression parameters.\\[4mm]

{\bf Facts we know about $\hat\beta$}:
\begin{itemize}
\item $\hat\beta$ is estimated with a standard error of $\sigma^{(\beta)}$.
\item The distribution of $\hat\beta$ is normal, namely $\hat\beta\sim\N(\beta,\sigma^{(\beta)2})$.
\item However, since we need to estimate $\sigma^{(\beta)2}$ from the data, we have a $t$-distribution.
\end{itemize}
}

\frame[containsverbatim]{
Doing some calculations (similar to those in chapter 8.2.2 of Mat183 script) leads us to the 95\% confidence interval\\[2mm]

\begin{center}
\colorbox{lightgray}{\begin{minipage}{6cm}
$$[\hat\beta - c \cdot \hat\sigma^{(\beta)} ; \hat\beta + c \cdot \hat\sigma^{(\beta)}] \ ,$$
\end{minipage}}
\end{center}
~\\[1mm]

where $c$ is the 97.5\% quantile of the $t$-distribution with $n-2$ degrees of freedom. \\[2mm]

Doing this for the bodfat example ``by hand'' is not hard. We have 241 degrees of freedom:\\[4mm]
<<lmbodyfatCI,echo=T,eval=T>>=
coefs <- summary(r.bodyfat)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
@
~\\[2mm]

}

\frame[containsverbatim]{
Even easier: directly ask R to give you the CIs.\\[2mm]

<<echo=T,eval=T>>=
confint(r.bodyfat,level=c(0.95))
@
~\\[2mm]

In summary,
<<results="asis",echo=F>>=
tableRegression(r.bodyfat)
@
~\\[6mm]
\underline{Interpretation:} for an increase in the bmi by one index point, roughly 1.82\% percentage points more bodyfat are expected, and all true values for $\beta$ between 1.61 and 2.03 are compatible with the observed data. \\[2mm]
}


\frame{\frametitle{Confidence and Prediction Ranges}
\begin{itemize}
\item Remember: When another sample from the same population was taken, the regression line would look slightly different.\\[4mm]
\item There are two questions to be asked:\\[6mm]
\end{itemize}

\begin{enumerate}
\item Which other regression lines are compatible with the observed data?\\[2mm]
$\Rightarrow$ This leads to the {\bf confidence range}. \\[4mm]
\item Where do future observations with a given $x$ coordinate lie? \\[2mm]
$\Rightarrow$ This leads to the {\bf prediction range}.
\end{enumerate}
}


\frame{\frametitle{Bodyfat example}
\vspace{-10mm}
\begin{center}
<<confpred,eval=T,fig.width=5,fig.height=5,out.width="7cm">>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c("confidence range (95%)", "prediction range (95%)"),
lty=8, cex=1,col=c(2,4),lwd=2)
@
\end{center}

Note: The prediction range is much broader than the confidence range.
}


\frame{\frametitle{Calculation of the confidence range}
Given a fixed value of $x$, say $x_0$. The question is: \\[2mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does $\hat y_0 = \hat\alpha + \hat\beta x_0$ lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}\\[2mm]

This question is not trivial, because both $\hat\alpha$ and $\hat\beta$ are estimates from the data and contain uncertainty. \\[2mm]

The details of the calculation are given in Stahel 2.4b. \\[6mm]%(Idea: $\hat y_0 \pm q\cdot se^{(\hat{y}_0)}$.) \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}

Plotting the confidence interval around all $\hat y_0$ values one obtains the {\bf confidence range} or {\bf confidence band for the expected values} of $y$.
\end{minipage}}\\[2mm]

~\\[2mm]
Note: For the confidence range, only the uncertainty in the estimates $\hat\alpha$ and $\hat\beta$ are decisive.
}


\frame{
\vspace{-5mm}
\begin{center}
<<conf,eval=T,fig.width=5,fig.height=5,out.width="7cm">>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Confidence range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
legend("bottomright", c("confidence range (95%)"),
lty=8, cex=1,col=c(2),lwd=2)
@
\end{center}

}



\frame{\frametitle{Calculation of the prediction range}
Given a fixed value of $x$, say $x_0$. The question is: \\[6mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does a {\bf future observation} lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}\\[6mm]

To answer this question, we have to \alert{consider not only the uncertainty in the predicted value} $\hat y_0 =  \hat\alpha + \hat\beta x_0$, but also the \alert{error in the equation $e_i \sim \N(0,\sigma_e^2)$}. \\[6mm]


This is the reason why the {\bf prediction range is always wider than the confidence range}.

}


\frame{
\vspace{-5mm}
\begin{center}
<<pred,eval=T,fig.width=5,fig.height=5,out.width="7cm">>=
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Prediction range",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c( "prediction range (95%)"),
lty=8, cex=1,col=c(4),lwd=2)
@
\end{center}

}



\frame{\frametitle{Tasks until the next practical (Thu/Fri)}

The idea of the course is that as a preparation for the practical part you will do the following:\\[2mm]

\begin{itemize}
\item Understand what today's lecture was about. You will certainly need to click through it again.\\[2mm]
%\item {\bf If necessary} (if things are not clear on the slides), consult the "Course material covered today" (see slide 3: today it was chapter 2 of the Stahel script \emph{Lineare Regression}).\\[2mm]
\item Go to openedX and do all the "Before class (BC)" tasks.\\[6mm]
\end{itemize}

{\bf $\rightarrow$ The same procedure applies to all course weeks.}
 

% ``Points of significance: Simple linear regression'' is a nice, 2-page overview of simple linear regression, summarized in Nature Methods in 2015:\\[4mm]
% 
% 
% 
% \url{http://www.nature.com/nmeth/journal/v12/n11/full/nmeth.3627.html}
}

% \frame{\frametitle{Summary}
% To do 
% }

%\frame{References:
%\bibliographystyle{Chicago}
%\bibliography{refs}
%}

\end{document}
