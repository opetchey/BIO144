---
title: "Lecture 3: Regression"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff, Owen Petchey & Uriah Daugaard"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```




## First an alert!

Downloading and opening data (CSV) files

**Excel will try to be helpful, but in fact it can break things. Do not accept its help, unless you are sure.**


\begin{center}
\includegraphics[width=11cm]{images/excel_save_as.png}
\end{center}




## Overview

* Why use (linear) regression?
* Fitting the line (= parameter estimation)
* Is linear regression good enough model to use?
* What to do when things go wrong?
* Transformation of variables/the response
* Handling of outliers

(Next lecture will be interpretation and use of the model. We don't use it until we know it is "working well").


## The lecture material of today is based on the following literature:

\ 

* Chapter 2 of _Lineare Regression_, p.7-20 (Stahel script)


## A head's up...

* Regression is a type of "linear model". 
* A t-test is a linear model.

Over the next five lectures you will learn about other types of linear model.

* Multiple regression.
* ANOVA.
* ANCOVA.

All are mathematically very similar, with many similar properties.

The same function in R 'lm' is used to make them all.



## Why use regression?


Imagine we want to know if BMI (easily measured; height and weight) is a good indicator of percentage body fat (less easily measured; specialist instrument required). If it is, then we save some time and effort, and still get the knowledge we need (e.g. percentage body fat).


Put another way: is BMI a good predictor for percentage body fat?

We can address this by looking at the relationship between the two continuous variables, BMI and percentage body fat (next slide).



## Does BMI related to percent body fat?

```{r echo = FALSE, eval=TRUE}
d.bodyfat <- read.table(here("3_datasets/bodyfat.clean.txt"),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
```

```{r eval=TRUE, fig.width=3.5, fig.height=3, out.width='5cm', fig.align='center', warning=FALSE, echo = FALSE}
library(ggplot2)
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + 
  theme_bw() + ylab("body fat (%)")
```

Happy? Not quite? There are many questions we may want to answer:

* What is a good mathematical representation of the relationship?
* Is the relationship different from what we would expect by chance?
* How good is the mathematical representation?
* How much uncertainty is there in any predictions?



## Linear regression

We often use linear regression because:

* A linear relationship is mathematically simple.
* Prior knowledge and observed data suggest it is at least a good starting point.
* Given an \emph{explanatory variable} ($X$) and a \emph{response variable} ($Y$) all points $(x_i,y_i)$, $i= 1,\ldots, n$, on a  straight line follow the equation
$$y_i = \beta_0 + \beta_1 x_i\ .$$
* $\beta_0$ is the \alert{intercept} - the value of $Y$ when $x_i = 0$
* $\beta_1$ the \alert{slope} of the line, also known as the regression coefficient of $X$.
* If $\beta_0=0$ the line goes through the origin $(x,y)=(0,0)$.
* \alert{Interpretation} of linear dependency: proportional increase in $y$ with increase (decrease) in $x$.



## Which is the "best" line?

```{r eval=TRUE, fig.width=3.5, fig.height=3, warning=FALSE, echo = FALSE, out.width='6.5cm', fig.align='center'}
ggplot(d.bodyfat,aes(x=bmi,y=bodyfat)) + geom_point() + theme_bw() + ylab("body fat (%)")  + 
  geom_abline(intercept = -25, slope = 1.7, color="red",   size=0.6) +
  geom_abline(intercept = -35, slope = 2.1, color="green",    size=0.6) +
  geom_abline(intercept = -36, slope = 2.25, color="blue",    size=0.6) 
```

__Task:__ Estimate the regression parameters $\beta_0$ and $\beta_1$ (by "eye") and write them down. 



## Not a perfect fit to the data

\

It is obvious that 

* the linear relationship does not describe the data perfectly
* another realization of the data (a different group of people) would lead to a slightly different picture.


$\Rightarrow$ We need a __model__ that describes the relationship between BMI and bodyfat.

## The simple linear regression model

\colorbox{lightgray}{\begin{minipage}{14cm}
In the linear regression model the dependent variable $Y$ is related to the independent variable $x$ as
$$Y = \beta_0 + \beta_1 x + \epsilon \ , \qquad \epsilon \sim N(0,\sigma^2)$$
\end{minipage}}

In this formulation $Y$ is a random variable $Y \sim N(\beta_0 + \beta_1 x, \sigma^2$) where
$$Y \quad= \quad \underbrace{\text{expected value}}_{E(Y) = \beta_0 + \beta_1 x} \quad + \quad \underbrace{\text{random error}}_{\epsilon}  \ .$$
Note:

* The model for $Y$ given $x$ has \alert{three parameters}: $\beta_0$, $\beta_1$ and $\sigma^2$ .
* $x$ is the \alert{independent} / \alert{explanatory} / \alert{regressor} variable.
* $Y$ is the \alert{dependent} / \alert{outcome} / \alert{response} variable.

## 

__Note__

* The linear model propagates the most simple relationship between two variables.
* It is often a good starting point.
* But before using it, please always think if such a relationship is meaningful/reasonable/plausible.
* Always look at the data \alert{before} you start with model fitting.


## Visualization of the regression assumptions

The assumptions about the linear regression model lie in the error term $$\epsilon \sim N(0,\sigma^2) \ . $$

\begin{center}
\includegraphics[width=9cm]{pictures/regrAssumptions.jpg}
\end{center}

Note: The regression line goes through $E(Y)$.




## Insights from data simulation

<!-- \scriptsize  -->
(Simulation are \emph{always} a great way to understand statistics!!)

Generate an independent (explanatory) variable __x__ and __two__ samples of a dependent variable __y__ assuming that
$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim N(0,0.5^2) \ .$$

```{r eval=T, fig.width=3.5, fig.height=3.3, out.width='4cm', echo = FALSE, fig.align='center'}
set.seed(134539)
par(mar=c(4,4,1,1))
x <- runif(25,-2,2)
y1 <- 4 - 2*x + rnorm(25,0,sd=0.5)
y2 <- 4 - 2*x + rnorm(25,sd=0.5)
plot(x,y1,ylim=c(min(c(y1,y2)),max(c(y1,y2))),ylab="y")
points(x,y2,col=2)
abline(c(4,-2))
legend("topright",legend=c("sample 1","sample 2"),col=1:2, pch=1)
```

$\rightarrow$ Random variation is always present. This leads us to the next question.



## Parameter estimation

In a regression analysis, the task is to estimate the \alert{regression coefficients} $\beta_0$, $\beta_1$ and the \alert{residual variance} $\sigma^2$ for a given set of $(x,y)$ data.

* __Problem:__ For more than two points $(x_i,y_i)$, $i=1,\ldots, n$, there is generally no perfectly fitting line.

* __Aim:__ We want to estimate the parameters $(\beta_0,\beta_1)$ of the best fitting line $Y = \beta_0 + \beta_1 x$.

* __Idea:__ Minimize the deviations between the data points $(x_i,y_i)$ and the regression line.

\
But how? 

## Should we minimize these distances...

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="8cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
x1 <- -0.66
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x1,4-2*(x1),col=2,lwd=2)
```

## Or these?

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="8cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],x[11],4-2*x[11],col=2,lwd=2)
```

## Or maybe even these?

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="8cm", echo = FALSE, fig.align='center'}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
plot(x,y)
abline(c(4,-2),lwd=2)
segments(x[11],y[11],(y[11]-4)/(-2),y[11],col=2,lwd=2)
```

## Least squares

For multiple reasons (theoretical aspects and mathematical convenience), the parameters are estimated using the **least squares** approach. In this, yet something else is minimized:

The parameters $\beta_0$ and $\beta_1$ are estimated such that the **sum of squared vertical distances** (sum of squared residuals)

SSE means **S**um of **S**quared **E**rrors.

$$SSE = \sum_{i=1}^n e_i^2 \ , \qquad \text{where} \quad e_i = y_i - \underbrace{(\beta_0 + \beta_1 x_i)}_{=\hat{y}_i} $$

is being minimized.
\end{minipage}}

__Note:__ $\hat y_i = a + b x_i$ are the \alert{predicted values}.

## So we minimize the sum of these areas!

```{r eval=T,fig.width=5.7, fig.height=5,warning=FALSE,out.width="8cm", fig.align='center', echo=FALSE}
set.seed(9670)
n <- 10
x <- rnorm(n)
y <- 4 - 2*x + rnorm(n,0,sd=1)
x[11]  <- -0.5
y[11] <- 6.2
dd <- 0.38
from_x <- c(x[11],x[11],x[11]+dd,x[11] + dd) 
from_y <- c(y[11],(4-2*x[11]),(4-2*x[11]),y[11])

to_x <- c(x[11],x[11] + dd,x[11]+ dd,x[11])
to_y <- c(4-2*x[11],4-2*x[11],y[11], y[11])

plot(x,y)
abline(c(4,-2),lwd=2)
polygon(from_x,from_y,to_x,to_y,col=2,lwd=2)
```

## Least squares estimates

For a given sample $(x_i,y_i), i=1,..,n$, with mean values $\overline{x}$ and $\overline{y}$, the least squares estimates $\hat\beta_0$ and $\hat\beta_1$ are computed as 

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{eqnarray*}
\hat\beta_1 &=& \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)} \ , \\[4mm]
\hat\beta_0 &=& \overline{y} - \hat\beta_1 \overline{x}  \ .
\end{eqnarray*}
\end{minipage}}

Moreover,

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{eqnarray*}
\hat\sigma^2 &=& \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i)
\end{eqnarray*}
\end{minipage}}

is an unbiased estimate of the residual variance $\sigma^2$.

\small (Derivations are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)

## Do-it-yourself "by hand"

Go to the Shiny gallery and try to ``estimate'' the correct parameters.

You can do this here:

\url{https://gallery.shinyapps.io/simple_regression/}




## Estimation using R

Let's estimate the regression parameters from the bodyfat example

\tiny
```{r echo=T,eval=T}
r.bodyfat <- lm(bodyfat ~ bmi, d.bodyfat)
summary(r.bodyfat)
```

<!--$\Rightarrow$ $\hat\beta_0 = $ `r round(r.bodyfat$coef[1],2)` ,  $\hat\beta_1=$ `r round(r.bodyfat$coef[2],2)`, $\hat\sigma_e =$ 
`r round(summary(r.bodyfat)$sigma,2)`-->

## 

The resulting line can be added to the scatterplot:

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(reporttools)
# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")
library(biostatUZH)
```

```{r eval=T,fig.width=3.5, fig.height=3,warning=FALSE,echo=F,out.width="6cm", fig.align='center', message=FALSE, warning=FALSE}
ggplot(d.bodyfat,aes(bmi,bodyfat)) + geom_point() + geom_smooth(method='lm',se=F) + theme_bw()
```

\underline{Interpretation:} for an increase in the BMI by one index point, we roughly expect a 1.82\% percentage increase in bodyfat.




## Is the model good enough to use?

* All models are wrong, but is ours good enough to be useful.
* Are the assumption of the model justified?
* It would be very unwise to use the model before we know if it is good enough to use.
* Don't jump out of an aeroplane until you know your parachute is good enough!

\begin{center}
\includegraphics[width=6cm]{images/jumping.jpeg}
\end{center}


## What assumptions do we make?

The main assumption in linear regression is that the residuals follow a $N(0,\sigma^2)$ distribution.

We make this assumption because it is often well enough met, and it gives great mathematical tractibility.

This assumption implies that:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{enumerate}[(a)]
\item The $\epsilon_i$ are normally distributed.\\[2mm]
\item All $\epsilon_i$ have the same variance: $Var(\epsilon_i)=\sigma^2$. \\[2mm]
\item The $\epsilon_i$ are independent of each other.
\end{enumerate}
\end{minipage}}
```

Furthermore:

- (d) we assumed a linear relationship.
- (e) implies there are no outliers (implied by (a) above)




## (a) Normally distributed residuals

Look at the histogram of the residuals:

```{r fig.width=4,fig.height=4,echo=FALSE,out.width='5.5cm', fig.align='center'}
hist(r.bodyfat$residuals,nclass=15,xlab="Residuals",main='')
```

The normal distribution assumption (a) seems ok as well.


## (a) Normally distributed residuals: The QQ-plot

Usually, not the histogram of the residuals is plotted, but the
so-called \alert{quantile-quantile} (QQ) plot. The quantiles of the
observed distribution are plotted against the quantiles of the
respective theoretical (normal) distribution:

```{r eval=T,fig.width=4.5, fig.height=4.5,warning=FALSE,echo=F,out.width="4cm", message=FALSE, fig.align='center'}
qqnorm(residuals(r.bodyfat))
qqline(residuals(r.bodyfat))
```

If the points lie approximately on a straight line, the data is fairly
normally distributed.

This is often "tested" by eye, and needs some experience.

## What on earth is a quantile???

Imagine we make 21 measures of something, say 21 reaction times:

```{r fig.width=6, fig.height=1, warning=FALSE, echo=F, out.width="10cm", message=FALSE, fig.align='centre'}
set.seed(1)
dd <- tibble(Reaction_time = round(rnorm(21, 300, 20),1))
ggplot(dd) +
  geom_jitter(aes(y=1, x=Reaction_time)) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

The median of these is `r median(dd$Reaction_time)`. The median is the
50% or 0.5 quantile, because half the data points are above it, and half
below.

```{r}
quantile(dd$Reaction_time)
```

## The QQ-plot continued...

The *theoretical quantiles* come from the normal distribution. The
*sample quantiles* come from the distribution of our residuals.

```{r eval=T,fig.width=6.5, fig.height=4.5,warning=FALSE,echo=F,out.width="8cm", message=FALSE, fig.align='center'}
par(mfrow=c(1,2))
x<-seq(-3,3,0.1)
stdr <- sort(scale(r.bodyfat$residuals))

s <- seq(2)  # one shorter than data
x <- c(-2.5,qnorm(0.6),qnorm(0.6))
y <- c(0.6,0.6,0)

plot(stdr,pnorm(stdr),xlab="Theoretical quantiles",ylab="Cumulative distribution (F)")
arrows(x[s], y[s], x[s+1], y[s+1])
n<-nrow(d.bodyfat)
plot(stdr,(1:n)/n,xlab="Sample quantiles of the residuals",ylab="Empirical cumulative distribution")
arrows(x[s], y[s], x[s+1], y[s+1])
```


## How do I know if a QQ-plot looks "good"?

There is __no quantitative rule__ to answer this question, experience is needed. However, you can gain this experience from \alert{simulations}. To this end, generate the same number of data points of a normally distributed variable and compare to your plot.

Example: Generate 59 points $\epsilon_i \sim N(0,1)$ each time:

```{r eval=T,fig.width=6.5, fig.height=4.0,warning=FALSE,out.width="8.5cm", fig.align='center',echo=FALSE}
set.seed(390457)
par(mfrow=c(2,3),mar=c(4,4,1,1))
for (ii in 1:6){
  ss <- rnorm(59)
qqnorm(ss,main="")
qqline(ss,xlab="")
}
```




## (b) Equal variance (all $\epsilon_i$ have the same variance)

Scale-location plot (Streuungs-Diagramm).

The scale-location plot is particularly suited to check the assumption of equal variances (__homoscedasticity / Homoskedastizität__).

The idea is to plot the square root of the (standardized) residuals $\sqrt{|R_i|}$ against the fitted values $\hat{y_i}$. There should be __no trend__: 

```{r eval=T,warning=FALSE,out.width="6cm", fig.align='center', echo=FALSE}
plot(r.bodyfat,which=3, add.smooth = FALSE)
```

## Sketch explanation of scale-location





## (c) Independence (the $\epsilon_i$ are independent of each other)

* Think carefully about how the data were collected.
* Think carefully about any structure and / or groupings in the data that are not described in it.



## (d) Linearity

With simple regression, we can assess this directly on the graph with fitted line <!-- scroll to an example  -->

Even then, it can be useful to look at a graph called the **Tukey-Anscombe plot*. It is a graph of the residuals versus the fitted values.


```{r eval = TRUE, message=FALSE, fig.width=5, fig.height=4,warning=FALSE,echo=F,out.width="7cm", fig.align='left'}
plot(r.bodyfat$fitted,r.bodyfat$residuals,xlab="Fitted", ylab="Residuals")
abline(h=0,lty=2)
```

\vspace*{-6pt}

\begingroup
\fontsize{8}{8}\selectfont
It can be useful to add a "smoothed mean" to the TA-plot, which can give hints on the trend of the residuals.
However, generally we recommend to \alert{not} add a smoothing line, because it may bias our view on the plot.
\endgroup

<!-- But: The dependency is not necessarily on the fitted values ($x$-axis of TA plot). Ideas:   -->

<!-- * Plot residuals in dependency of time (if available) or sequence of obervations. -->
<!-- * Plot residuals against the explanatory variable. -->


## (e) No outliers

An outlier is a data point (value) that is an abnormal distance from the others.

How to identify outliers:

* Look at the histograms of the data, and scatter plots (OK, but not foolproof).
* Look at the distribution of the residuals.

How to identify important outliers:
* Look at "leverage".


## Leverages ("Hebel")

```{r fig.width=5, fig.height=4, warning=FALSE, echo=F, out.width="6cm", fig.align='center'}
plot(r.bodyfat, which=5, add.smooth = FALSE)

```


To understand the leverage plot, we need to introduce the idea of the \emph{leverage} ("Hebel").

In simple regression, the leverage of individual $i$ is defined as $H_{ii} = (1/n) + (x_i-\overline{x})^2 / SSQ^{(X)}$. 



## Graphical illustration of the leverage effect

Data points with $x_i$ values far from the mean have a stronger leverage effect than when $x_i\approx \overline{x}$:

```{r eval=T,fig.width=6.5, fig.height=2.3,warning=FALSE,out.width="9.5cm", echo=FALSE, fig.align='center'}
set.seed(37489)
par(mfrow=c(1,3),mar=c(4,4,1,1))
x <- sort(rnorm(18))
y <- 2*x + rnorm(18,0,0.4)
plot(y~x)
abline(lm(y~x))
y1 <- y
y1[18] <- y[18] -5
plot(y1~x,col=c(rep(1,17),2))
abline(lm(y~x))
abline(lm(y1~x),col=2,lty=2)
y2 <- y
y2[9] <- y2[9] + 5
plot(y2~x,col=c(rep(1,8),2,rep(1,9)))
abline(lm(y~x))
abline(lm(y2~x),col=2,lty=2)
```

The outlier in the middle plot "pulls" the regression line in its direction and biases the slope.



## Leverage plot (Hebelarm-Diagramm)

In the leverage plot, (standardized) residuals $\tilde{R_i}$ are plotted against the leverage $H_{ii}$ :

```{r out.width="5cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
plot(r.bodyfat, which=5, add.smooth = FALSE)
```

\alert{Critical ranges} are the top and bottom right corners!!

Here, observations 36, 39, and 207 are labelled as potential \alert{outliers}.


## An extreme outlier...

Now I multiplied by 3 the observed value of bodyfat in observation 39.

```{r out.width="7cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
d.bodyfat_plusextreme <- d.bodyfat
d.bodyfat_plusextreme$bodyfat[39] <- d.bodyfat$bodyfat[39]*3
r.bodyfat_plusextreme <- lm(bodyfat ~ bmi, d.bodyfat_plusextreme)
#plot(r.bodyfat_plusextreme, which=5, smooth.colour = "red") + theme_bw()
plot(r.bodyfat_plusextreme, which=5, add.smooth = FALSE)
```

Some texts will give a rule of thumb that points with Cook’s distances greater than 1 should be considered influential, while other books claim a reasonable rule of thumb is  $4 / ( n - p - 1 )$
 where $n$ is the sample size, and $p$ is the number of $beta$ parameters.


## What can go "wrong" during the modeling process?

Answer: a lot of things!

* Non-linearity.
* Non-normal distribution of residuals.
* Heteroscadisticity (unequal variance).
* Important outliers.


## What to do when things "go wrong"?

1. Now: Transform the response and/or explanatory variables.
2. Now: Take care of outliers.
3. Later in the course: Improve the model, e.g., by adding additional terms or interactions.
4. Later in the course: Use another model family (generalized or nonlinear regression model).
5. Not in this course: Use weighted regression.


## Non-linearity

```{r out.width="10cm", out.height="5cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
eg_data <- tibble(x = runif(50)) %>%
  mutate(y = x ^ 3 + rnorm(50,0,0.1), 
         log10_y = log10(y),
         sqrt_y = sqrt(y))

p1 <- ggplot(eg_data, aes(x = x, y = y)) +
  geom_point() + 
  theme_bw() + geom_smooth(method = "lm", se = FALSE)

m1 <- lm(y ~ x, eg_data)

p2 <- ggplot(mapping = aes(x = fitted(m1), y = residuals(m1))) +
  geom_point() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed")


library(patchwork)

p1 + p2
```


## Transformation of the response?

Square root transform of the response variable $Y$:

```{r out.width="7cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
p3 <- ggplot(eg_data, aes(x = x, y = sqrt_y)) +
  geom_point() + 
  theme_bw() + geom_smooth(method = "lm", se = FALSE)
m2 <- lm(sqrt_y ~ x, eg_data)
p4 <- ggplot(mapping = aes(x = fitted(m2), y = residuals(m2))) +
  geom_point() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed")



(p1 + p2) / (p3 + p4)
```


## Another transformation

Log transformation of the response variable $Y$:

```{r out.width="7cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
p5 <- ggplot(eg_data, aes(x = x, y = log10_y)) +
  geom_point() + 
  theme_bw() + geom_smooth(method = "lm", se = FALSE)
m3 <- lm(log10_y ~ x, eg_data)
p6 <- ggplot(mapping = aes(x = fitted(m3), y = residuals(m3))) +
  geom_point() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed")

(p1 + p2) / (p3 + p4) / (p5 + p6)
```


## Common transformations

Which transformations should be considered to cure model deviation symptoms? There is no simple answer. But some guidelines. E.g. if we see nonlinearity and increasing variance with increasing fitted values, then a log transform may improve matter.

* This is why we look at if the model is good / appropriate before we start using it. I.e. we make sure our parachute is in good working order *before* we jump from the aeroplane.

Some common and useful \alert{first aid transformations} are:

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{itemize}
\item The log transformation for {\bf concentrations} and {\bf absolute values}.
\item The square-root ($\sqrt{\cdot}$) transformation for {\bf count data}.
\item The $\arcsin(\sqrt{\cdot})$ transformation for {\bf proportions/percentages}.
\end{itemize}
\end{minipage}}

These transformations can also be applied on explanatory variables!



## Outliers

What do we do when we identify the presence of one or more outliers?

1. Start by checking the "correctness" of the data. Is there a typo or a digital point that was shifted by mistake? Check both the response and explanatory variables.
2. If not, ask whether the model has been mis-specified. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?
3. Sometimes, an outlier may be the most interesting observation in a dataset!
4. Consider that outliers can also occur by chance!
5. Was the outlier created by some interesting but different process from the other data points.
6. Only if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.


## Deleting outliers

It might seem tempting to delete observations that apparently don't fit into the picture. However:

* Do this __only with absolute care__ e.g., if an observation has extremely implausible values!  
* Before deleting outliers, check points 1-6 from the previous slide.
* When deleting outliers or the x\% of most extreme observations, you __must mention this in your report__.




## Overview

* Why use (linear) regression?
* Fitting the line (= parameter estimation)
* Is linear regression good enough model to use?
* What to do when things go wrong?
* Transformation of variables/the response
* Handling of outliers

During the course we'll see many more examples of things going at least a bit wrong. And we'll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do next week in Lecture 4.

## Tasks until the next practical (Thu/Fri)

The idea of the course is that as a preparation for the practical part you will do the following:

* Consolidate your understanding of today's lecture.
* Go to OLAT and do all the "Homework" tasks.  


\bf $\rightarrow$ The same procedure applies to all course weeks.


