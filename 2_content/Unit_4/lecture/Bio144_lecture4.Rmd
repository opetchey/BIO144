---
title: "Lecture 4: Regression (continued) and multiple regression"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff & Owen Petchey"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## Recap of last week

* Why use linear regression?
* Fitting the line (least squares).
* Is the linear model good enough -- the five assumptions.
* What if something goes wrong (transformations and handling outliers)?


## Overview of this week

Regression continued...

* How well does the model describe the data: Correlation and $R^2$
* Are the parameter estimates compatible with some specific value (t-test)?
* What range of parameters values are compatible with the data (confidence intervals)?
* What regression lines are compatible with the data (confidence band)?
* What are plausible values of other data (prediction band)?

Multiple regression:
```{=tex}
\begin{itemize}
\item Multiple linear regression $x_1$, $x_2$, \ldots, $x_m$\\[2mm]
\item Checking assumptions\\[2mm]
\item $R^2$ in multiple linear regression\\[2mm]
\item $t$-tests, $F$-tests and $p$-values\\[2mm]
\end{itemize}
```

## Course material covered today

The lecture material of today is based on the following literature:\

```{=tex}
\begin{itemize}
\item Chapters 3.1, 3.2a-q of \emph{Lineare Regression}
\item Chapters 4.1 4.2f, 4.3a-e of \emph{Lineare Regression}
\end{itemize}
```


```{r echo = FALSE}
d.bodyfat <- read.table(here("3_datasets/bodyfat.clean.txt"),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
r.bodyfat <- lm(bodyfat ~ bmi, d.bodyfat)
```


## How good is the regression model?

This is, per se, a difficult question....

One often considered index is the __coefficient of determination (Bestimmtheitsmass)__ $R^2$.
Let us again look at the regression output form the bodyfat example:

```{r echo=TRUE, eval = TRUE}
summary(r.bodyfat)$r.squared
```

Compare this to the squared correlation between the two variables:

```{r echo = TRUE, eval = TRUE}
cor(d.bodyfat$bodyfat,d.bodyfat$bmi)^2
```


\colorbox{lightgray}{\begin{minipage}{10cm}
$\rightarrow$ In simple linear regression, $R^2$ is the squared correlation between the independent and the dependent variable.
\end{minipage}}

##

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{itemize} 
\item $R^2$ indicates the proportion of variability of the response variable ${y}$ that is \textbf{explained by the ensemble of all covariates}. 
\item Its value lies between 0 and 1.
\end{itemize}
\end{minipage}}  

\

The __larger__ $R^2$  
\
$\Rightarrow$  the \textbf{more} variability of ${y}$ is captured ("explained") by the covariate  
$\Rightarrow$ the \textbf{"better"} is the model.  

\
\scriptsize(However, it's a bit more complicated, as we will see in the multiple regression later in the lecture today)

## 

$R^2$ is also called the \emph{coefficient of determination} or
\alert{"Bestimmtheitsmass"}, because it measures the proportion of the
reponse's variability that is explained by the ensemble of all
explanatory variables:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
R^2 = SSQ^{(R)} / SSQ^{(Y)} = 1 - SSQ^{(E)}/ SSQ^{(Y)}
\end{equation*}
\end{minipage}}
```
 

With \begin{eqnarray*}
\text{total variability} &=&  \text{explained variability} + \text{  residual variability} \\[2mm]
\sum_{i=1}^n (y_i - \overline{y})^2 &=&  \sum_{i=1}^n (\hat{y_i}-\overline{y})^2 \qquad \quad + \quad \qquad \sum_{i=1}^n (y_i - \hat{y_i})^2 \\[2mm]
SSQ^{(Y)} &=& SSQ^{(R)} \qquad\qquad \quad + \qquad\quad\qquad SSQ^{(E)} \\[2mm]
\end{eqnarray*}

## 

This can be visualized for a model with only one predictor:\
 

```{r eval=T,fig.width=5, out.height="5cm", warning=FALSE,echo=F,out.width="12cm", fig.width = 12, fig.height = 5, fig.align='center'}
par(mfrow=c(1,2))
r.bodyft <- lm(bodyfat ~ bmi , d.bodyfat)
plot(bodyfat~bmi,d.bodyfat,cex.lab=1.5)
abline(h=mean(d.bodyfat$bodyfat),lty=2)
#abline(r.bodyfat)
text(35,15,expression(bar(y)),cex=1.5)
#text(35,40,expression(hat(y)),cex=1.5)
plot(bodyfat~bmi,d.bodyfat,cex.lab=1.5)
#abline(h=mean(d.bodyfat$bodyfat),lty=2)
abline(r.bodyfat)
#text(35,15,expression(bar(y)),cex=1.5)
text(35,40,expression(hat(y)),cex=1.5)



```





## Are the parameter estimates compatible with some specific value (t-test)?

*Important*: $\hat\beta_0$ and $\hat\beta_1$ are themselves \alert{random variables} and as such contain \alert{uncertainty}!

Let us look again at the regression output, this time only for the coefficients. The second column shows the *standard error* of the estimate:

```{r echo = FALSE}
d.bodyfat <- read.table(here("3_datasets/bodyfat.clean.txt"),header=T)
d.bodyfat <- d.bodyfat[,c("bodyfat","age","gewicht","hoehe","bmi","neck","abdomen","hip")]
r.bodyfat <- lm(bodyfat ~ bmi, d.bodyfat)
```


\ 
\tiny
```{r echo=T,eval=T}
summary(r.bodyfat)$coef

```

\
 
\normalsize
$\rightarrow$ The logical next question is: what is the distribution of the estimates?


## Distribution of the estimators for $\hat\beta_0$ and $\hat\beta_1$

To obtain an idea, we generate data points according to model

$$y_i = 4 - 2x_i + \epsilon_i \ , \quad \epsilon_i\sim N(0,0.5^2). $$
In each round, we estimate the parameters and store them:

```{r echo = FALSE, eval = TRUE}
set.seed(1)
```

\tiny
```{r echo = T, eval = T}
niter <- 1000
pars <- matrix(NA,nrow=niter,ncol=2)
for (ii in 1:niter){
  x <- rnorm(100)
  y <- 4 - 2*x + rnorm(100,0,sd=0.5)
  pars[ii,] <- lm(y~x)$coef
}
```

\normalsize
Doing it 1000 times, we obtain the following distributions for $\hat\beta_0$ and $\hat\beta_1$:

```{r eval=T,fig.width=6, fig.height=3,warning=FALSE,out.width="5cm", echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(cowplot)
pars <- data.frame(pars)
names(pars) <- c("alpha","beta")
 
p1 <-  ggplot(pars,aes(x=alpha)) + geom_histogram() +  theme_bw()
p2 <-  ggplot(pars,aes(x=beta)) + geom_histogram() +  theme_bw()
p <- plot_grid(p1,p2,  ncol = 2, rel_heights = c(1, .2))
p
```

## 

This looks suspiciously normal!

In fact, from theory it is known that  

\begin{eqnarray*}
\hat\beta_1 \sim N(\beta_1,{\sigma^{(\beta_1)2}}) & \quad \text{and} \quad & \hat\beta_0 \sim N(\beta_0,{\sigma^{(\beta_0)2}})
\end{eqnarray*}

For formulas of the variances ${\sigma^{(\beta_1)2}}$ and ${\sigma^{(\beta_0)2}}$, please consult Stahel 2.2.h.

\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf To remember:}
\begin{itemize}
\item $\hat\beta_0$ and $\hat\beta_1$ are \alert{unbiased estimators} of $\beta_0$ and $\beta_1$.
\item the parameters estimates $\hat\beta_0$ and $\hat\beta_1$ are \alert{normally distributed.}
\item the formulas for the variances depend on the residual variance $\sigma^2$, the sample size $n$ and the variability of $X$ (SSQ$^{(X)(\star)})$.
\end{itemize}
\end{minipage}}

$$^{(\star)}$$  $$\text{SSQ}^{(X)} = \sum_{i=1}^n (x_i-\overline{x})^2 $$

## 

With all this, we can calculate a standardised measure of the uncertainty in the parameter estimates, known as the *standard error*, or *SE*:

Standard error of parameter estimate: $se^{(\beta_1)} = \sqrt{\frac{\hat{\sigma}^2}{SSQ^{(X)}}}$

Estimated residual variance: $\hat{\sigma}^2 = \frac{1}{n-2}\sum_{i=1}^{n}{R_i^2}$

Residuals (also sometimes $e_i$): $R_i = Y_i - (\hat{\beta_0} + \hat{\beta_1}x_i)$

Sum of squares of $X$: $SSQ^{(X)} = \sum_{i=1}^{n}{(x_i - \bar{x})^2}$




## Are the parameter estimates compatible with some specific value (t-test)?

Let's first go back to the output from the bodyfat example: 

```{r echo = TRUE, eval = TRUE}
summary(r.bodyfat)$coef
```
\  
Besides the estimate and the standard error (which we discussed before), there is a \alert{\texttt{t value}} and a probability \alert{\texttt{Pr(>|t|)}} that we need to understand.

How do these things help us?



## Testing the "effect" of a covariate  

Remember: in a statistical test you first need to specify the \emph{null hypothesis}. Here, typically, the null hypothesis is

\colorbox{lightgray}{\begin{minipage}{14cm}
$$H_0: \quad \beta_{1} =  0  \ .$$
\begin{center}
In words: $H_0$ =   "no association" \\
\end{center}
\end{minipage}}



Here, the \emph{alternative hypothesis} is given by
\colorbox{lightgray}{\begin{minipage}{14cm}
$$H_A: \quad \beta_1 \neq  0  $$
\end{minipage}}

## 

Remember: To carry out a statistical test, we need a \emph{test statistic}.

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{center}
What is a test statistic? %\\[6mm]%(Clicker exercise!)
\end{center}
\end{minipage}}
\end{center}

\vspace{2mm}
$\rightarrow$ It is some type of \textbf{summary statistic} that follows a known distribution under $H_0$. For our purpose, we use the so-called \textbf{$T$-statistic} 

\begin{center}
\colorbox{lightgray}{\begin{minipage}{5cm}
\begin{equation}\label{eq:beta}
T=\frac{\hat\beta_1 - \beta_{1,H_0}}{se^{(\beta_1)}}\ . %\quad \text{with} \quad se^{(\beta_1)}=\sqrt{\hat\sigma_e^2/SSQ^{(X)}}  \ .
\end{equation}
\end{minipage}}
\end{center}

Again: typically, $\beta_{1,H_0}=0$, so the formula simplifies to $T=\frac{\hat\beta_1}{se^{(\beta_1)}}$.

Under $H_0$, $T$ has a $t$-distribution with $n-2$ degrees of freedom ($n=$ number of data points).

\small (You should try to recall the t-distribution. Check Mat183, keyword: t-test.)

## 

So let's again go back to the bodyfat regression output:

```{r echo = TRUE, eval = TRUE}
summary(r.bodyfat)$coef
```
\
Task: 
\
$\rightarrow$ Please use equation (1) to find out how the first three columns (Estimate, Std. Error and t value) are related! Check by a calculation...

Note: The last column contains the \textbf{$p$-value} of the test of the null hypothesis of $\beta_1=0$.


## Recap: Formal definition of the $p$-value

\vspace*{6pt}

\colorbox{lightgray}{\begin{minipage}{14cm}
The \textbf{formal definition of $p$-value} is the probability to observe a data summary (e.g., an average) that is at least as extreme as the one observed, given that the Null Hypothesis is correct.
\end{minipage}}  
\
Example (normal distribution): Assume that we calculated that $t$-value = -1.96  
\
$\Rightarrow$ $Pr(|t|\geq 1.96)=0.05$ and $Pr(t\leq-1.96)=0.025$.

\vspace*{-4pt}

```{r eval=TRUE, fig.align='center',fig.width=10, fig.height=4,echo = FALSE, out.height="5cm"}
par(mfrow=c(1,2))

zz1 <- qnorm(0.025)
zz2 <- qnorm(0.975)
zz3 <- qnorm(0.025)

cord.x1 <- c(-4,seq(-4,zz1,0.01),zz1)
cord.y1 <- c(0,dnorm(seq(-4,zz1,0.01)),0)

cord.x2 <- c(zz2,seq(zz2,4,0.01),4)
cord.y2 <- c(0,dnorm(seq(zz2,4,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="Two-sided p-value (0.05)",xlab="")
polygon(cord.x1,cord.y1,col='gray')
polygon(cord.x2,cord.y2,col='gray')
text(-3,0.05,labels="2.5%")
text(3,0.05,labels="2.5%")

cord.x3 <- c(-4,seq(-4,zz3,0.01),zz3)
cord.y3 <- c(0,dnorm(seq(-4,zz3,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="One-sided p-value (0.025)",xlab="")
polygon(cord.x3,cord.y3,col='gray')
text(-3,0.05,labels="2.5%")
```

## 

The regression output from R indicates that the $p$-value for BMI is very small ($p<0.0001$).

Conclusion: there is __very strong evidence__ that the BMI is associated with bodyfat, because $p$ is extremely small (thus it is very unlikely that such a slope $\hat\beta_1$ would be seen if there was no association of BMI and body fat).

This basically answers question 1: "Are the parameters compatible with some specific value?"


## A cautionary note on the use of $p$-values

Maybe you have seen that in statistical testing, often the criterion $p\leq 0.05$ is used to test whether $H_0$ should be rejected. This is often done in a black-or-white manner.

\colorbox{lightgray}{\begin{minipage}{14cm}
However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 
\end{minipage}}


##  What range of parameters values are compatible with the data (confidence intervals)?

To answer this question, we can determine the confidence intervals of the regression parameters.

__Facts we know about $\hat\beta_1$__

* $\hat\beta_1$ is estimated with a standard error of $\sigma^{(\beta_1)}$
* The distribution of $\hat\beta_1$ is normal, namely $\hat\beta_1\sim N(\beta_1,\sigma^{(\beta_1)2})$.
* However, since we need to estimate $\sigma^{(\beta_1)}$ from the data, we have a $t$-distribution.

## 

Doing some calculations (similar to those in chapter 8.2.2 of Mat183 script) leads us to the 95\% confidence interval  

\begin{center}
\colorbox{lightgray}{\begin{minipage}{6cm}
$$[\hat\beta_1 - c \cdot \hat\sigma^{(\beta_1)} ; \hat\beta_1 + c \cdot \hat\sigma^{(\beta_1)}] \ ,$$
\end{minipage}}
\end{center}

where $c$ is the 97.5\% quantile of the $t$-distribution with $n-2$ degrees of freedom.

Doing this for the bodfat example "by hand" is not hard. We have 241 degrees of freedom:

```{r echo = TRUE, eval = TRUE}
coefs <- summary(r.bodyfat)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
```

## 

Even easier: directly ask R to give you the CIs.

```{r echo = TRUE, eval = TRUE}
confint(r.bodyfat,level=c(0.95))
```

In summary,
```{r results='asis', echo = FALSE}
tableRegression(r.bodyfat)
```

\underline{Interpretation:} for an increase in the bmi by one index point, roughly 1.82\% percentage points more bodyfat are expected, and all true values for $\beta_1$ between 1.61 and 2.03 are compatible with the observed data.

## Confidence and Prediction Bands

* Remember: When another sample from the same population was taken, the regression line would look slightly different.  

* There are two questions to be asked:  


1. Which other regression lines are compatible with the observed data?
\
$\Rightarrow$ This leads to the \textbf{confidence band}.
 
2. Where do future observations with a given $x$ coordinate lie?
\
$\Rightarrow$ This leads to the \textbf{prediction band}.

## Bodyfat example

\begin{center}
```{r eval=TRUE, fig.width=5, fig.height=5,out.width="6cm", echo=FALSE}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c("confidence band (95%)", "prediction band (95%)"),
lty=8, cex=1,col=c(2,4),lwd=2)
```
\end{center}

Note: The prediction band is much broader than the confidence band.

## Calculation of the confidence band

Given a fixed value of $x$, say $x_0$. The question is: 

\colorbox{lightgray}{\begin{minipage}{14cm}
Where does $\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0$ lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}

This question is not trivial, because both $\hat\beta_0$ and $\hat\beta_1$ are estimates from the data and contain uncertainty. 

The details of the calculation are given in Stahel 2.4b. 

\colorbox{lightgray}{\begin{minipage}{14cm}
Plotting the confidence interval around all $\hat y_0$ values one obtains the \textbf{confidence band} or \textbf{confidence band for the expected values} of $y$.
\end{minipage}}

Note: For the confidence band, only the uncertainty in the estimates $\hat\beta_0$ and $\hat\beta_1$ matters.

## 


```{r eval=TRUE, fig.width=5,fig.height=5, out.width='7cm', echo = FALSE, fig.align='center'}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Confidence band",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
legend("bottomright", c("confidence band (95%)"),
lty=8, cex=1,col=c(2),lwd=2)
```


## Calculations of the prediction band

Given a fixed value of $x$, say $x_0$. The question is:  

\colorbox{lightgray}{\begin{minipage}{10cm}
Where does a {\bf future observation} lie with a certain confidence (i.e., 95\%)? 
\end{minipage}}

To answer this question, we have to \alert{consider not only the uncertainty in the predicted value} $\hat y_0 =  \hat\beta_0 + \hat\beta_1 x_0$, but also the \alert{error in the equation $\epsilon_i \sim N(0,\sigma^2)$}.  

This is the reason why the \bf{prediction band is always wider than the confidence band}.

## 

```{r eval=T,fig.width=5,fig.height=5,out.width="7cm",echo=FALSE, fig.align='center'}
t.range <- range(d.bodyfat$bmi)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(r.bodyfat,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(d.bodyfat$bmi,d.bodyfat$bodyfat,main="Prediction band",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(r.bodyfat,lwd=2)
lines(x=t.xwerte,y=t.vorh[,2],lty=8,lwd=2,col=4)
lines(x=t.xwerte,y=t.vorh[,3],lty=8,lwd=2,col=4)
legend("bottomright", c( "prediction band (95%)"),
lty=8, cex=1,col=c(4),lwd=2)
```



## That is regression done (at least for our current purposes)

* Why use (linear) regression?
* Fitting the line (= parameter estimation)
* Is linear regression good enough model to use?
* What to do when things go wrong?
* Transformation of variables/the response.
* Handling of outliers.
* Goodness of the model: Correlation and $R^2$
* Tests and confidence intervals
* Confidence and prediction bands

(Homework and Practical class: Presentation of findings)

## Multiple linear regression

Multiple continuous explanatory variables.

* Question 1: Are the explanatory variables (i.e. more than one) associated with the response?
* Question 2: Which variables are associated with the response?
* Question 3: What proportion of variability is explained?



## Bodyfat example

We have so far modeled bodyfat in dependence of bmi, that is:
$(body fat)_i = \beta_0 + \beta_1 \cdot bmi_i + \epsilon_i$.\


However, other explanatory variables might also be relevant for an
accurate prediction of bodyfat.

\textbf{Examples:} Age, neck fat (Nackenfalte), hip circumference,
abdomen circumference etc.

```{r eval=T,fig.width=6, fig.height=3,warning=FALSE,echo=F,out.width="9.5cm", fig.align='center', message=FALSE}
par(mfrow=c(1,3))
plot(bodyfat ~ age,d.bodyfat,xlab="age", ylab="bodyfat (y)")
plot(bodyfat ~ neck,d.bodyfat,xlab="neck", ylab="bodyfat (y)")
plot(bodyfat ~ hip,d.bodyfat,xlab="hip", ylab="bodyfat (y)")
```

## 

**Multiple linear regression** is when we have more than one explanatory
variable. We can then ask three questions:

```{=tex}
\begin{enumerate}
\item Is the \textbf{ensemble} of all explanatory variables associated with the response?\\[4mm]
\item If yes, which explanatory variables are associated with the response? \\[4mm]
\item What proportion of response variability ($SSQ^{(Y)}$) is explained by the model?
\end{enumerate}
```
## Multiple linear regression model

The idea is simple: Just
\textbf{extend the linear model by additional predictors}.

```{=tex}
\begin{itemize}
\item Given several influence explanatory variables $x_i^{(1)}$, \ldots, $x_i^{(m)}$, 
the straightforward extension of the simple linear model is\\[4mm]

\colorbox{lightgray}{\begin{minipage}{10cm}
\vspace{-3mm}
\begin{eqnarray*}
y_i &=& \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_m x_i^{(m)} + \epsilon_i  \\[2mm]
\text{with  } \ \epsilon_i &\sim& N (0,\sigma^2).
\end{eqnarray*}
\end{minipage}}
~\\[6mm]
\item The parameters of this model are $\beta=(\beta_0,\beta_1,\ldots,\beta_m)$ and $\sigma^2$.
\end{itemize}
```
## 

The components of $\beta$ are again estimated using the
\textbf{least squares} method. Basically, the idea is (again) to
minimize

$$\sum_{i=1}^n e_i^2$$ with
$$e_i = y_i - (\beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_m x_i^{(m)}) $$

It is a bit more complicated than for simple linear regression, see
Section 3.4 of the Stahel script.\
 

Some \textbf{linear algebra} is needed to understand these sections; we look at this in Lecture 7.

## Multiple linear regression for bodyfat

Let us regress the proportion (%) of bodyfat (from last week) on the
predictors \textbf{bmi} and \textbf{age} simultaneously. The model is
thus given as

```{=tex}
\begin{eqnarray*}
(bodyfat)_i &=& \beta_0 + \beta_1 \cdot bmi_i + \beta_2 \cdot age_i + \epsilon_i \ , \\
\text{with} \quad \epsilon_i &\sim& N(0,\sigma^2) \ .
\end{eqnarray*}
```
## Multiple linear regression with R

Let's now fit the model with R, and quickly glance at the output:

\tiny

```{r eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
r.bodyfatM <- lm(bodyfat ~ bmi + age, d.bodyfat) 

summary(r.bodyfatM)
```

## Model checking

Before we look at the results, we must check if the modelling
assumptions are fulfilled (check our 'chute before we jump):

```{=tex}
\begin{center}

```{r eval=T,fig.width=6, fig.height=6,warning=FALSE,echo=F,out.width="5cm", message=FALSE}
library(ggfortify)
autoplot(r.bodyfatM, which=c(1,2,3,5),smooth.colour=NA)
```

\end{center}
```
This seems ok, so continue with answering questions 1-3.

## Question 1: Are the explanatory variables associated with the response?

To answer question 1, we need to perform a so-called \alert{$F$-test}.
The results of the test are displayed in the final line of the
regression summary. Here, it says:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\texttt{F-statistic: 165.9 on 2 and 240 DF, p-value: < 2.2e-16}
\end{minipage}}
```
So apparently (and we already suspected that) the model has some
explanatory power.\
 

\scriptsize

\*The $F$-statistic and -test is briefly recaptured in 3.1.f) of the
Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact that

```{=tex}
\begin{equation*}
\frac{SSQ^{(R)}/m}{SSQ^{(E)}/(n-p)} \sim F_{m,n-p}
\end{equation*}
```
follows an $F$-distribution with $m$ and $(n-p)$ degrees of freedom,
where $m$ are the number of variables, $n$ the number of data points,
$p$ the number of $\beta$-parameters (typically $m+1$).
$SSQ^{(E)}=\sum_{i=1}^nR_i^2$ is the squared sum of the residuals, and
$SSQ^{(R)} = SSQ^{(Y)} - SSQ^{(E)}$ with
$SSQ^{(y)}=\sum_{i=1}^n(y_i-\overline{y})^2$.

## 

$n$ is the number of data points

$m$ is the number of explanatory variables in the regression model

$p$ is the number of beta parameters estimated (e.g. intercept, plus a
slope for each explanatory variable, hence $p = m + 1$)

And the degrees of freedom for error are $n - p$

```{r eval = TRUE, echo = FALSE}
y <- d.bodyfat$bodyfat
n<-nrow(d.bodyfat)
m <- 2
p <- m+1
ssQE <- sum((predict(r.bodyfatM)-y)^2) 
ssQR <- sum((y-mean(y))^2) - ssQE
F <- ssQR/m / (ssQE/(n-p))
invisible(pf(F,m,n-p,lower.tail=FALSE))
```

## Question 2: Which variables are associated with the response?

\tiny

```{r echo = TRUE, eval = TRUE}
summary(r.bodyfatM)$coef
```

\normalsize

To answer this question, again look at the \alert{$t$-tests}, for which
the $p$-values are given in the final column. Each $p$-value refers to
the test for the null hypothesis $\beta^{(j)}_0=0$ for explanatory
variable $x^{(j)}$.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
As in simple linear regression, the $T$-statistic for the $j$-th explanatory variable is calculated as 

\begin{equation}\label{eq:beta}
T_j =  \frac{\hat\beta_j}{se^{(\beta_j)}}\ ,
\end{equation}
with  $se^{(\beta_j)}$ given in the second column of the regression output.
\end{minipage}}
```
 

The distribution of this statistic is $T_j \sim t_{n-p}$.

## 

Therefore: A "small" $p$-value indicates that the variable is relevant
in the model.\
 

Here, we have

```{=tex}
\begin{itemize}
\item $p<0.001$ for bmi
\item $p<0.001$ for age
\end{itemize}
```
Thus both, bmi and age seem to be associated with bodyfat.

Again, a 95% CI for $\hat\beta_j$ can be calculated with R:\
 

\tiny

```{r echo = TRUE}
confint(r.bodyfatM)
```

\scriptsize (The CI is again
$[\hat\beta - c \cdot \sigma^{(\beta)} ; \hat\beta + c \cdot \sigma^{(\beta)}]$,
where $c$ is the 97.5% quantile of the $t$-distribution with $n-p$
degrees of freedom; compare to slides 38-40 of last week).

## 

\textbf{!However!: }

The $p$-value and $T$-statistics should only be used as a
\textbf{rough guide} for the "significance" of the coefficients.

For illustration, let us extend the model a bit more, including also
neck, hip and abdomen:

```{r results="asis",echo=F, message=FALSE, warning=FALSE}
r.bodyfatM2 <- lm(bodyfat ~ bmi + age + neck + hip + abdomen,d.bodyfat)
tableRegression(r.bodyfatM2)
```

It is now much
\alert{less clear how strongly age ($p=0.60$) and bmi ($p=0.07$) are associated with bodyfat}.

## 

Basically, the problem is that the
\alert{variables in the model are correlated} and therefore explain
similar aspects of bodyfat. \textbf{Example:} Abdomen (Bauchumfang)
seems to be a relevant predictor and it is obvious that abdomen and BMI
are correlated:

```{r eval=TRUE, fig.width=3.5,fig.height=3.5,warning=FALSE,echo=F,out.width="4cm",fig.align='center'}
ggplot(d.bodyfat, aes(y=abdomen ,x=bmi)) + geom_point()
```

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
This problem of \alert{collinearity} is at the heart of many confusions of regression analysis, and we will talk about such issues later in the course (lectures 8 and 9).
\end{minipage}}
```
Please see also IC: practical 4 (milk example) for an analysis and more
thoughts.

## Question 3: Which proportion of variability is explained?

To answer this question, we can look at the \alert{multiple $R^2$} (see
Stahel 3.1.h). It is a generalized version of $R^2$ for simple linear
regression:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
$R^2$ {\bf for multiple linear regression} is defined as the squared correlation between $(y_1,\ldots,y_n)$ and $(\hat{y}_1,\ldots,\hat{y}_n)$, where the $\hat y$ are the fitted values 
\begin{equation*}
\hat{y}_i = \hat\beta_0 + \hat\beta_1 x^{(1)} + \ldots + \hat\beta_m x^{(m)}
\end{equation*}
\end{minipage}}
```
 

```{=tex}
\begin{center}

```{r eval=T,fig.width=4, fig.height=4.3,warning=FALSE,echo=F,out.width="4.0cm",fig.align='center',message=FALSE}
par(mfrow=c(1,1))
plot(r.bodyfatM$fitted.values,d.bodyfat$bodyfat,xlab=expression(paste("fitted values (",hat(y),")")),ylab="true values y")
abline(c(0,1))
```

\end{center}
```

## 

Let us look at the $R^2$s from the three bodyfat models  

model r.bodyfat: $y\sim bmi$ 

model r.bodyfatM: $y\sim bmi + age$ 

model r.bodyfatM2: $y\sim bmi + age + neck + hip + abdomen$: \tiny

```{r echo = TRUE}
summary(r.bodyfat)$r.squared
summary(r.bodyfatM)$r.squared
summary(r.bodyfatM2)$r.squared
```

\normalsize

The models explain
`r round(summary(r.bodyfat)$r.squared*100,0)`%, 
`r round(summary(r.bodyfatM)$r.squared*100,0)`% and
`r round(summary(r.bodyfatM2)$r.squared*100,0)`%
of the total
variability of $y$.

It thus \emph{seems} that larger models are "better". However, $R^2$
does always increase when new variables are included, but this does not
mean that the model is more reasonable.

## Adjusted $R^2$

When the sample size $n$ is small with respect to the number of
variables $m$ included in the model, an \alert{adjusted} $R^2$ gives a
better ("fairer") estimation of the actual variability that is explained
by the explanatory variables:

\begin{equation*}
R^2_a = 1-(1-R^2 )\frac{n-1}{n-m-1}
\end{equation*}  

Why $R^2_a$?

It \textbf{penalizes for adding more variables} if they do not really
improve the model!

\textbf{Note:} $R_a$ may decrease when a new variable is added.

## Interpretation of the coefficients

Apart from model checking and thinking about questions 1-3, it is
probably even \textbf{more important to understand what you \emph{see}.}
Look at the output and ask yourself:

```{=tex}
\begin{center}
\textcolor{blue}{\bf What does the regression output actually \emph{mean}?}
\end{center}
```
```{r echo=FALSE, results='asis'}
tableRegression(r.bodyfatM,caption="Parameter estimates of model 2.", label="tab:m3")
```

Task in teams: Interpret the coefficients, 95% CIs and $p$-values.

## Example: Catheter Data

```{r echo = FALSE, eval = TRUE}
path <- "../../../5_some_examples/data_examples/WBL/"
d.cath <- read.table(paste(path,"catheter.dat",sep=""),header=T)
r.cath <- lm(y~x1+x2,data=d.cath)
```

Catheter length ($y$) for heart surgeries depending on two
characteristic variables $x^{(1)}$ and $x^{(2)}$ of the patients.

Aim: estimate $y$ from $x^{(1)}$ and $x^{(2)}$ ($n=12$).

Again look at the data first ($x^{(1)}$ and $x^{(2)}$ are highly
correlated!):

```{r eval=T,fig.width=5, fig.height=5,warning=FALSE,echo=F,out.width="5cm", fig.align='center'}
pairs(d.cath)
```

## 

Regression results with both variables:
$R^2=`r round(summary(r.cath)$r.squared,2)`$, $R^2=`r round(summary(r.cath)$adj.r.squared,2)`$, $F$-test $p=0.0006$.

```{r results='asis', echo=FALSE, message=FALSE}
tableRegression(r.cath)
```

With $x_1$ only: $R^2=0.78, R_a^2=0.75$, $F$-test $p=0.0002$

```{r results='asis', echo=FALSE}
tableRegression(lm(y~x1,d.cath))
```

With $x_2$ only: $R^2=0.80, R_a^2=0.78$, $F$-test $p=0.0001$

```{r echo = FALSE, results='asis'}
tableRegression(lm(y~x2,d.cath))
```

## 

Questions to consider:

```{=tex}
\begin{enumerate}
\item Is $x_1$ an important explanatory variable?
\item Is $x_2$ an important explanatory variable?
\item Are both explanatory variables needed in the model?
\item Interpretation of the results?\\[4mm]
\end{enumerate}
```




## Recap

* How well does the model describe the data: Correlation and $R^2$
* Are the parameter estimates compatible with some specific value (t-test)?
* What range of parameters values are compatible with the data (confidence intervals)?
* What regression lines are compatible with the data (confidence band)?
* What are plausible values of other data (prediction band)?

Multiple regression:
```{=tex}
\begin{itemize}
\item Multiple linear regression $x_1$, $x_2$, \ldots, $x_m$\\[2mm]
\item Checking assumptions\\[2mm]
\item $R^2$ in multiple linear regression\\[2mm]
\item $t$-tests, $F$-tests and $p$-values\\[2mm]
\end{itemize}
```




## Next steps

* Homework.
* Practical.
* Then week 5: Binary/categorical explanatory variables, and interactions


