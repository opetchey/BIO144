---
title: "Lecture 5: Binary/categorical explanatory variables, and interactions"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff, Owen Petchey & Uriah Daugaard"
institute: "University of Zurich"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
---
  
```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
library(here)
```


## Overview

* Binary and categorical explanatory variables
* Interactions between explanatory variables
* Multiple vs.\ many single regressions
<!-- * Recap of checking model assumptions -->


## Course material covered today

The lecture material of today is based on the following literature:

* Chapters 3.2u-x, 3.3, 4.1-4.5 in \emph{Lineare Regression} 

## Recap of last week

* Intepretation of a regression model:
* How well does the model describe the data: Correlation and $R^2$
* Are the parameter estimates compatible with some specific value (t-test)?
* What range of parameters values are compatible with the data (confidence intervals)?
* What regression lines are compatible with the data (confidence band)?
* What are plausible values of other data (prediction band)?


* Multiple linear regression model $y_i= \beta_0 + \beta_1 x_i^{(1)}+\beta_2x_i^{(2)}+\ldots + \beta_m x_i^{(m)}+\epsilon_i$.





## Binary explanatory variables

So far, the explanatory variables $x$ were always continuous.

In reality, there are
\textbf{no restrictions assumed with respect to the $x$ variables}.

One very frequent data type are \textbf{binary} variables, that is,
variables that can only attain values 0 or 1.

See section 3.2c of the Stahel script:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
If the binary variable $x$ is the only variable in the model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, the model has only two predicted outcomes (plus error):\\
\begin{equation*}
y_i = \left\{ 
\begin{array}{ll}
 \beta_0  + \epsilon_i \quad &\text{if } x_i=0 \\
 \beta_0 + \beta_1 + \epsilon_i \quad &\text{if } x_i =1\\
\end{array}
\right .
\end{equation*}
\end{minipage}}
```

## Sketch


## Example: Smoking variable in Hg Study

For the 59 mothers in the Hg study, check if their smoking status
(0=no,1=yes) influences the Hg-concentration in their urine.

We fit the following linear regression model:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
\log(Hg_{urin})_i  = \beta_0 +  \beta_1 \cdot x^{(1)}_i +  \beta_2\cdot x^{(2)}_i + \beta_3 \cdot x^{(3)}_i + \epsilon_i \ ,
\end{equation*}
\end{minipage}}
```
Where

```{=tex}
\begin{itemize}
\item $\log(Hg_{urin})$ is the urine mercury concentration.
\item $x^{(1)}$ is the binary smoking indicator (0/1).
\item $x^{(2)}$ the number of amalgam fillings.
\item $x^{(3)}$ the monthly number of marine fish meals.
\end{itemize}
```
\scriptsize(Assume that we already looked at the data and see that log of Hg concentrations is needed.)

## 

```{r echo = FALSE, message=FALSE,warning=FALSE}
path <- "../../../3_datasets/"
d.hg <- read.table(paste(path,"hg_urine_fuzzed.csv",sep=""),header=T, sep=",")
d.hg <- d.hg[d.hg$mother==1,-c(3,4,9,10,11)]
#names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish")

r1.urin.mother <- lm(log10(Hg_urin) ~  smoking +
                     + amalgam + fish,data=d.hg)
```

First check the modelling assumptions:


```{r eval=T,fig.width=6, fig.height=6,warning=FALSE,echo=F,out.width="6cm",fig.align='center'}
autoplot(r1.urin.mother,which=c(1,2, 3, 5),smooth.colour=NA) 
```


Seems ok, apart from one point (106) that could be categorized as an
outlier. We ignore it for the moment.

## 

The results table is given as follows:

\small

```{r results='asis',echo=FALSE}
tableRegression(r1.urin.mother)
```

There is some weak ($p=0.073$) indication that smokers have an increased
Hg concentration in their body. Their $\log(Hg_{urin})$ is in average by
0.26 (log10 units) higher than for nonsmokers.

In principle, we have now -- at the same time -- fitted
\textbf{two models:} one for smokers and one for non-smokers, assuming
that the slopes of the remaining explanatory variables are the same for
both groups.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
Smokers: $y_i = -1.03 + 0.26  + 0.098\cdot amalgam _i+ 0.032\cdot fish_i + \epsilon_i$ \\
Non-smokers:  $y_i = -1.03  + 0.098\cdot amalgam_i + 0.032\cdot fish_i  + \epsilon_i$ 
\end{minipage}}
```

## Categorical explanatory variables

Some explanatory variables indicate a \textbf{category}, for instance
the species of an animal or a plant. This type of explanatory variable
is termed \textbf{categorical}. For this there is trick: we can convert
a categorical variable with $k$ levels (for instance 3 species) into $k$
dummy variables $x_i^{(j)}$ with

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
x_i^{(j)} = \left\{ 
\begin{array} {ll}
1, & \text{if the $i$th observation belongs to group $j$}.\\
0, & \text{otherwise.}
\end{array}\right.
\end{equation*}
\end{minipage}}
```
Each of the explanatory variables $x^{(1)},\ldots, x^{(k)}$ can then be
included as a binary variable in the model

```{=tex}
\begin{equation*}
y_i = \beta_0 + \beta_1 x^{(1)}_i + \ldots + \beta_k x^{(k)}_i + \epsilon_i \ .
\end{equation*}
```
\scriptsize However: this model is \alert{not identifiable}. We could
add a constant to $\beta_1, \beta_2, ...\beta_k$ and subtract it from
$\beta_0$, and the model would fit equally well to the data, so it
cannot be decided which set of the parameters is best.

## Sketch (1)





## Categorical explanatory variables (duplicate slide)

Some explanatory variables indicate a \textbf{category}, for instance
the species of an animal or a plant. This type of explanatory variable
is termed \textbf{categorical}. For this there is trick: we can convert
a categorical variable with $k$ levels (for instance 3 species) into $k$
dummy variables $x_i^{(j)}$ with

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
x_i^{(j)} = \left\{ 
\begin{array} {ll}
1, & \text{if the $i$th observation belongs to group $j$}.\\
0, & \text{otherwise.}
\end{array}\right.
\end{equation*}
\end{minipage}}
```
Each of the explanatory variables $x^{(1)},\ldots, x^{(k)}$ can then be
included as a binary variable in the model

```{=tex}
\begin{equation*}
y_i = \beta_0 + \beta_1 x^{(1)}_i + \ldots + \beta_k x^{(k)}_i + \epsilon_i \ .
\end{equation*}
```
\scriptsize However: this model is \alert{not identifiable}. We could
add a constant to $\beta_1, \beta_2, ...\beta_k$ and subtract it from
$\beta_0$, and the model would fit equally well to the data, so it
cannot be decided which set of the parameters is best.


## Solution...

\textbf{Solution: } One of the $k$ categories must be selected as a
\emph{reference category} and is \emph{included in the model as the intercept}.
Typically: the alphabetically first category is the reference, thus $\beta_1=0$.

The model thus discriminates between the categories, such that (assuming
$\beta_1=0$)

```{=tex}
\begin{equation*}
\hat y_i = \left\{
\begin{array}{ll}
\beta_0 , & \text{if $x_i^{(1)}=1$ }\\
\beta_0 + \beta_2 , & \text{if $x_i^{(2)}=1$ }\\
...\\
\beta_0 + \beta_k , & \text{if $x_i^{(k)}=1$ } \ .
\end{array}\right.
\end{equation*}
```



## Sketch (2)



## Sketch (3)




## !!Important to remember!!

\textcolor{gray}{(Common aspect that leads to confusion!)} 

Please note that
\alert{a categorical variable with $k$ categories requires $k-1$ parameters!}

$\rightarrow$ The \alert{degrees of freedom} are also reduced by $k-1$.


<!-- Note 2024 by Uriah: I changed this slide for this year, before it was just a sketch -->

<!-- ## Sketch (about degrees of freedom, 1) -->

## Degrees of freedom DF, example

* When we calculate something from the data and use it then we constrain the data $\rightarrow$ it has less "freedom"

* For example, to calculate $\text{SSQ}^{(Y)} = \sum_{i=1}^n (y_i - \overline{y})^2$ we need $\overline{y}$

  * Now let's say we know that $\overline{y}=4$ & $n=5$, what values can $y_i$ have?
  
  * 4 out of the 5 $y_i$ can have whichever value, e.g. $y_i=\{2,5,3,6,\Box\}$, but given the first 4 values and given $\overline{y}$ & $n$, we know that the fifth value must be $\Box=4$ $\Rightarrow$ the fifth value is no longer free to vary
  
  * Thus, for the calculation of $\text{SSQ}^{(Y)}$ we have $n-1$ degrees of freedom (DF), because we used 1 DF to calculate $\overline{y}$ from $y_i$ (in other words, $\overline{y}$ and $y_i$ are not independent)
  
* The DF are the number of data points that are free to vary, given what we want to calculate

  * **Note**: We lose one DF for every beta parameter that we want to estimate!

## Sketch (about degrees of freedom, 2)




## Example: Earthworm study

\tiny (Angelika von Förster und Burgi Liebst)

\scriptsize Die Dachse im Sihlwald ernähren sich zu einem grossen
Prozentsatz von Regenwürmern. Ein Teil des Muskelmagens der Regenwürmer
wird während der Passage durch den Dachsdarm nicht verdaut und mit dem
Kot ausgeschieden. Wenn man aus der Grösse des Muskelmagenteilchens auf
das Gewicht des Regenwurms schliessen kann, ist die Energiemenge
berechnenbar, die der Dachs aufgenommen hat.

\normalsize

\textbf{Frage:} Besteht eine Beziehung zwischen dem Umfang des
Muskelmagenteilchens und dem Gewicht des Regenwurms?

Data set of $n=143$ worms with three genera (Lumbricus, Nicodrilus,
Octolasion), weight, stomach circumference (Magenumfang).

## 

Data inspection suggests that the three genera have different weights:


```{r eval=T,fig.width=12, fig.height=6,warning=FALSE,echo=FALSE,out.width="12.cm",fig.align='center',message=FALSE}
library(lattice)
par(mfrow=c(1,2))
d.wurm <- read.table (here("5_some_examples/data_examples/ancova/Projekt6Regenwuermer/RegenwuermerDaten_Haupt.txt"),header=T)
d.wurm[d.wurm$Gattung=="N","GEWICHT"] <- d.wurm[d.wurm$Gattung=="N","GEWICHT"]*0.5
boxplot(GEWICHT ~ Gattung, d.wurm,xlab="Gattung",ylab="Gewicht (g)",cex.lab=1.5, cex.axis=1.5)
boxplot(log10(GEWICHT) ~ Gattung, d.wurm,xlab="Gattung",ylab="Log10 Gewicht (g)",cex.lab=1.5, cex.axis=1.5)
```



```{r eval=FALSE, fig.align='center', fig.height=2.9, fig.width=4.2, message=FALSE, warning=FALSE, include=FALSE, out.width="5.5cm"}
library(ggplot2)
ggplot(d.wurm,aes(x=MAGENUMF,y= GEWICHT,colour=Gattung)) + geom_point() + theme_bw()
```



##

Formulate a model with $\log_{10}(\text{Gewicht})$ as response and `Gattung` as the explanatory variable.

\tiny

```{r echo = TRUE, eval = TRUE}
r.lm <- lm(log10(GEWICHT) ~  Gattung, d.wurm)
```

\normalsize

Before doing anything else, check the modeling assumptions:

```{=tex}
\begin{center}
```{r eval = TRUE, fig.width=5,fig.height=5,warning=FALSE,echo=F,out.width="6cm",fig.align='center',message=FALSE}
autoplot(r.lm,which=c(1,2, 3, 5),smooth.colour=NA) 
```
\end{center}
```

## 

\textbf{Results:}

```{r results='asis', echo = FALSE}
tableRegression(r.lm)
```

$R^2=0.33$, $R^2_a = 0.32$.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{itemize}
\item Question: Why is Gattung Lumbricus (L) not in the results table? \\
\item Answer: L was chosen as the ``reference category'', thus $\beta_L=0$. 
\end{itemize}
\end{minipage}}
```
\textbf{Degrees of freedom:} We had 143 data points. How many degrees of
freedom are left for the residual error? 



```{r echo = FALSE}
worm.coef <- summary(r.lm)$coef
```

## Interpreting the results I

```{=tex}
\begin{itemize} 
\item $\beta_0=`r format(worm.coef[1,1],2,2,2)`$ is the intercept.
\item $\beta_2=`r round(worm.coef[2,1],2)`$ is the coefficient for Gattung=Nicodrilus.
\item $\beta_3=`r format(worm.coef[3,1],2,2,2)`$ is the coefficient for Gattung =Octolasion.
\item No coefficient is needed for Gattung Lumbricus, because $\beta_L=0$.\\[6mm]
\end{itemize}
```
```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
We have now actually fitted {\bf three} models, one model for each genus:\\[2mm]

Lumbricus: $\hat{y}_i = `r format(worm.coef[1,1],2,2,2)`$ \\[2mm]
Nicodrilus: $\hat{y}_i = `r format(worm.coef[1,1],2,2,2)` + (`r round(worm.coef[2,1],2)`)$ \\[2mm]
Octolasion: $\hat{y}_i = `r format(worm.coef[1,1],2,2,2)` + (`r format(worm.coef[3,1],2,2,2)`)$
\end{minipage}}
```


## Interpreting the results II

\textbf{Question:} Is the "Gattung" explanatory variable relevant in the
model, that is, do the model intercepts differ for the three genera? 

\textbf{Problem:} The $p$-values of the t-test for each of the worm genus are not very
meaningful. They belong to tests that compare the intercept of a
category with the intercept of the reference level (i.e., the
\emph{difference} in intercepts!). However, the question is whether the
variable `Gattung` has an effect in total.  

\textbf{Solution:} To test if a categorical explanatory variable explains a significant amount of variability, we use an $F$-test (we saw this in regression; it is a "variance ratio" test.}

## $F$-test

```{=tex}
\begin{center}
\includegraphics[width=11cm]{pictures/Ftest.png}
\end{center}
```


## $F$-test: does the categorical variable *Gattung* explain a significant amount of variability?

\vspace*{-10pt}

$F\text{-statistic} =\frac{\text{additional variability explained by including } Gattung}{\text{residual variability not explained}}=$

\vspace*{60pt}


\par\noindent\rule{\textwidth}{0.4pt}

\vspace*{-6pt}

Remember, $SST = SSR + SSE$

\vspace*{40pt}


## $F$-test (sketch)

\vspace*{50pt}


```{r, fig.width=8, fig.height=3.5, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(patchwork)

L1 = c(1.8,1.6,2.6)
N1 = c(6.3,5,6.7) + 0.5
Oc1 = c(10.4,9.1,10.5) - 0.5
mean1 = mean(c(L1,N1,Oc1))

Fdd1 <- data.frame(Gattung = rep(c("L","N","Oc"),each=3),
                   y = c(L1,N1,Oc1))

p1 <-
  Fdd1 %>% 
  ggplot(aes(Gattung, y))+
  geom_hline(yintercept = mean1, linetype=2)+
  geom_point(position = position_dodge2(width = 0.3), col="red", size=.9) +
  theme_half_open()+
  theme(axis.title.y = element_text(angle = 0, vjust = 1))+
  scale_y_continuous(breaks = mean1, labels = expression(bar("y")), limits = c(min(Fdd1$y),max(Fdd1$y))) +
  geom_segment(aes(x = 0.85, y = mean(L1), xend = 1.15, yend = mean(L1)), colour = "red")+
  geom_segment(aes(x = 1.85, y = mean(N1), xend = 2.15, yend = mean(N1)), colour = "red")+
  geom_segment(aes(x = 2.85, y = mean(Oc1), xend = 3.15, yend = mean(Oc1)), colour = "red")

L2 = c(6.3, 4.4 , 7.6) +0.2
N2 = c(6.5, 4 , 7.6) + 0.2
Oc2 = c(5.7, 4.1, 7.8) -0.4
mean2 = mean(c(L2,N2,Oc2))

Fdd2 <- data.frame(Gattung = rep(c("L","N","Oc"),each=3),
                   y = c(L2,N2,Oc2))

p2 <- Fdd2 %>% 
  ggplot(aes(Gattung, y))+
  geom_hline(yintercept = mean2, linetype=2)+
  geom_point(position = position_dodge2(width = 0.3), col="red", size=.9) +
  theme_half_open()+
  theme(axis.title.y = element_text(angle = 0, vjust = 1))+
  scale_y_continuous(breaks = mean2, labels = expression(bar("y")), limits = c(min(Fdd1$y),max(Fdd1$y))) +
  geom_segment(aes(x = 0.85, y = mean(L2), xend = 1.15, yend = mean(L2)), colour = "red")+
  geom_segment(aes(x = 1.85, y = mean(N2), xend = 2.15, yend = mean(N2)), colour = "red")+
  geom_segment(aes(x = 2.85, y = mean(Oc2), xend = 3.15, yend = mean(Oc2)), colour = "red")

p1 + plot_spacer() + p2 + plot_layout(widths = c(3,0.5,3))
```



## $F$-test for the earthworms

The function `anova()` in R does the $F$-test for categorical variables.


\tiny

```{r echo = TRUE}
anova(r.lm)
```

\normalsize \textbf{Note:} Here, the $F$-value for `Gattung` is
distributed as $F_{2,140}$ under the Null-Hypothesis.

This gives $p=`r format(anova(r.lm)[1,5],2,4,2)`$, thus a clear
difference in the regression models for the three genera ("Gattung is
relevant").

## Plotting the earthworms results

```{r eval=T,fig.width=5, fig.height=4,warning=FALSE,echo=F,out.width="8cm", message=FALSE, fig.align='center'}
cc <- data.frame(Gattung = sort(unique(d.wurm$Gattung)),
             means = predict(r.lm, newdata = data.frame(Gattung = sort(unique(d.wurm$Gattung)))))
ggplot(d.wurm,aes(x=Gattung,y=log10(GEWICHT),colour=Gattung)) +
  geom_point(data = cc, mapping = aes(y = means), size = 5, alpha = 0.5) + 
  geom_point()

```

## That was a lot...

Binary and categorical variables...

* Take a few minutes to consolidate, identify questions, ask them.

## The earthworm data has more to learn from


```{r eval=T,fig.width=5.5, fig.height=4,warning=FALSE,out.width="6.5cm", echo = FALSE, fig.align='center', message=FALSE}
library(ggplot2)
library(dplyr)
ggplot(d.wurm,aes(x=MAGENUMF,y=log10(GEWICHT),colour=Gattung)) + geom_point() + geom_smooth(method="lm") + theme_bw()

```

This model will be fitted in this week's BC videos.


## Binary variable with interaction

For simplicity, let us look at a binary explanatory variable ($x_i \in \{0,1\}$).

Remember the mercury (Hg) example. We now extended the dataset and include mothers __and__ children ($\leq 11$ years).

It is known that Hg concentrations may change over the lifetime of humans. So let us look at \texttt{log(Hg$_\text{urin}$)} depending on the age of the participants:

```{r eval=T,warning=FALSE,out.width="6.5cm", fig.align='center', echo=FALSE, message=FALSE, fig.height=3, fig.width=4}
d.hg <- read.table(here("3_datasets/hg_urine_fuzzed.csv"),header=T, sep=",")
d.hg <- d.hg[,c(1,2,5,6,7,8,10)]
#names(d.hg) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish","mother")
d.hg <- mutate(d.hg,mother=factor(mother))

d.hg <- d.hg %>%
  mutate(log_Hg_urin = log(Hg_urin))



ggplot(d.hg,aes(x=age,y=log_Hg_urin,colour=mother)) + geom_point() + geom_smooth(method="lm") + theme_bw()
```

## 

Observation: __The regression lines are not parallel.__

$\rightarrow$ Children and mother's Hg level seem to depend differently on age!
\ 

What does this mean for the model?
 
$\rightarrow$ Formulate a model that allows for \alert{different intercepts \emph{and} slopes}, depending on group membership (mother/child).

$\rightarrow$ This can be achieved by introducing a so-called \alert{interaction term} into the regression equation.

## 

The smallest possible model is then given as
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation}\label{eq:HgInt}
y_i =  \beta_0 + \beta_1 \text{mother}_i + \beta_2 \text{age}_i + \beta_3\text{age}_i\cdot \text{mother}_i + \epsilon_i \ , 
\end{equation}
\end{minipage}}

where $y_i=\log(Hg_{\text{urin}})_i$, and \texttt{mother} is a binary "dummy" variable that indicates if the person is a mother (1) or a child (0).

This results in essentially __two__ models with group specific intercept and slope:
\colorbox{lightgray}{\begin{minipage}{14cm}
Mothers ($x_i=1$): $\hat{y}_i = \beta_0 +  \beta_1 + (\beta_2 +\beta_3)\text{age}_i$  

\ 

Children ($x_i=0$): $\hat{y}_i = \beta_0  + \beta_2 \text{age}_i $  
\end{minipage}}

## 

Fitting model (1) in R is done as follows, where \texttt{age:mother} denotes the interaction term ($\text{age}_i\cdot \text{mother}_i$):
\  

\tiny
```{r echo=T}
r.hg <- lm(log_Hg_urin ~ mother + age + age:mother, data = d.hg)
summary(r.hg)$coef
```
\normalsize

Interpretation:

Mothers: $\hat{y}_i = -1.06 + (-2.48) + (-0.10 + 0.16) \cdot \text{age}_i$  
\
Children: $\hat{y}_i = -1.06 + (-0.10) \cdot \text{age}$

* The Hg level drops in young children.
* The Hg level increases in adults (mothers).

## 

On the previous slide we have actually fitted 2 models at the same time.  

\ 

* What is the advantage of this?
* Why is this usually better than fitting two separate models, one for children and one for mothers?  

\




## 

Remember, however, that the Hg model also included smoking status, amalgam fillings and fish consumption as important predictors. It is very straightforward to just include these predictors in model (1), which leads to the following model
\  

\small

```{r echo = TRUE}
r.hg <- lm(log_Hg_urin ~  mother * age + smoking + amalgam + fish, d.hg)
```

```{r echo = FALSE, results='asis', message=FALSE, warning=FALSE}
library(biostatUZH)
tableRegression(r.hg)
```

\small (Note that \texttt{mother*age} in R encodes for \texttt{mother} + \texttt{age} + \texttt{mother:age}.)

## 

Again, for completeness, some model checking (which one usually does before looking at the results): 

```{r  eval=T,warning=FALSE,out.width="8.5cm", echo = FALSE, message=FALSE, fig.align='center'}
library(ggfortify)
autoplot(r.hg,which=c(1,2, 3, 5),smooth.col=NA)
```

## Linear regression is even more powerful!

We have seen that it is possible to include continuous, binary or categorical explanatory variable in a regression model.

\colorbox{lightgray}{\begin{minipage}{14cm}
Even \alert{transformations} of explanatory variables can be included in (almost) any form. For instance the square of a variable ${x}$
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i \ , 
\end{equation*}
which leads to a {\bf quadratic} or {\bf polynomial} regression (if higher order terms are used).
\end{minipage}}

Other common transformations are:

* $\log$
* $\sqrt{..}$
* $\sin$, $\cos$, ...

## How can a _quadratic_ regression be a _linear regression_??

__Note:__ The word _linear_ refers to the \alert{linearity in the coefficients}, and not on a linear relationship between ${y}$ and ${x}$!

\begin{center}
\includegraphics[width=11cm]{pictures/multiplReg.png}
\end{center}


## Multiple vs. many single regressions

Question: Given multiple regression variables ${x}^{(1)}, {x}^{(2)},...$. Could I simply fit separate simple models for each variable, that is

$y_i = \beta_0 + \beta_1 x_i^{(1)} + \epsilon_i$  

$y_i = \beta_0 + \beta_1 x_i^{(2)} + \epsilon_i$

etc.?

Answer (Stahel 3.3o):

\begin{center}
\includegraphics[width=11cm]{pictures/citation.png}
\end{center}

<!-- 

## Illustration

Chapter 3.3c in the Stahel script illustrates the point on four artificial examples. The "correct" model is always given as 

\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i \  ,
\end{equation*}

where ${x}^{(1)}$ is a continuous variable, and ${x}^{(2)}$ is a binary grouping variable (thus taking values 0 and 1 to indicate the group).

Thus the correct model is 

\begin{equation*}
\begin{array}{lll}
\hat{y_i} &= \beta_0 +  \beta_1 x_i^{(1)} & \text{if $x_i^{(2)}=0$.} \\
\hat{y_i} &= \beta_0 + \beta_2 + \beta_1 x_i^{(1)}   & \text{if $x_i^{(2)}=1$.} 
\end{array}
\end{equation*}

## 

\begin{center}
\includegraphics[width=11cm]{pictures/FigsAB.png}
\end{center}

Example A: Within-group slope is $>0$. Fitting $y$ against $x$ leads to an overestimated slope when group-variable is not included in the model.

Example B: Within-group slope is $0$, but fitting $y$ against $x$ leads to a slope estimate $>0$, which is only an artifact of not accounting for the group variable $x^{(2)}$.

##

\begin{center}
\includegraphics[width=11cm]{pictures/FigsCD.png}
\end{center}

Example C: Within-group slope is $<0$, but fitting $y$ against $x$ leads to an estimated slope of $>0$!

Example D: Within-group slope is $<0$, but fitting $y$ against $x$ leads to a slope estimate of $0$.

Write your comments here -->












<!-- Note 2024 by Uriah: I commented out most of the following slides as they are usually skipped (because repetition) -->

<!--

## Another interpretation of multiple regression

In multiple regression, the coefficient $\beta_x$ of explanatory variable $x$ can be interpreted as follows:

\colorbox{lightgray}{\begin{minipage}{14cm}
$\beta_x$ explains how the response changes with $x$, while holding all the other variables constant.
\end{minipage}}

This idea is similar in spirit to an experimental design, where the influence of an explanatory variable of interest on the response is investigated in various environments\footnote{Clayton, D. and M. Hills (1993). Statistical Models in Epidemiology. Oxford: Oxford University Press.}. Clayton and Hills (1993) continue (p.273):

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{quote}
[...] the data analyst is in a position like that of an experimental scientist who has the capability to plan and carry out many experiments within a single day. Not surprisingly, a cool head is required!
\end{quote}
\end{minipage}}

## Checking modeling assumptions (recap)


Remember that in linear regression the modeling assumption is that the errors $\epsilon_i$ are independently normally distributed around zero, that is, $\epsilon_i \sim N(0,\sigma^2)$. This implies four things:

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{enumerate}[a)]
\item The expected value of each residual $\epsilon_i$ is 0: $E(\epsilon_i)=0$.
\item All $\epsilon_i$ have the same variance: $Var(\epsilon_i)=\sigma^2$.
\item The $\epsilon_i$ are normally distributed.
\item The $\epsilon_i$ are independent of each other.
\end{enumerate}
\end{minipage}}

The aim is to formulate a model that describes the data well. But always keep in mind the following statement from a wise man:

\colorbox{lightgray}{\begin{minipage}{14cm}
All models are wrong, but some are useful. \scriptsize{(Box 1978)}
\end{minipage}}

## Overview of model-checking tools

Overview of tools used in this course:

* Tukey-Anscombe plot (see lectures 3 and 4)



* Quantile-quantile (QQ) plot (see lectures 3 and 4)



* Scale-location plot (Streuungs-Diagramm)  



* Leverage plot (Hebelarm-Diagramm)



__Note:__ these four diagrams are plotted automatically by R when you use the \texttt{plot()} or the \texttt{autoplot()}  function (from the \texttt{ggfortify} package) on an \texttt{lm} object, for example \texttt{autoplot(r.hg)}.

## Tukey-Anscombe plot

It is sometimes useful to enrich the TA-plot by adding a "running mean" or a "smoothed mean", which can give hints on the trend of the residuals. For the mercury example where $\log(Hg_{\text{urin}})$ is regressed on smoking, amalgam  and fish consumption:  

```{r echo= FALSE, message=FALSE, warning=FALSE}
d.hg <- read.table(here("3_datasets/hg_urine_fuzzed.csv"),header=T, sep=",")
d.hg.m <- d.hg[d.hg$mother==1,-c(3,4,9,10,11)]
#names(d.hg.m) <- c("Hg_urin", "Hg_soil", "smoking","amalgam", "age", "fish")
```

```{r eval=T,echo=FALSE,message=FALSE,warning=FALSE,fig.align='center', out.width="4cm", fig.width=4, fig.height=4}
r1.urin.mother <- lm(log10(Hg_urin) ~  smoking + amalgam + fish,data=d.hg.m)
plot(fitted(r1.urin.mother),residuals(r1.urin.mother),xlab="Fitted values",ylab="Residuals")
abline(h=0,lty=2)
aa <- data.frame(cbind(x=fitted(r1.urin.mother), y=residuals(r1.urin.mother)))
aa <- aa[order(aa$x),]
lo <- loess(y ~ x, aa)
lines(aa$x,predict(lo), col='red', lwd=2,lty=5)
```


The TA plot (again) indicates that there is an outlier in the range of -0.7 to -0.6.

However, generally we recommend to \alert{not} add a smoothing line, because it may bias our view on the plot.

## 

The TA plot is also able to check the \emph{independence assumption} d). But how?

$\rightarrow$ A dependency would be reflected by some kind of \alert{trend}. 

##

But: The dependency is not necessarily on the fitted values ($x$-axis of TA plot). Ideas:  

* Plot residuals in dependency of time (if available) or sequence of obervations.
* Plot residuals against the explanatory variable.

```{r echo=FALSE, fig.align='center', fig.height=3, fig.width=8, message=FALSE, warning=FALSE, out.width="8cm"}
library(ggplot2)
library(gridExtra)
d1 <- data.frame(obsNr= 1:length(residuals(r1.urin.mother)), 
                 resid = residuals(r1.urin.mother),
                 amalgam = d.hg.m$amalgam,
                 fish = d.hg.m$fish)
p1 <- ggplot(d1, aes(x=obsNr,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("Observation number (time)") +
  ylab("Residuals")
p2 <- ggplot(d1, aes(x=amalgam,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("No. of amalgam fillings") +
  ylab("")
p3 <- ggplot(d1, aes(x=fish,y=resid))+  geom_point() +  geom_hline(yintercept = 0,linetype=2,size=0.3,col=2) +
  xlab("No. of fish meals/month") +
  ylab("")
grid.arrange(p1, p2, p3, ncol=3)
```

Again, no pattern = good.

## QQ-plot

The \alert{outlier} recorded above is also visible in the QQ-plot, which is useful to check for normal distribution of residuals (assumption c):

```{r eval=T,warning=FALSE,out.width="8cm",echo=FALSE, fig.align='center', fig.width=8, fig.height=4}
library(ggfortify)
autoplot(r1.urin.mother,2) 
```


## Check equal variance assumption

```{r eval=T,warning=FALSE,out.width="8cm", fig.align='center', echo=FALSE, fig.width=8, fig.height=4}
autoplot(r1.urin.mother,which=3) 
```


## Leverage plot (Hebelarm-Diagramm)

In the leverage plot, (standardized) residuals $\tilde{R_i}$ are plotted against the leverage $H_{ii}$ (still for the Hg example):

```{r out.width="5cm", fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
autoplot(r1.urin.mother,which=5)
```

\alert{Critical ranges} are the top and bottom right corners!!

Here, individuals 95, 101 and 106 are potential \alert{outliers}.


## What to do when things go wrong?

1. \alert{Transform the response and/or explanatory variables.}
2. \alert{Take care of outliers.}
3. Use weighted regression (not discussed here).
4. Improve the model, e.g., by adding additional terms or interactions (see "model selection" in lecture 8).
5. Use another model family (generalized or nonlinear regression model).

## Transformation of the response?

Example: Use again the mercury study, include only mothers. Use the response (Hg-concentration in the urine) \alert{without log-transformation}. What would it look like?
\small
```{r echo = TRUE}
r2.urin.mother <- lm(Hg_urin ~  smoking  + amalgam + fish,data=d.hg.m)
```

```{r fig.width=5.5, fig.height=5.5,out.width="6cm", fig.align='center',echo=FALSE,message=FALSE,warning=FALSE}
autoplot(r2.urin.mother) + theme_bw()
```

## 

Comparison to the model with log-transformed response:

```{r echo = FALSE}
r3.urin.mother <- lm(log10(Hg_urin) ~  smoking  + amalgam + fish,data=d.hg.m)
```

```{r fig10,fig.width=5.5, fig.height=5.5,out.width="6cm", fig.align='center',echo=FALSE,message=FALSE,warning=FALSE}
autoplot(r3.urin.mother,smooth.colour=NA) + theme_bw()
```

This looks __much__ better! However... there is this individual 106 that needs some closer inspection (see slide 43 for the solution regarding this outlier).

## Common transformations

Which transformations should be considered to cure model deviation symptoms? Answering this depends on plausibility and simplicity, and requires some experience.  
\

The most common and useful \alert{first aid transformations} are:

\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{itemize}
\item The log transformation for {\bf concentrations} and {\bf absolute values}.
\item The square-root ($\sqrt{\cdot}$) transformation for {\bf count data}.
\item The $\arcsin(\sqrt{\cdot})$ transformation for {\bf proportions/percentages}.
\end{itemize}
\end{minipage}}

These transformations can also be applied on explanatory variables!

## 

For instance, the number of amalgam fillings and the number of monthly fish meals could be sqrt-transformed in the mercury example:

```{r echo = FALSE, warning=FALSE, message=FALSE}
r4.urin.mother <- lm(log10(Hg_urin) ~  smoking + sqrt(amalgam) + sqrt(fish),data=d.hg.m)
```

```{r fig.width=5.5, fig.height=5.5,out.width="7cm", fig.align='center', echo=FALSE, warning=FALSE, message=FALSE}
autoplot(r4.urin.mother,smooth.colour=NA) + theme_bw()
```



## The outlier in the Hg study

In the Hg study, it turned out later on that the outlier 106 had five unreported amalgam fillings!

A corrected analysis gives a much more regular picture (please compare to slide 40):

```{r echo = FALSE, warning=FALSE, message=FALSE}
d.hg.m["106","amalgam"]<-5
r5.urin.mother <- lm(log10(Hg_urin) ~  smoking + sqrt(amalgam) + sqrt(fish),data=d.hg.m)
```

```{r eval=T,fig.width=5, fig.height=5,warning=FALSE,out.width="6cm", fig.align='center',echo=FALSE, message=FALSE}
autoplot(r5.urin.mother,smooth.colour=NA) + theme_bw()
```

-->

## Recap

* \alert{Binary} and \alert{categorical} explanatory variables.
* Interactions: a categorical explanatory variables allows for \alert{group-specific intercepts and slopes} (see earthworm example).
* The \alert{$F$-test} is used to test if $\beta_2=\beta_3=...=\beta_k=0$ at the same time for a categorical explanatory variable with $k$ levels. Use the \texttt{anova()} function in R to carry out this test.
* The $F$-test is a \alert{generalization of the $t$-test}, because the latter is used to test $\beta_j = 0$ for one single variable $x^{(j)}$.  
* Test for a single $\beta_j=0$ $\rightarrow$ $t$-test.
* Test for several $\beta_2 = ... = \beta_{k}=0$ simultaneously $\rightarrow$ $F$-test.

Thus you will __always__ need the $F$-test `anova()` to obtain a $p$-value for a categorial explanatory variable with more than 2 levels!



## Next steps

* (BC) Before (practical) Class: three videos (20, 13, and 18 mins) going through analysis of the earthworm study data.
* (IC) In (practical) Class: naked mole rats, reaction times (more independent)
* Then week 6: Analysis of variance (ANOVA).

