---
title: "Lecture 10: Modeling count data"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff, Owen Petchey, Erik Willems"
institute: "University of Zurich"
date: "06 May, 2024"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## Recap

## Overview

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(ggfortify)
```

```{=tex}
\begin{itemize}
\item When the outcome ($y$) is a count\\[2mm]
\item Generalized Linear Models (GLM)\\[2mm]
\item Poisson regression\\[2mm]
\item Link function\\[2mm]
\item Residual analysis / model checking / deviances\\[2mm]
\item Interpretation of results\\[2mm]
\item Overdispersion, zero-inflation
\end{itemize}
```
## Course material covered today

Today's lecture is largely based on the following:

```{=tex}
\begin{itemize}
\item Chapter 7 of GSWR (Beckerman et al.)\\[2mm]
\item Hothorn T and Everitt BS (2014), A Handbook of Statistical Analyses Using R\\[2mm]
\end{itemize}
```
## Introduction

```{=tex}
\begin{itemize}
\item We have seen that \alert{explanatory variables} in regression models can be \alert{continuous}, \alert{categorical} (binary or multi-level), or \alert{count} (non-negative integers).\\[2mm]
\item The \alert{response variable} (${y}$), so far, has always been \alert{continuous}, with the assumption that residuals $\epsilon_i\sim N(0,\sigma^2)$.\\[2mm]
\item Often, however, the response variable will be a count, or categorical variable.\\[2mm]
\item Today we will look at the case when the response variable is a \alert{count}, that is, $y_i =0,1,2,\dots$\\[4mm]
\end{itemize}
```
## Count data

In biological or medical data, the outcome of interest is quite often a
count:

```{=tex}
\begin{itemize}
\item Counting items in time or space (animals, plants, species)\\[2mm]
\begin{itemize}
\item Number of offspring in animals\\[2mm]
\item Number of pathological structures in humans (e.g. polyps)\\[4mm]
\end{itemize}
\end{itemize}
```
In such cases, the research question is:

\textbf{How do the explanatory variables influence the probability of a given count of the outcome?}

## Example: Soay sheep

Hirta, a small island of Scotland, is inhabited by an unmanaged and
feral population of Soay sheep.

Ecologists were interested whether body mass of females influences their
fitness, measured as their \alert{lifetime reproductive success} (the
number of offspring over their lifespan).

```{=tex}
\colorbox{lightgray}{\begin{minipage}{13cm}
\begin{center}
{\bf Question:} "Are heavier females fitter than lighter females?" 
\end{center}
\end{minipage}}
```
```{r echo = FALSE}
soay <- read.table(here("3_datasets/SoaySheepFitness.csv"),header = T, sep = ',')
```

Â 

\small

```{r echo = TRUE}
glimpse(soay)
```

## 

As always, explore the data with a graph (code in GSWR book):

```{r echo = FALSE, fig.width=4, fig.height=4,out.width="6cm",fig.align='center',message=FALSE, warning=FALSE}
ggplot(soay, aes(x = body.size, y = fitness)) +
geom_point(alpha= .4) +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Body mass (kg)") + ylab("Lifetime fitness")
```

## 

\textbf{What can we see from the figure?}

```{=tex}
\begin{itemize}
\item Reproductive success indeed seems to increase with female body weight.\\[2mm]
\item The straight line does not capture the pattern very well, the red (smoothed) line seems better. Maybe a quadratic term would improve things?\\[2mm]
\item However, the "problem" with these data is more subtle.\\[8mm]
\end{itemize}
```
\textbf{How does one analyze these data correctly?}

```{=tex}
\begin{itemize}
\item The outcome represents a count (non-negative integer)!
\item So far, we have always used normal linear regression.\\[2mm]
\item This is \alert{not} the correct approach here, but let's do it anyway.\\[2mm]
\end{itemize}
```
## The wrong analysis

Use the `lm()` function to fit the linear model
$y=\beta_0 + \beta_1 \cdot \text{bodySize} + \epsilon$, and look at the
diagnostic plots:

```{r echo = FALSE, fig.width=6, fig.height=6,out.width="5.6cm",fig.align='center',message=FALSE,warning=FALSE}
r.soay <- lm(fitness ~ body.size ,data=soay)
autoplot(r.soay)
```

$\rightarrow$ The diagnostic plots indicate that the
\textbf{linear regression assumptions are violated}!

## 

What about the model with a quadratic term?

```{=tex}
\begin{equation*}
y=\beta_0 + \beta_1 \cdot \text{bodySize} + \beta_2 \cdot \text{bodySize}^2 + \epsilon
\end{equation*}
```
```{r echo = FALSE, fig.width=6, fig.height=6,out.width="5.6cm",fig.align='center',message=FALSE,warning=FALSE}
r2.soay <- lm(fitness ~ body.size + I(body.size^2),data=soay)
autoplot(r2.soay)
```

$\rightarrow$ This looks a bit better. However...

## What is the problem?

There is still a clear upward trend in the scale-location plot,
indicating that the \alert{variance increases} as the fitted values get
larger.

Moreover:

```{=tex}
\begin{itemize}
\item The normal distribution is for continuous variables.\\[2mm]
\item The normal distribution allows values $<0$, count data are non-negative integers.\\[2mm]
\item The normal distribution is symmetrical, counts often are not!\\[2mm]
\item The variability in count data tends to increases with higher values.
\end{itemize}
```
## A different distribution for count data I

Remember from Mat183?

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The probability distribution\footnote{A probability distribution is a mathematical statement of how likely different events are, see GSWR book p.\ 169.} of Poisson-distributed random variable $Y$ with parameter $\lambda$ is defined as:
\begin{equation*}
P(Y=k) = \frac{\lambda^k}{k!} e^{-\lambda} \ , \quad k=0, 1, 2,\ldots  \ .
\end{equation*}
In short: $$Y \sim Po(\lambda) \ .$$\\
\end{minipage}}
```
## A different distribution for count data II

\textbf{Characteristics of the Poisson distribution:} \label{sl:poisson}

```{=tex}
\begin{itemize}
\item Suitable to model unbounded counts ($k=0,1,2,...$).
\item $E(Y) = Var(Y) = \lambda$. In words: \\[2mm]
Mean = variance = $\lambda$.\\[2mm]

$\rightarrow$ The variance of the distribution increases with the mean.\\[4mm]
\end{itemize}
```
## A different distribution for count data III

Some examples:

```{=tex}
\begin{center}
\includegraphics[width=10cm]{pictures/poisson.png}
\end{center}
```
So: How can one use the Poisson distribution in a regression model?

## Doing it right: The Generalized Linear Model (GLM) for count data

The aim of the GLM approach is that we can still use a
\alert{linear predictor} $\eta_i$ in the form of the linear model:
\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}

```{=tex}
\begin{itemize}
\item In {\bf linear regression}, $\eta_i$ is the \alert{predicted value} for the mean $E(y_i)=\eta_i$.\\  
{\small (Why $E(y_i)$ and not $y_i$? Because the residual/error term $+  \epsilon_i$ is missing!)}\\[2mm]

\item However, we cannot simply set $\eta_i$ equal to $E(y_i)$, if $y_i$ is a \alert{count}!\\[2mm]

\end{itemize}
```
## 

Why can't we set $E(y_i)=\eta_i$ for count data?

\textbf{$\rightarrow$ Because nothing prevents $\eta_i$ from being negative!}

Let us try to use the same approach as in linear regression, assuming
that $E(y_i)=\eta_i$ for our soay sheep counts, that is,

$$E(y_i) = \beta_0 + \beta_1\cdot \text{bodySize}_i \ ,$$

using a model with $\beta_0=-2$ and $\beta_1=1.2$.

What is the predicted number of offspring for a 1kg sheep? Plug-in:
$-2 + 1.2\cdot 1 = -0.8$, thus a negative prediction!

This is very unreasonable, right?

$\rightarrow$ We need a trick!

## The trick: Use a link function

Instead of using $E(y_i)=\eta_i$ as in linear regression, we
log-transform the expected value.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation}\label{eq:logLink}
\log(E(y_i)) = \eta_i  = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)}\ .
\end{equation}
\end{minipage}}
```
```{=tex}
\begin{itemize}
\item The $\log$ is called the \alert{link function}.\\[2mm]
\item The {\bf advantage}: The predicted fitness $E(y_i)$ is now \alert{always positive}, because equation \eqref{eq:logLink} is identical to 
$$E(y_i) = \exp(\beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)})  \ ,$$
which is now {\bf always} $>0$! {\scriptsize (Plot the $\exp()$ function if you forgot what it looks like...)}
\end{itemize}
```
## The probability model

```{=tex}
\begin{itemize}
\item Finally, we need a reasonable \alert{probability model for the response variable}. \\[3mm
]
\item Remember: we always used the normality assumption $y_i \sim N(\eta_i, \sigma^2)$ in linear regression.\\[3mm]

\item Given that $y_i$ are counts, a Poisson model is more appropriate:  
$$y_i \sim Po (\lambda_i) \ .$$

In words: $y_i$ is a realization of a random variable distributed as $Po(\lambda_i)$, where $\lambda_i=E(y_i)$.\\[3mm]
\item We say that the model belongs to the Poisson \alert{family}.
\end{itemize}
```
## Key terms for GLMs

In summary, we have introduced three terms related to GLMs:

```{=tex}
\begin{itemize}
\item \emph{\bf Linear predictor}: The linear predictor is always given as:
\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}
In the soay sheep example, it is $\eta_i = \beta_0 + \beta_1\cdot \text{bodySize}_i$ for sheep $i$.\\[2mm]

\item \emph{\bf Family}: The family corresponds to the likelihood model that is used for the (transformed) response. For count data, the family is \alert{Poisson}. Another very common distribution is the \alert{binomial} family for binary outcome (see next week). Other families are the gamma or the negative binomial. The family is determined by the data type!\\[2mm]

\item \emph{\bf Link function}: This defines how the linear predictor $\eta_i$ is \alert{related} to $E(y_i)$. In Poisson regression this is typically the log: $\log(E(y_i)) = \eta_i$. We will see another important link function next week (for binary data).
\end{itemize}
```
## Doing it right: Fitting a Poisson GLM

Uff... that was hard! But we finally have all the tools for fitting a
Poisson GLM. A statistician would now say:

```{=tex}
\begin{center}
"Let's fit a Poisson GLM using the logarithmic link-function"\\[4mm]
\end{center}
```
```{=tex}
\begin{itemize}
\item Basically, the idea is to perform \alert{maximum-likelihood estimation}(we won't go into the details of this).\\[4mm]

\item Luckily, there is an R-function that works (almost) like `lm()`, namely \alert{\texttt{glm()}}:
\end{itemize}
```
\small

```{r echo = TRUE}
soay.glm<- glm(fitness~ body.size, data= soay, family= poisson(link= log))
```

```{=tex}
\begin{itemize}
\item You {\bf must} specify the \alert{family}, but you can leave away the \texttt{link=log} argument (because R automatically picks the most appropriate link function).

\end{itemize}
```
## Doing it right: Model diagnostics

Before we look at the output, let's do some model diagnostics (as we did
for normal linear regression):

```{r echo = FALSE, fig.width=5.5, fig.height=5.5,out.width="6.2cm",fig.align='center',message=FALSE,warning=FALSE}
autoplot(soay.glm)
```

## 

```{=tex}
\begin{itemize}
\item Model diagnostics seem ok. In particular, the scale location plot is a bit better than when using the quadratic term in linear regression (slide 11).\\[2mm]

\item Note that the definition of a "residual" is no longer clear when link-functions are used (on which scale should residuals be calculated, on the link scale or on the observed scale?)\\[2mm]

\item For now, we don't care too much about this, but you should remember:\\[2mm]

\begin{itemize}
\item There are \alert{different types of residuals}.\\[2mm]
\item \texttt{autoplot()} \alert{automatically picks} the residuals that ``make most sense''.
\end{itemize}
\end{itemize}
```
## Doing it right: Interpreting the coefficients

Let's now look at the `summary()` output:

\tiny

```{r echo = TRUE, }
summary(soay.glm)
```

## 

You see several (familiar and less familiar) components in the output.
For the moment, we are interested in the coefficients, which are
estimated as

```{=tex}
\begin{equation*}
\hat\beta_0 = `r format(summary(soay.glm)$coef[1,1],3,3,3)` \quad \text{and}\quad \hat\beta_1 = `r format(summary(soay.glm)$coef[2,1],3,3,3)`
\end{equation*}
```
with respective standard errors and $p$-values. In particular, $p<0.001$
for $\hat\beta_1$ indicates
\alert{very strong evidence for a positive effect of female weight on reproductive success}
(number of offspring).

\textbf{Good to know:} Theory says that the estimated coefficients
$\hat\beta_0$ and $\hat\beta_1$ are approximately
\alert{normally distributed} around the true values:

```{=tex}
\begin{equation}
\hat\beta \sim N(\beta,\sigma_\beta^2)
\end{equation}
```
Thus a 95% CI can be approximated by the usual
$\hat\beta\pm 2\cdot \hat\sigma_\beta$ idea.

## What do the coefficients tell us?

Remember our model: $$
\log(E(y_i)) =\beta_0 + \beta_1 \cdot \text{bodySize}_i
$$ Which we can back-transform to obtain expected counts, by:
$$ \quad E(y_i) = \exp(\beta_0 + \beta_1 \cdot \text{bodySize}_i) \ .$$
Our estimates $\hat\beta_0$ and $\hat\beta_1$ can be plugged into this
equation!

For example, a 5\~kg female has \textbf{expected fitness}
$$\exp(`r format(summary(soay.glm)$coef[1,1],3,3,3)` + 
`r format(summary(soay.glm)$coef[2,1],3,3,3)`\cdot 5) = 
`r format(exp(summary(soay.glm)$coef[1,1] + summary(soay.glm)$coef[2,1]*5),2,2,2)` 
\text{ lambs} \ ,$$

while a 7\~kg female would have \textbf{expected fitness}
$$\exp(`r format(summary(soay.glm)$coef[1,1],3,3,3)` + `r  format(summary(soay.glm)$coef[2,1],3,3,3)`\cdot 7) = `r format(exp(summary(soay.glm)$coef[1,1] + summary(soay.glm)$coef[2,1]*7),2,2,2)` \text{ lambs} \ .$$

## Doing it right: The \texttt{anova()} table

\small

\texttt{anova()} gives us the \alert{Analysis of Deviance} table:

```{r echo = TRUE}
anova(soay.glm)
```

\textbf{Note:} The deviance is essentially a difference of likelihoods.
Think of it as the "Maximum-Likelihood Estimation" (MLE)
\alert{equivalent to the Sums of Squares} in OLS regression.

## 

Here, the \alert{total deviance} is the so-called \alert{NULL} deviance:
85.081. It is analogue to the total variability of the data in linear
regression.

Of this, 37.041 is \alert{explained by bodysize}.

The question is, whether this is much? This can be tested by a $\chi^2$
test (deviance follows a $\chi^2$- rather than an F-distribution:

```{r echo = TRUE}
pchisq(37.041, 1, lower.tail= F)
```

Or, directly specify the test in the \texttt{anova()} call:

\small

```{r echo = TRUE, eval = FALSE}
anova(soay.glm, test= "Chisq")
```

## Making a nice graph

See section 7.4.5 in the GSWR book.

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=4,out.width="6cm",fig.align='center',message=FALSE,warning=FALSE}
min.size <- min(soay$body.size)
max.size <- max(soay$body.size)
 
new.x <- expand.grid(body.size =seq(min.size, max.size, length=1000))

new.y = predict(soay.glm, newdata = new.x, se.fit = TRUE)
new.y = data.frame(new.y)
addThese <- data.frame(new.x, new.y)
addThese <- mutate(addThese, fitness = exp(fit),
lwr = exp(fit - 1.96 * se.fit),
upr = exp(fit + 1.96 * se.fit))
 
ggplot(soay, aes(x = body.size, y = fitness)) +
# first show the raw data
geom_point(size = 3, alpha = 0.5) + 
geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = 'identity') +
theme_bw()
```

## Your turn!

Let's look at another data example, taken from Hothorn and Everitt
(2014), chapter 7:

A new drug was tested in a clinical trial (Giardiello et al. 1993,
Piantodsi 1997), aiming to \alert{reduce the number of polyps} in the
colon (Dickdarm). The data are publicly available from the
Hothorn/Everitt book package: Â 

```{r echo = TRUE, message=FALSE, warning=FALSE}
library(HSAUR3)
data("polyps")
```

\textbf{Question:} Does the drug influence (reduce) the number of
polyps?

## 

Data: Number of polyps (outcome), the binary variable for the treatment,
and the continuous explanatory variable age.

\tiny

```{r echo = FALSE, message=FALSE, warning=FALSE, results='asis'}
library(xtable)
pol1 <- polyps[1:20,]
# pol2 <- polyps[11:20,]
ttab1 <- xtable(pol1,digits=0)
# ttab2 <- xtable(pol2,digits=0)
print(ttab1, floating = TRUE, include.rownames=FALSE)
# print(ttab2,  floating = TRUE, include.rownames=FALSE)
```

## 

\textbf{Your task} (in teams, if you like):

Look at the analysis on the next three slides, and answer the following
questions:

```{=tex}
\begin{enumerate}
\item Are there any problems visible from the diagnostics plots?\\[2mm]
\item Does the treatment seem to be effective? If yes, can you quantify the effect?\\[2mm]
\item Is age a relevant variable? If yes, what happens in older patients?\\[2mm]
\end{enumerate}
```
## 

\small

Fit the model:

```{r echo = TRUE}
polyps.glm<- glm(number~ treat + age, data= polyps, family= "poisson")
```

Look at diagnostics:

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6,out.width="5cm", fig.align='center'}
autoplot(polyps.glm)
```

## 

Perform an Analysis of Deviance: \tiny

```{r echo = TRUE}
anova(polyps.glm, test= "Chisq")
```

## 

Inspect the summary table: \tiny

```{r echo = TRUE}
summary(polyps.glm)
```

## Overdispersion

\textbf{"Overdispersion"} means \textbf{"extra variability"}. Why could
this be a problem?

\textbf{Remember:} The variance of the Poisson distribution increases
with the mean, because \alert{\textbf{mean= variance}} (see slide 13).

In Poisson regression it is assumed that, for each observation $i$,
$$y_i \sim Po (E(y_i)) \ .$$

However, the \alert{variance is often larger than the mean} in reality,
because there are factors that influence the response that cannot be
captured by the explanatory variables.

\small

Why? Maybe you cannot observe the variable, or it is too expensive to
monitor, or...

## Detecting overdispersion

Look at the summary output from your GLM object, check the "Residual
deviance" and compare it to the "degrees of freedom".

\textbf{Soay sheep data:} Res. deviance: 48.040, df= 48

\textbf{Polyps data:} Res. deviance: 178.54, df= 17

The residual deviance should be approximately $\chi^2$ distributed with
df degrees of freedom. This means that one should check whether:

```{=tex}
\begin{center}
\colorbox{lightgray}{\begin{minipage}{10cm}
  \begin{center}
  \texttt{Residual deviance} $\approx$ \texttt{df} .
  \end{center}
  \end{minipage}}
\end{center}
```
The sheep model seems fine, but the polyps model does not:

\texttt{Residual deviance}$>>$ \texttt{df}\

\alert{$\quad\rightarrow$ overdispersion}

## What is the problem with overdispersion?

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
  When there is unaccounted overdispersion, reported $p$-values are \alert{too small}! 
    \end{minipage}}
```
Â 

\textbf{Possible solutions:}

```{=tex}
\begin{itemize}
\item Use the \alert{quasipoisson-}, instead of poisson-family.\\[2mm]
\end{itemize}
```
This estimates the variance parameter (denoted as
\alert{dispersion parameter}) from the data.

```{=tex}
\begin{itemize}
\item Use a \alert{negative binomial regression} (the \texttt{glm.nb()} function in the \texttt{MASS} package)
\end{itemize}
```
## Reanalyzing the polyps data with a quasipoisson family

\tiny

```{r echo = TRUE}
polyps.glm2<- glm(number~ treat + age, data= polyps, family= "quasipoisson")
summary(polyps.glm2)
```

## 

```{=tex}
\begin{itemize}
\item The \alert{dispersion parameter} is estimated as 10.73, which is close to that calculated by  
\begin{center}
\texttt{Residual deviance / df}.\\[6mm]
\end{center}

(See the relevant FAQ on OLAT for further details.)

\item The $p$-values for the coefficients are now larger, and there is only weak evidence ($p=0.063$) for an effect of age.\\[2mm]
$\rightarrow$ The \texttt{poisson} model gave $p$-values that were too optimistic\\[2mm]

We say: The $p$-values were \alert{anti-conservative} or non-conservative.\\[6mm]
\end{itemize}
```
\small

(Anti-conservative results are \textbf{a big problem}. Why?)

## Underdispersion?

Can it happen that the observations are \alert{less variable} than
expected?

\textbf{Yes}: Especially when observations are \alert{dependent}.

You can \textbf{detect it} by checking if ; \texttt{Residual deviance}
$<$ \texttt{df}.

In that case, your $p$-values may be too large, that is, the results are
\alert{overly conservative}.

The \texttt{quasipoisson} regression is a pragmatic solution in that
scenario as well (a negative binomial regression is not!)

## Zero-inflation

A special type of overdispersion may be caused by an
\alert{overrepresentation of zeros} in the obervations.

\textbf {Example}: Numbers of cigarettes smoked

Some people are never-smokers, so they will always produce a zero
obervation, while smokers may smoke any number of cigarettes.\

Please read chapter 7.5.2 of GSWR for some ideas how to handle this
scenario.

## A note on interpretation and model selection

The same remarks and warnings from the last weeks for linear models
regarding:

```{=tex}
\begin{itemize}
\item Caution with model selection  
\item Interpretation of $p$-values
\item Reproducibility aspects
\end{itemize}
```
also apply to GLMs!

## Summary

```{=tex}
\begin{itemize}
\item Use Poisson regression to model count outcomes.\\[2mm]
\item Pretending that count outcomes are continuous may lead to wrong results.\\[2mm]
\item The main ingredients of a GLM are\\[1mm]
\begin{itemize}
\item The linear predictor\\[1mm]
\item The family\\[1mm]
\item The link function.\\[2mm]
\end{itemize}
\item Model diagnostics are similar as in normal regression (\texttt{autoplot()}).\\[2mm]
\item Always check the dispersion of your data, and correct for possible issues\\[2mm]
\item Interpret the coefficients by back-transforming to the original scale.\\[2mm]
\item Analysis of Deviance is the MLE analogue to OLS ANOVA.\\[2mm]
\end{itemize}
```
