---
title: "Lecture 11: Modeling binary data"
subtitle: "BIO144 Data Analysis in Biology"
author: "Stephanie Muff, Owen Petchey & Erik Willems"
institute: "University of Zurich"
date: "13 May, 2024"
output:
  beamer_presentation:
    includes:
      in_header: ../../beamer_stuff/preamble.tex
classoption: "aspectratio=169"
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE, echo = FALSE, message=FALSE, warning=FALSE}
source(here::here("2_content/beamer_stuff/preamble.r"))
```

## Overview

```{=tex}
\begin{itemize}
\item Generalized Linear Models (GLMs)
  \begin{itemize}
  \item Count outcome $\rightarrow$ Poisson regression
  \item Binary outcome $\rightarrow$ logistic regression
  \end{itemize}
\item Contingency tables, $\chi^2$-test
\item Odds and (log) odds ratios
\item Residual analysis / model checking / deviances
\item Interpretation of the results
\end{itemize}
```
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(ggfortify)
```

## Course material covered today

The lecture material of today is based on the following literature:\
 

```{=tex}
\begin{itemize}
\item Repetition: Chapter 9.4 about $\chi^2$-Tests in the Luchsinger script \\[4mm]
\item Chapters 9.1 - 9.3 from \emph{The new statistics with R} (Hector book).\\[12mm]
\end{itemize}
```
Note: on OLAT you'll also find the continuation of the Stahel script,
chapters 7-9 that cover GLMs. This is \textbf{not} mandatory literature.

## Recap of last week: GLMs and Poisson regression

```{=tex}
\begin{itemize}
\item We encountered \alert{Generalized Linear Models} (GLMS) and their key components:
  \begin{itemize}
  \item Linear predictor
  \item Family
  \item Link function
  \end{itemize}
\item GLMs are useful when the response variable ${y}$ is not continuous, or more generally, when we can't assume that residuals follow a Gaussian distribution.\\[4mm]
\item Count data (without known maximum) can be modelled using \alert{Poisson regression}.\\[4mm]
\end{itemize}
```
## Introduction

```{=tex}
\begin{itemize}
\item Today, we will look at situations in which the \alert{response variable} is binary (1/0) or binomial (\emph{e.g.} 5 out of 7 trials).\\[2mm]

\item The question is: "Which variables influence the \alert{probability} $p$ of the outcome?"\\[4mm]
\end{itemize}
```
\textbf{Examples:}

```{=tex}
\begin{itemize}
\item Outcome: Heart attack (yes= 1, no= 0). \\
Question: which variables affect the risk of heart attack?\\[2mm]
\item Outcome: Survival (yes= 1, no= 0).\\
Question: which variables influence the survival probability of premature babies (Frühgeburten)?\\[2mm]
\end{itemize}
```
## Some repetition: The $\chi^2$ test

You may recall binary (categorical) data from Mat183, where you
encountered the $\chi^2$ test for contingency tables (simplest case is a
2 x 2 table).\
 

Example: Heart attack and hormonal contraception (Verhütungspille; from
Stahel):

```{=tex}
\begin{center}
\includegraphics[width=5cm]{pictures/table1.png}
\end{center}
```
\small

"Hormonal contraception" is the predictor ($x$) and "heart attack" the
outcome ($y$).\
  \normalsize \textbf{Question:} Does hormonal contraception ($x$)
influence the risk of heart attack ($y$)?

This question is \alert{equivalent to asking whether the proportion} of
patients with heart attack \alert{is the same} in both treatment groups.

## 

The test-statistic can be calculated as:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
\sum_{\text{all entries}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}} \ .
\end{equation*}
\end{minipage}}
```
```{=tex}
\begin{equation*}
\frac{(23 - 14.8)^2}{14.8} + \frac{(34 - 42.2)^2}{42.2} + \frac{(35 -  43.2)^2}{43.2} + \frac{(132 - 123.8)^2}{123.8} =  8.329
\end{equation*}
```
  and is expected to be $\chi^2_{1}$ distributed \tiny(1 degree of
freedom: $(2-1)\cdot(2-1)$).\
 

\normalsize

The $p$-value of this test is given as
$\Pr(X\geq 8.329)=`r format(1-pchisq(8.329,1),4,4,4)`$.\
 

\tiny

```{r echo = TRUE}
pchisq(8.329, 1, lower.tail= F)
```

\normalsize

$\rightarrow$ There is \alert{strong evidence} for an association
between hormonal contraception and the risk of heart attack.

## Quantifying the strength of the association

If we consider hormonal contraception vs. no hormonal contraception as
our two treatment groups, then $\pi_1$ and $\pi_2$ represent the
relative frequencies (proportions) of women with heart attack in each
group:

```{=tex}
\begin{eqnarray*}
\pi_1 = 23/57 &=& 0.404 \\
\pi_2 = 35/167 &=& 0.210\\
\end{eqnarray*}
```
Using these values, there are at least 3 metrics that can be used to
quantify how the two groups differ...

## 

Three measures to quantify the association/difference between groups:

```{=tex}
\begin{itemize}
\item Risk difference: $\pi_1 - \pi_2 = 0.404 - 0.210$ = 0.194\\[2mm]

\item Relative risk: $\pi_1 / \pi_2 = 0.404 / 0.210 = 1.92 $\\[2mm]
\item \alert{Odds ratio} ("Chancenverhältnis"):
\begin{equation*}
OR = \frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} = \frac{ 0.404 / (1- 0.404)}{0.210 / (1-0.210)} = 2.55 \ ,
\end{equation*}

where $\pi/(1-\pi)$ is the odds (die "Chance").  
\ 

Interpretation:
\begin{enumerate}
\item   $OR=1$  $\rightarrow$ the two groups do not differ.
\item   $OR > 1 (<1) $ $\rightarrow$ group 1 > (or <) group 2
\end{enumerate}
\end{itemize}
```
## The odds and the odds ratio

```{=tex}
\begin{itemize}
\item The {\bf odds} ("Wettverhältnis"): For a probability $\pi$ the odds is \begin{equation}
\frac{\pi}{(1-\pi)} = \frac{\text{Wahrscheinlichkeit}}{\text{Gegenwahrscheinlichkeit}} \ .
\end{equation}
For example, if the probability to win a game is 0.75, then the odds is given as 0.75/0.25 or 3:1.  

\item The {\bf odds ratio} is given on the previous slide. It is a ratio of two ratios, or, the {\bf ratio of two odds}.\\[4mm]

\item Often the {\bf log odds ratio} is used:
\begin{equation*}
\log(OR) = \log\left(\frac{\pi_1 / (1-\pi_1)}{\pi_2 / (1-\pi_2)} \right) \ .
\end{equation*}

\begin{enumerate}
\item   $\log(OR)=0$ $\rightarrow$ the two groups do not differ.
\item   $\log(OR)>0 (<0)$ $\rightarrow$ group 1 > (or <) group 2
\end{enumerate}
\end{itemize}
```
## Binomial and binary regression

Often, the situation is more complicated than:

```{=tex}
\begin{center} {\bf binary outcome (${y}$)$\sim$ binary explanatory variable (${x}$)}\end{center}
```
\
Often, we are interested in:

```{=tex}
\begin{center} {\bf binary outcome (${y}$)$\sim$ continuous/categorical variables ${x}^{(1)}$, ${x}^{(2)}$, ... }\\[10mm]
\end{center}
```
$\rightarrow$ A (multiple) regression model is needed!

## Illustrative/working example

Let us look at an example from chapter 9.2 in Hector (2015):

Eight groups of beetles were exposed to carbon disulphide (an
insecticide) for 5h. For each beetle it was then reported whether it was
killed or not (1 or 0), but the data were reported in
\textbf{aggregated} form:

\small

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(AICcmodavg)
data(beetle)
beetle
```

\normalsize

## 

As always, start with a graph:

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.width=4, fig.height=3,out.width="6cm", fig.align='center'}
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
geom_smooth(span = 1, colour = "red", se = FALSE) +
xlab("Dose") + ylab("Mortality rate")
```

with linear (blue) and smoothed line (red). \textbf{Question:} (How)
does the dose of the insecticide (${x}$) affect the mortality (${y}$) of
the beetles?

## 

\textbf{What can we see from the plot?}

```{=tex}
\begin{itemize}
\item Mortality increases with higher doses of the herbicide \\[2mm]
\item The linear line seems unreasonable. In particular, extrapolation to lower or higher doses leads to mortalities $<0$ or $>1$, which is not possible.  
\tiny (Remember: A probability is between 0 and 1 by definition.)\\[8mm]
\end{itemize}
```
\normalsize

\textbf{How does one analyze these data correctly?}

```{=tex}
\begin{itemize}
\item So far, we know linear and Poisson regression.\\[2mm]
\item \alert{Neither} are the correct approach here.\\[2mm]
\end{itemize}
```
## The "wrong" analyses

\textbf{Wrong analysis 1: Linear regression}

We could simply use: $$E(y_i) = \beta_0 + \beta_1 Dose_i $$ with
$E(y_i)=\pi_i =$ probability to die for individuals $i$ with $Dose_i$.\
\
\small  

```{r echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
r.lm<- lm(Mortality_rate~ Dose, data= beetle)
```

\normalsize

Estimates are
$\hat\beta_0 = `r format(r.lm$coef[1],2,2,2)`$ and $\hat\beta_1=`r format(r.lm$coef[2],2,2,2)`$.
This means for instance that, for a zero dose, the probability to die would be
$E(y_i)=`r format(r.lm$coef[1],2,2,2)`$.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf Problems:} 
\begin{itemize}
\item Impossible predicted probabilities \\
\item Residuals $\epsilon_i$ are {\bf not} normally distributed
\end{itemize}
\end{minipage}}
```
## 

\textbf{Wrong analysis 2: Poisson regression}

What about Poisson regression with the counts `Number_killed` in the
response? We could use: $$\log(E(y_i)) = \beta_0 + \beta_1 Dose_i$$

with $E(y_i)=$ number killed.  

```{r echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
r.pois<- glm(Number_killed~ Dose, data= beetle,family= poisson)
```

\normalsize

This leads to
$\hat\beta_0 = `r format(r.pois$coef[1],2,2,2)`$ and $\hat\beta_1=`r format(r.pois$coef[2],2,2,2)`$.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
{\bf Problem:} This model also makes impossible predictions as, for a dose of 76, it predicts that $E(y_i)=\exp(\hat\beta_0 + \hat\beta_1 \cdot 76) = 
`r format(exp(r.pois$coef[1] + r.pois$coef[2]*76),2,2,2)`$ beetles die, which is more than the number of beetles that were tested!
\end{minipage}}
```
## Sidenote: Poisson vs. binomial distribution

Clarification of the difference between the Poisson and binomial
distribution:

\textbf{Poisson is appropriate when:}

```{=tex}
\begin{itemize}
\item There is no theoretical upper limit to the number of times an "event" can occur, or observed values are far from such an upper limit (e.g., number of birds observed in a forest plot)
\item Counts cannot be expressed as a proportion.\\[6mm]
\end{itemize}
```
  \textbf{Binomial is appropriate when:}

```{=tex}
\begin{itemize} 
\item Aggregated version of many binary experiments, that is, each can be 0 or 1. 
\item There is an upper limit to the number of times an "event" can occur (e.g., number of deaths out of a known total number of individuals).
\item Events can be expressed as a proportion (number of successes/number of trials).
\end{itemize}
```
## A model for binary data?

Remember the Bernoulli distribution from Mat183:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
The probability distribution of a binary random variable $Y$ $\in \{0,1\}$ with parameter $\pi$ is defined as:
\begin{equation*} 
P(Y=1) = \pi \ , \quad  P(Y=0) = 1-\pi \ .
\end{equation*}
\end{minipage}}
```
 

\textbf{Characteristics of the Bernoulli distribution:}
\label{sl:bernoulli}

```{=tex}
\begin{itemize}
\item $E(Y) = \pi =P(Y=1)$\\[2mm]
\item $Var(Y)=\pi(1-\pi)$.\\[4mm]

$\rightarrow$ The variance of the distribution is determined by its mean.\\[4mm]
\end{itemize}
```
## From binary to binomial data

Binomial data is an \alert{aggregation of binary data}:

```{=tex}
\begin{itemize}
\item Repeat the experiment with $P(Y=1)=\pi$ a total number of $n$ times, calculate how often a success was observed ($k$ times).

\item The expected proportion of successes (``success rate'', here $k/n$) has the same expectation as the success probability of a single experiment: 
\begin{equation*}
E\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \pi = E(Y)  \ .
\end{equation*}
%and the variance of this $Var\left(\frac{\sum_{i=1}^n{Y}}{n}\right) = \frac{\pi(1-\pi)}{n}$.\\[4mm]
\end{itemize}
```
```{=tex}
\vspace{8mm}
\small
```
Example: In the beetle data $n=49$ beetles were tested for the lowest
dose, of which $k=6$ died, thus the "success rate" is $6/49=0.122$.

## The binomial distribution

The \textbf{binomial distribution} assigns the probability of seeing $k$
successes out of $n$ trials, where the success probability of a single
trial is $\pi$.

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
\begin{equation*}
P(Y = k) = {{n}\choose{k}}\pi^k (1-\pi)^{n-k} \ ,\quad k=0, 1, 2,\ldots, n
\end{equation*}
In short: $$Y \sim  Binom(n, \pi) \ .$$
\end{minipage}}
```
 

\textbf{Characteristics of the binomial distribution:}
\label{sl:bernoulli}

```{=tex}
\begin{itemize}
\item Mean: $E(Y)=n \cdot \pi$
\item Variance: $Var(Y)=n\cdot \pi(1-\pi)$\\[2mm]
\end{itemize}
```
$\rightarrow$ For given $n$, the variance is determined by its mean.\
\small R functions: `rbinom(), dbinom()`

## Doing it right: Logistic regression

We can again use the GLM machinery from last week! The
\alert{linear predictor} is as always:

```{=tex}
\begin{equation*}
\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ .
\end{equation*}
```
We need a \alert{link function} that relates the linear predictor
$\eta_i$ to the expected value $E(y_i)$.\
 

The link function must be chosen such that the expected value $E(y_i)$
is always between 0 and 1.

## Link function: The logit transformation

The \alert{logit-transformation} assigns a probability ($\pi$) between 0
and 1, to a value between $-\infty$ and $\infty$:

```{=tex}
\colorbox{lightgray}{\begin{minipage}{14cm}
$$g(\pi) = \log \left(\frac{\pi}{1-\pi}\right)\ .$$
\end{minipage}}
```
A graph depicts the functional form of $g(\cdot)$:

```{r fig.width=4, fig.height=4,out.width="4.5cm", fig.align='center', echo = FALSE, message=FALSE, warning=FALSE}
x <- seq(0,1,0.001)
plot(log(x/(1-x)), x, ylab=expression(pi), xlab=expression(log(pi/(1-pi))),cex=0.7)
```

\small See also Box 9.2 (p. 123) in \emph{The new statistics with R}.

## The logistic regression model

```{=tex}
\colorbox{lightgray}
{\begin{minipage}{14cm}
In order to prevent the expected value $E(y_i)$ of a binary outcome (1/0) to attain unreasonable values, we thus formulate the \alert{logistic regression model} as
\begin{equation*}
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} \ 
\end{equation*}

with $\pi_i = P(y_i=1)\ .$
\end{minipage}}
```

\

```{=tex}
\begin{itemize}
\item The \alert{link function} is called the {\bf logit link}.\\[2mm]
\item To backtransform from the link scale (log odds) to the response scale (probability), the {\bf logistic function} is applied\\[2mm]
\item The \alert{family} is {\bf binomial}.\\[2mm]
\end{itemize}
```
## Doing it right: Fitting a logistic regression

```{=tex}
\begin{itemize}
\item As for the Poisson GLM, we can estimate the parameters $\beta_0, \beta_1, \ldots $ by maximizing the likelihood (ML estimation).\\[3mm]

\item The \texttt{glm()} function in R can also handle binomial and binary data\\[3mm]

\item For \texttt{glm(..., family= binomial)}, the default link function is the logistic link. \\[3mm]

\item Another novelty lies in the fact that we need to give the function \alert{two numbers for the response}:
  \begin{itemize}
  \item The number of successes, encoded as 1 (here: number killed )
  \item The number of failures, encoded as 0 (here: number survived)
  \end{itemize}
  \vspace{4mm}

\end{itemize}
```
\small

```{r echo = TRUE, message=FALSE, warning=FALSE}
beetle$Number_survived<- beetle$Number_tested - beetle$Number_killed
beetle.glm<- glm(cbind(Number_killed, Number_survived)~ Dose, 
                 data= beetle, family= binomial)
```

## Doing it right: Model diagnostics

As always, before looking at the output, assess model diagnostics:

```{r echo = FALSE, warning=FALSE, message=FALSE,fig.width=5, fig.height=5,out.width="6cm", fig.align='center'}
library(ggfortify)
autoplot(beetle.glm,smooth.colour=NA)
```

$\rightarrow$ Hard to see anything due to low number of data points.

## 

```{=tex}
\begin{itemize}
\item As in Poisson regression, it is not clear how to define residuals, there are many ways (data scale, linear predictor scale, likelihood scale).\\[3mm]
\item Again, different types of residuals are used in the plots generated by \texttt{autoplot()}. \\[3mm]
\item \alert{Be careful}: such plots are only reasonable for \alert{aggregated data} (which we have here)! The larger the groups, the more precise the underlying assumptions (approximate equality of distributions).\\[3mm]
\item See example on slide 40 for an example with non-aggregated (binary) data.
\end{itemize}
```

## Doing it right: Interpreting the coefficients

Let's look at the coefficients:

\tiny

```{r echo = TRUE, message=FALSE, warning=FALSE}
summary(beetle.glm)$coef
```

\normalsize

The intercept and slope are estimated as\
\begin{equation*}
\hat\beta_0 = `r format(summary(beetle.glm)$coef[1,1],3,3,3)` \quad \text{and}\quad \hat\beta_1 = `r format(summary(beetle.glm)$coef[2,1],3,3,3)` \ ,
\end{equation*}

with standard errors and $p$-values. Very clearly, the dose influences
the survival probability ($p<<0.001$), and $\hat\beta_1 >0$, thus,
\alert{the larger the dose, the larger the mortality probability}
(positive relation; \scriptsize be careful, this is wrong in the Hector
book!!).

\normalsize

This is a \textbf{qualitative interpretation} of the coefficients.

\small

Note: The $\beta$ coefficients are approximately normally distributed as
$N(\hat\beta,\hat\sigma^2_\beta)$.

$\rightarrow$ confidence intervals etc. can be calculated as in the
linear case

## Quantitative interpretation of the coefficients 
 
Remember the regression model 
\begin{equation}\label{eq:logmodel} 
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1 Dose_i \ .  
\end{equation} 
 
 
To understand what $\beta_1$ tells us, let's solve for $\pi_i$: 
 
\begin{equation}\label{eq:prob} 
\pi_i = P(y_i=1 | Dose_i)= \frac{\exp(\beta_0 + \beta_1 Dose_i)}{1 + \exp(\beta_0 + \beta_1 Dose_i)} \ .  
\end{equation} 
\   
 
\colorbox{lightgray}{\begin{minipage}{14cm} 
From model \eqref{eq:logmodel} it is possible to calculate the \alert{odds} ("Chance"): 
\begin{equation*} 
 odds(y_i=1 | Dose_i) = \frac{\pi_i}{1-\pi_i} = \frac{P(y_i=1 | Dose_i)}{P(y_i=0 | Dose_i)}= \exp(\beta_0 + \beta_1 Dose_i)\ .  
\end{equation*} 
\end{minipage}} 
 
##  
 
If $Dose_i$ is increased by 1 unit (from $x$ to $x+1$), the \alert{odds ratio} is given as:
 
\begin{equation*} 
\frac{odds(y_i=1 | Dose_i = x + 1)}{odds(y_i=1 | Dose_i = x)} = \exp{(\beta_1)} = \exp(`r format(summary(beetle.glm)$coef[2,1],3,3,3)`) = `r format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)` \end{equation*} 
\   
 
\colorbox{lightgray}{\begin{minipage}{14cm} 
\textbf{Interpretation:} When the dose is increased by 1 unit, the odds to die increase by a factor of $`r format(exp(summary(beetle.glm)$coef[2,1]),2,2,2)`$. 
\end{minipage}} 
\   
 
\colorbox{lightgray}{\begin{minipage}{14cm} 
Moreover, taking the $\log$ of the above equation shows that \textbf{$\beta_1$ can be interpreted as a log odds ratio}: 
\begin{equation*} 
\beta_1 = \log\left( \frac{odds(y_i=1 | Dose_i = x + 1)}{odds(y_i=1 | Dose_i = x)}\right) 
\end{equation*} 
\end{minipage}} 
 
## Doing it right: The `anova()` table 
 
We can look at the \alert{Analysis of Deviance} table (using `test= "Chisq"`): 
 
\tiny 
```{r echo = TRUE, message=FALSE, warning=FALSE} 
anova(beetle.glm, test= "Chisq") 
``` 
\normalsize 
 
\textbf{Interpretation:} The total deviance is 267.66, and of this 259.23 is explained by `Dose` (using 1 degree of freedom). This seems really good, because the $\chi^2$ test gives a $p$-value that is reallly small. 
 
## Plotting the fit 
 
A fitted curve can be added to the raw data by plotting $P(y_i=1)$ against the Dose, using equation (\ref{eq:prob}): 
 
```{r echo = FALSE, warning=FALSE, message=FALSE, fig.align='center',fig.width=4, fig.height=3,out.width="6cm"} 
 
xx <- seq(45,80,0.01) 
eta <- -14.578 + 0.2455*xx 
ppi <- exp(eta)/(1+exp(eta)) 
dd <- data.frame(xx=xx,ppi=ppi) 
ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + theme_bw() + 
geom_point() + 
xlab("Dose") + ylab("Mortality rate (Pr(Y=1))")  + 
geom_line(aes(xx, ppi,colour = "red"), dd, lwd=1.2) +  
  scale_color_discrete(guide= "none") 
``` 
 
(Compare to Figure 9.1 in the Hector book \emph{The new statistics with R}.) 
 
## Overdispersion 
 
Remember : 
\begin{itemize} 
\item Slides 18 and 20: $E(Y)=\pi$ and $Var(Y)=\pi(1-\pi)$, thus \alert{the variance is determined by the mean}   
 
 
\item "Overdispersion" means \alert{"extra variability"} (larger than the model predicts or allows). \\[2mm] 
\item Probable reason: Missing variables in the model\\[2mm] 
\item Overdispersion leads to \alert{$p$-values that are too small}.\\[2mm] 
\item Can be detected by looking at the \alert{residual deviance}: \\[2mm] 
\texttt{Residual deviance} $>>$ \texttt{df} $\quad\rightarrow$ Overdispersion \\[2mm] 
 
\item Also possible: underdispersion (dependency in the data), if:\\[2mm] 
\texttt{Residual deviance} $<<$ \texttt{df}   
\end{itemize} 
 
##  
 
Here, the residual deviance is \textbf{8.44} with \textbf{6} degrees of freedom. Is this good or bad?   
\   
 
```{r echo = TRUE} 
pchisq(8.438, 6, lower.tail=F) 
``` 
 
$\rightarrow$ $p=`r format(1-pchisq(8.438,6),2,2,2)`$ does not seem problematic.   
\   
 
One can nevertheless account for overdispersion by switching to a \alert{`quasibinomial`} model, which estimates the dispersion parameter separately. 
 
##  
 
\tiny 
```{r echo = TRUE, message=FALSE, warning=FALSE} 
beetle.glm2<- glm(cbind(Number_killed, Number_survived)~ Dose,  
             data= beetle, family= quasibinomial) 
summary(beetle.glm2) 
``` 
\normalsize 
 
## Binary response / non-aggregated data 
 
\begin{itemize} 
\item In the beetle example, we were in a comfortable situation: For each level of the dosis, we had several beetles. For instance, 49 beetles at lowest dose (49.06), of which 6 died (1) and 43 survived (0). This was \alert{binomial} data, an aggregated version of many (here 49) trials with 0 or 1 outcome.\\[3mm] 
\item In reality, one often has only one trial (0/1) for a (combination of) explanatory variable(s).\\[3mm] 
\item The analysis is the same as for aggregated data, however there are a few complications with graphical descriptions and model checking. \\[6mm] 
\end{itemize} 
 
\textbf{Example}: Blood screening \tiny (see week 1; data from Hothorn \& Everitt 2014, chapter 7.3) 
 
## Blood screening example 
 
\tiny 
```{r echo = FALSE, message=FALSE, warning=FALSE} 
library(HSAUR3) 
data("plasma",package="HSAUR3") 
plasma$y <- as.integer(plasma$ESR)-1 
``` 
 
```{r echo = FALSE, results='asis', warning=FALSE, message=FALSE} 
library(xtable) 
pol1 <- plasma[1:32,] 
ttab1 <- xtable(pol1,display=c("s","f","d","s","d")) 
print(ttab1, floating = TRUE, include.rownames=FALSE) 
``` 
 
##  
 
\textbf{Introduction:} Individuals with a low ESR (Erythrocyte Sedimentation Rate) are generally considered healthy, while those with an ESR $>20mm/hr$ are not. We are interested whether concentrations of the plasma proteins Fibrinogen and Globulin (which can be disease indicators) are associated with an increased probability that an individual has a high ESR (ESR$>20mm/hr$, encoded as $y_i=1$)? 
 
 
\textbf{The model to be fitted:} 
 
\begin{equation*} 
\log\left( \frac{\pi_i}{1-\pi_i} \right) = \beta_0 + \beta_1\cdot fibrinogen_i + \beta_2 \cdot globulin_i \ , 
\end{equation*} 
 
with $E(y_i)=P(y_i=1)=\pi_i$. \\ 
\tiny Equivalently: $y_i \sim Bern(\pi_i)$ 
 
## Complication 1 with binary data: Graphical description 
 
Plotting the response $y$ ($y=1$ if ESR$>20$ and $y=0$ if ESR$<20$) against the covariates does not lead to very informative graphs:  
\   
 
```{r echo = FALSE, message=FALSE, warning=FALSE, fig.align='center',fig.width=5.5, fig.height=2.5,out.width="8cm"} 
require(cowplot) 
theme_set(theme_cowplot(font_size=10)) 
 
p1 <- ggplot(plasma, aes(x = fibrinogen, y = y)) + theme_bw() + 
geom_point() + 
xlab("Fibrinogen") + ylab("y") + scale_y_continuous(breaks=c(0,1)) 
 
p2 <- ggplot(plasma, aes(x = globulin, y = y)) + theme_bw() + 
geom_point() + 
xlab("Globulin") + ylab("y") + scale_y_continuous(breaks=c(0,1)) 
 
plot_grid(p1, p2, ncol=2) 
``` 
 
## 
 
It is a bit more informative to look at a \alert{conditional density plot} (`cdplot()`): 
\   
 
```{r fig.align='center', message=FALSE, warning=FALSE, echo = FALSE, fig.width=7, fig.height=4.5,out.width="10cm"} 
par(mfrow=c(1,2)) 
plasma$y <- as.factor(plasma$y) 
cdplot(y ~ fibrinogen,plasma) 
cdplot(y ~ globulin,plasma) 
``` 
 
## Complication 2: Model diagnostics 
 
\textbf{a) Residual plots:} 
 
\alert{Plotting the residuals} is possible, but \alert{not meaningful}. 
\scriptsize Why? Because the model checking assumptions rely on aggregated data! 
 
```{r echo = FALSE, warning=FALSE, message=FALSE, fig.align='center',fig.width=5, fig.height=4.5,out.width="6.5cm"} 
plasma.glm <- glm(y ~ fibrinogen + globulin, data = plasma, family=binomial) 
autoplot(plasma.glm) 
``` 
 
##  
 
\textbf{b) Residual deviance:} 
 
For non-aggregated data, the `residual deviance` vs.\ `df` relation \textbf{cannot be used to detect overdispersion}!!   
\   
 
\scriptsize  
Why? Becasue for a single binary (0/1) variable it is impossible to estimate a variance, thus it is also impossible to say if the variance is too high/too low. 
 
## Your turn! 
 
Apart from these complications, fitting and interpreting the model is analogous to aggregated binary data. Let's continue with the blood screening example:   
\   
\tiny 
```{r echo = TRUE, eval = FALSE} 
plasma.glm <- glm(y~ fibrinogen + globulin, data= plasma, family= binomial) 
 
``` 
\normalsize 
 
Please look at the model outcomes (summary and anova table) on the next slides and answer the following questions: 
 
\begin{enumerate} 
\item Is there evidence for an an effect of fibrinogen and/or globulin on the outcome?\\[2mm] 
\item What is the \emph{quantitative} interpretation of the $\beta_1$ coefficient (what happens to $P(ESR>20)$ when fibrinogen incrases by 1 unit)? \\[2mm] 
\item Is a \texttt{quasibinomial} model more suitable for these data? 
\end{enumerate} 
 
 
##  
 
\tiny 
```{r echo = TRUE} 
summary(plasma.glm) 
``` 
 
##  
 
\tiny 
```{r echo = TRUE} 
anova(plasma.glm,test="Chisq") 
``` 
 
## Summary 
 
\begin{itemize} 
\item Logistic regression is also known as a \alert{binary/binomial GLM}\\[3mm] 
\item The link function is the logistic link.\\[3mm] 
\item The coefficients of logistic regression are log odds ratios \\ 
\begin{center}$\Leftrightarrow \quad$ $\exp(\beta)$ is an odds ratio \\[2mm]\end{center} 
\item You can think of binomial data as aggregated binary data\\[3mm] 
\item Overdispersion does not apply to binary data
\end{itemize} 